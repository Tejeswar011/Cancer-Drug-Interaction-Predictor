{
  "title": "Paper_1110",
  "abstract": "pmc Entropy (Basel) Entropy (Basel) 3926 entropy entropy Entropy 1099-4300 Multidisciplinary Digital Publishing Institute  (MDPI) PMC12470245 PMC12470245.1 12470245 12470245 41008117 10.3390/e27090991 entropy-27-00991 1 Article Category Name Expansion and an Enhanced Multimodal Fusion Framework for Few-Shot Learning https://orcid.org/0009-0007-4340-8697 Gao Tianlei Conceptualization Methodology Resources Writing – original draft Funding acquisition 1 2 3 Lyu Lei Methodology Validation Resources Writing – original draft 4 https://orcid.org/0000-0002-3137-5070 Xie Xiaoyun Validation Formal analysis Writing – review & editing 2 3 https://orcid.org/0009-0005-8086-8786 Wei Nuo Software Validation Resources Data curation 2 3 https://orcid.org/0009-0001-7587-4242 Geng Yushui Conceptualization Validation Resources Project administration 1 3 * https://orcid.org/0009-0008-0014-3841 Shu Minglei Methodology Software Formal analysis Resources 1 2 3 * Bossé Éloi Academic Editor 1 gaotl@sdas.org 2 xiexy@sdas.org wein@sdas.org 3 4 lvlei@sdnu.edu.cn * gys@qlu.edu.cn sdai@sdas.org 22 9 2025 9 2025 27 9 497633 991 24 7 2025 12 9 2025 20 9 2025 22 09 2025 27 09 2025 29 09 2025 © 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ With the advancement of image processing techniques, few-shot learning (FSL) has gradually become a key approach to addressing the problem of data scarcity. However, existing FSL methods often rely on unimodal information under limited sample conditions, making it difficult to capture fine-grained differences between categories. To address this issue, we propose a multimodal few-shot learning method based on category name expansion and image feature enhancement. By integrating the expanded category text with image features, the proposed method enriches the semantic representation of categories and enhances the model’s sensitivity to detailed features. To further improve the quality of cross-modal information transfer, we introduce a cross-modal residual connection strategy that aligns features across layers through progressive fusion. This approach enables the fused representations to maximize mutual information while reducing redundancy, effectively alleviating the information bottleneck caused by uneven entropy distribution between modalities and enhancing the model’s generalization ability. Experimental results demonstrate that our method achieves superior performance on both natural image datasets (CIFAR-FS and FC100) and a medical image dataset. category name expansion image feature augmentation cross-modal residual connection multimodal fusion semantic representation Key R&D Program of Shandong Province, China 2024CXGC010101 20 Guidelines for New Colleges in Jinan City 202333044 This document provides the results of the research project funded in part by Key R&D Program of Shandong Province, China under Grant No. 2024CXGC010101 and in part by 20 Guidelines for New Colleges in Jinan City under Grant No. 202333044. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction With the rapid advancement of image processing technology, the cost of acquiring annotated data has gradually increased, especially in medical imaging and certain specific natural image domains. This makes building large-scale training datasets extremely difficult. To address this challenge, few-shot learning (FSL) has garnered increasing attention [ 1 2 3 4 5 6 From an information-theoretic perspective, the core dilemma of few-shot learning lies in the challenge of information entropy. When the number of samples is scarce, the amount of information that each category can provide is extremely limited. This means that our understanding of a category’s true data distribution is highly uncertain, i.e., the prior entropy of that category is very high. Although traditional few-shot learning methods can learn effectively with data scarcity, this field still faces many challenges, particularly in improving the model’s generalization ability to new samples and avoiding performance degradation caused by sample scarcity. Traditional few-shot learning methods typically rely on metric learning, where both the support set and query set are mapped into a metric space, and classification is performed using nearest neighbor search. However, the effectiveness of these methods is often limited by the quantity and quality of samples in the support set. Due to sample scarcity, the support set often cannot fully capture the diversity within a category. This means that the amount of information provided by the support set is insufficient to significantly reduce the uncertainty of the category distribution. This leads to unstable model performance when encountering new, unseen samples. Especially when support samples are located at the edges of the metric space, it is difficult for the model to extract sufficiently discriminative features from these scarce samples, thereby affecting the accuracy and stability of classification. With the continuous development of deep learning, utilizing visual features for classification or recognition has achieved some success in certain tasks. However, when faced with limited annotated samples, single-modal models expose some obvious shortcomings. A core challenge of few-shot learning is understanding how to improve a model’s generalization ability to unseen samples. Relying solely on visual features, especially models trained from a single modality, may introduce bias when processing unseen samples. Due to limited training samples, the model struggles to learn comprehensive representative features, which makes it difficult to handle new, unseen categories or variations, thus affecting generalization performance. This is essentially because the information available from a single source (visual modality) to reduce classification uncertainty is limited. To address this problem, few-shot learning research combining multimodal features like text and images has gradually attracted attention [ 7 To address the aforementioned challenges, this paper proposes a novel method based on category name extension, which incorporates sample-specific image features as additional information. Unlike traditional methods that rely solely on textual information for category extension, our method introduces visual modality features by combining the visual characteristics of sample images with category text. This fusion process enhances the semantic representation of each category and increases the information entropy of category embeddings, enabling them to capture more diverse and information-rich cues. From an information-theoretic perspective, the enrichment of entropy reflects a greater ability to describe the inherent variability within a category. By jointly modeling category names and image features, the proposed method allows the model to capture fine-grained visual differences beyond just category labels. Consequently, each category representation not only encompasses semantically meaningful descriptions but also integrates structural and textural information extracted from real images, effectively compensating for the limitations of pure text enhancement in complex tasks and improving the model’s sensitivity to subtle visual details. Furthermore, to obtain higher-quality multimodal fusion features, we propose an improved feature fusion strategy aimed at optimizing the joint representation of image and text modalities. In traditional multimodal learning frameworks, visual and textual information are typically fused through simple concatenation or weighted averaging. Although these methods allow for basic information transfer between modalities, their fusion architecture lacks a conscious, entropy-guided regulation mechanism, making it difficult to effectively balance redundant and useful features. This can lead to the proliferation of irrelevant information or the dilution of crucial signals. To address this problem, we introduce an attention-based feature fusion strategy that includes a dynamic weighting mechanism. This allows the model to adaptively assign importance to different modalities based on the task context and the uncertainty of the input data. The main contributions of this paper are as follows: The combination of category name expansion and image feature augmentation: Based on traditional category name expansion, we incorporate specific sample image features to supplement the expanded content. By introducing image features, the semantic description of the categories becomes more precise and detailed, effectively improving the performance of multimodal few-shot learning. Improved visual and textual feature fusion method: In each layer of feature fusion, we designed a cross-modal residual connection structure to ensure that the output at each layer retains the original information while facilitating the effective alignment of multimodal information. Multi-task validation: We validated the proposed method’s effectiveness across multiple few-shot learning tasks. Experimental results demonstrate that our approach outperforms existing multimodal few-shot learning methods across various datasets, showcasing its broad applicability and excellent performance in different domains and tasks. 2. Related Works 2.1. Few-Shot Learning Few-shot learning aims to improve model generalization under conditions of limited samples by leveraging prior knowledge or developing effective methods. Research in this field primarily focuses on meta-learning-based methods, transfer-learning-based methods, data augmentation-based methods, and metric-learning-based methods. Meta-learning [ 8 9 10 11 12 Transfer learning addresses the issue of data scarcity by transferring knowledge from large-scale pre-trained models to small sample tasks. Data augmentation alleviates the problem of limited data by generating or expanding samples, thus enhancing model generalization. For example, RBC [ 13 Metric learning [ 14 15 16 17 18 19 20 21 22 Although current methods have made progress in small-sample tasks, most rely on assumptions of task distribution consistency or have high computational costs, particularly in multiple optimization updates. Traditional approaches mainly focus on single-modality tasks and lack adaptability in multimodal scenarios, such as joint reasoning between images and text. Therefore, multimodal few-shot learning has become a new research direction, emphasizing how to leverage multimodal information to collaboratively enhance the model’s generalization ability. 2.2. Multimodal Few-Shot Learning Multimodal few-shot learning has become a key research area in fields like computer vision and natural language processing. It aims to enhance model generalization by integrating information from different modalities (such as images, text, speech) when labeled data is scarce. Zhang et al. [ 23 24 In addition, transfer learning has also made significant progress in multimodal few-shot learning. Models like AMU-Tuning [ 25 26 27 28 29 30 3. Method 3.1. Problem Description Few-shot learning tasks typically involve two datasets: the Base Set and the Novel Set. The Base Set is used to train the model’s metric space, while the Novel Set is used to test the model’s performance on unseen categories. Specifically, the Base Set D base D novel (1) D b a s e = { ( x , y ) | x ∈ X b a s e , y ∈ C b a s e } , D n o v e l = { ( x , y ) | x ∈ X n o v e l , y ∈ C n o v e l } , x y C base C novel C base C novel In the training phase, the Base Set C base f θ : χ → R d x d C b a s e ≫ C n o v e l · I Z Y Z f θ X base Y (2) θ ∗ = a r g m a x θ I ( f θ ( X b a s e ; Y b a s e ) ) . This training can be interpreted as modeling the category distribution p y x In the testing phase of few-shot learning, the Novel Set D Novel S K N D Novel Q M S (3) S = ( x i , y i ) i = 1 N × K , Q = ( x i , y i ) i = 1 N × M , x i y i N K M N K N K To achieve the above goal, we propose a multimodal few-shot learning approach based on category name expansion and image feature supplementation. In the following sections, we will introduce our method in terms of three aspects: First, we provide an overview of the model architecture and core design. Next, we analyze the training process, including how the Base Set is utilized to enhance generalization. Finally, we explain the testing phase in detail, focusing on the roles of the novel support and query sets and their application in few-shot learning. 3.2. Overall Architecture of the Model In this section, we provide a detailed introduction to the proposed multimodal few-shot classification model. The model integrates category name expansion with image feature supplementation to enhance the expressiveness of the textual modality. Additionally, a multi-stage fusion strategy is designed to optimize category prototype representations, thereby improving the performance of multimodal few-shot classification tasks. The training-phase diagram shown in Figure 1 First, category names are expanded and processed by a text encoder to extract initial text features. Simultaneously, images are processed through an image encoder to obtain visual features. The extracted text and image features are then concatenated and fed into a multi-level fusion module. This module integrates category semantics with structural and textural information from the visual modality, thereby increasing the information entropy of the category representations. This enhancement helps mitigate information loss and semantic ambiguity caused by reliance on a single modality. Next, the enhanced text features and image features are further processed by the multimodal fusion module, which adopts a multi-stage fusion strategy. In the first stage, the initial text and image features serve as the pre-stage features. These initial features provide the original unimodal semantics, preventing semantic drift after multiple stages of fusion. At the same time, they continuously participate in the multi-stage fusion process, offering contextual support to the current stage and ensuring that the model can still capture key semantics under data-scarce few-shot conditions. The enhanced text features and the initial image features are used as the current-stage features for fusion. In the second stage, the enhanced text features and initial image features serve as pre-stage features, with the fusion results from the previous stage used as current-stage features for further integration. The multi-stage fusion not only enables progressive alignment and complementation of features across modalities but also introduces new information flow that gradually reduces the conditional entropy of category representations, enhancing the model’s ability to distinguish subtle differences between categories. Through multiple stages of fusion, the final multimodal representation is obtained. The fused features are then used to construct class prototypes, enabling similarity matching in the feature space for precise few-shot classification. This section is divided into two parts: first, we introduce the multi-level fusion module for category name expansion and image feature supplementation; then, we detail the prototype construction process based on the multi-stage fusion module. 3.2.1. Multi-Level Fusion Module for Name Expansion and Image Feature Augmentation In multimodal few-shot classification tasks, the textual modality serves as one of the key information sources and typically relies on category names for classification. However, the expressiveness of category names is inherently limited and often fails to capture the full semantic scope of a category. Recent studies have attempted to enhance category understanding by leveraging large language models (LLMs) to expand category names into richer textual descriptions. While this approach can increase the information content of the textual modality—effectively raising the semantic entropy of category representations—it often depends on predefined vocabularies or synonyms and related expressions generated by the model, which may not fully capture the visual characteristics of the category. To address this issue, we propose integrating image features with category text expansion. As a high-dimensional perceptual modality, image features inherently encode low-entropy representations of category-specific information such as spatial structures, textures, and shapes. These features can effectively supplement the fine-grained details that are often missing from purely language-based descriptions. By incorporating image features into the text expansion process, we aim to increase the mutual information between the generated text and visual content, thereby enhancing the alignment between modalities. This strategy improves the accuracy and consistency of category representations and ultimately boosts model performance and generalization in few-shot classification tasks. As described in Figure 1 t C t 23 C t e x p a n d (4) z t t e x t = B E R T ( C t e x p a n d ) [ C L S ] , z t t e x t Next, based on the previous content, we utilize image data to enhance the contextual richness of text descriptions for each category t S t t x i t (5) S t = x i i = 1 N t , N t t x i i t (6) P t i m a g e = f ( x i ) , f x i x i f x i In a few-shot setting, representations from a single modality—whether text or images—contain limited information and thus possess high entropy (or uncertainty). Our strategy is designed to fuse these two complementary sources of information, thereby significantly lowering the uncertainty of the resulting joint representation. Specifically, we extract semantic latent variables z t t e x t P t i m a g e z t j o i n t First, we concatenate the semantic latent variable z t t e x t P t i m a g e z t (7) z t = f m ( c o n c a t ( z t t e x t , P t i m a g e ) ) , f m Secondly, to further enhance the robustness and generalization of category representations in multimodal few-shot classification, we introduce a distribution modeling mechanism aimed at capturing the potential semantic and visual variability within each category. Our core insight is that when labeled samples are extremely scarce, relying on a single fixed vector to represent an entire category is essentially a ‘point estimate’. While this method achieves very high information compression, it consequently ignores the rich internal diversity of the category. Therefore, instead of representing a category as an isolated point, we model it as a probability distribution. This approach allows us to not only capture the category’s ‘central’ information (the distribution’s mean) but also to quantify its range of semantic and visual variation (the distribution’s variance). Specifically, after obtaining the fused category representation z t z t j o i n t (8) q ( z t j o i n t | C t e x p a n d , P t i m a g e ) = Δ ( μ t j o i n t , ∑ t j o i n t ) , μ t j o i n t ∑ t j o i n t z t p ( z t j o i n t | C t e x p a n d , P t i m a g e ) q ( z t j o i n t | C t e x p a n d , P t i m a g e ) z t j o i n t To sample the latent variable z t j o i n t (9) z t j o i n t = μ i j o i n t + ϵ σ i j o i n t , ϵ I σ i j o i n t σ i j o i n t z t j o i n t z t j o i n t z t t e x t P t i m a g e C t s a m p This form of representation allows the model to move beyond a fixed point when representing a class, enabling it to learn and express a neighborhood region that covers potential variations. Such flexibility is crucial under few-shot conditions, where limited samples are often insufficient to capture the full semantic space of a class. By introducing distribution modeling, the network is encouraged to learn a more generalizable representation, allowing class boundaries to better accommodate intra-class variability and inter-class similarity. During training, our objective is to ensure that the joint latent representation z t j o i n t C t s a m p C t s a m p (10) L r e s = E q ( z t j o i n t | C t e x p a n d , P t i m a g e ) [ l o g p ( C t s a m p | z t j o i n t ) ] − D K L ( q ( z t j o i n t | C t e x p a n d , P t i m a g e ) ‖ p ( z t j o i n t ) ) . The first term reflects the decoder’s ability to reconstruct the semantic text from the joint latent representation, ensuring that the generated extended class information incorporates both visual context and semantic accuracy. The second term serves as a regularization component, helping the model maintain structural stability in the latent space and reducing overfitting caused by limited samples. By minimizing this loss function through backpropagation, the model is iteratively optimized during training, enabling the generated class descriptions to more comprehensively integrate both image and text information. The resulting enhanced class representations are not only semantically richer but also more suitable for subsequent prototype construction and few-shot classification—especially in scenarios with incomplete modal information (e.g., blurred images) or insufficient semantics (e.g., underdeveloped class descriptions). By using the backpropagation algorithm to minimize the KL divergence and reconstruction error, the model parameters are optimized. Through multiple training iterations, the parameters are fine-tuned so that the generated latent variables z t j o i n t C i s a m p 3.2.2. Multi-Stage Fusion Module In multimodal few-shot classification, efficiently integrating heterogeneous modalities is a core challenge. While the previous section focused on enhancing text features, this section concentrates on achieving more effective cross-modal fusion based on these augmented representations. Text and images, as distinct encoding systems, inherently possess a ‘semantic gap,’ implying potentially very low mutual information between their raw feature representations. Traditional fusion methods, such as feature weighting or direct concatenation, often act as a ‘lossy channel’ because they fail to effectively bridge this gap, leading to information loss. Particularly in deep networks, due to the data processing inequality, information can only decrease or remain constant as it propagates layer by layer. This creates an ‘information bottleneck,’ limiting the discriminative power of the final representation. To address this issue, we propose a cross-modal hierarchical residual connection strategy. This approach aims to fully leverage the strengths of each modality for specific tasks and enhance the effectiveness of cross-modal feature interaction. Compared to traditional residual connections, our method optimizes the problem of information loss in deep networks by implementing a layer-by-layer residual connection mechanism, thereby improving the efficiency of information flow across different levels. Specifically, we design a multi-stage multimodal fusion module, as illustrated in the lower part of Figure 1 As shown in Figure 2 (11) A t t p ( C t s a m p ( l ) , P t i m a g e ( l ) ) = softmax ( C t s a m p ( l ) W t e x t ( P t i m a g e ( l ) ) T d k ) P t i m a g e ( l ) , W text d k l The second cross-attention mechanism then computes the fusion result with text features as the primary representation, where the image features serve as the query and the text features are used as key–value pairs: (12) A t t c ( C t s a m p ( l ) , P t i m a g e ( l ) ) = softmax ( P t i m a g e ( l ) W i m a g e ( C t s a m p ( l ) ) T d k ) C t s a m p ( l ) , W image In cross-modal learning, the interaction between image and text is crucial. To enhance the model’s understanding and representation of both modalities, we not only compute the interaction between the text and image at the current layer but also incorporate the text and image features from the previous layer into the current layer. This helps further improve the flow of information and semantic alignment. In the cross-modal hierarchical residual connection, the third cross-attention mechanism computes the cross-attention between the text features C t s a m p ( l ) P t i m a g e ( l − 1 ) l (13) A t t t 2 i ( C t s a m p ( l ) , P t i m a g e ( l − 1 ) ) = softmax ( C t s a m p ( l ) W t 2 i ( P t i m a g e ( l − 1 ) ) T d k ) P t i m a g e ( l − 1 ) , Att t i (14) A t t i 2 t ( C t s a m p ( l − 1 ) , P t i m a g e ( l ) ) = softmax ( P t i m a g e ( l ) W i 2 t ( C t s a m p ( l − 1 ) ) T d k ) C t s a m p ( l − 1 ) , W i t To further enhance the flexibility of cross-modal fusion path selection, we propose a path self-aware routing mechanism based on cross-attention representations. From an information theory perspective, the goal of multimodal fusion is to minimize uncertainty while integrating information from different sources. Traditional methods typically generate fusion weights from the entire support set, a relatively coarse approach to information processing that struggles to achieve optimal information filtering and fusion for specific tasks. Our method introduces a more refined dynamic routing process. In information theory, entropy measures the uncertainty or disorder of information. To make better decisions with complex cross-modal data, we need to reduce the entropy in the decision-making process. This method directly utilizes the outputs of multiple cross-attention paths at the current layer as input for the dynamic routing process, which is essentially a process of information filtering and refinement. This allows the model to automatically assign weights to different information paths according to the specific needs of the task. This enables the model to more intelligently determine which information paths are more important for the current task, thereby reducing decision uncertainty and achieving more fine-grained task-specific adaptation. To this end, we have designed a task encoder based on a multi-layer attention architecture. Given the four outputs A t t = A t t p , A t t c , A t t t 2 i , A t t i 2 t Att i (15) h i = σ W 1 A t t i + b 1 , W 1 b 1 σ h i h avg α i (16) α i = e x p h i W q h a v g W k T / d k ∑ j e x p h j W q h a v g W k T / d k , W q W k (17) γ i ( T ) = softmax W 2 [ α 1 , α 2 , α 3 , α 4 ] T + b 2 + α i , W 2 b 2 (18) A t t f = M L P ∑ i = 1 4 γ i ( T ) A t t i . 3.2.3. Model Training Process In the previous chapter, we provided a detailed explanation of how to construct reconstructed prototypes using the proposed method. This chapter will focus on the training process of the model based on this prototype construction approach. In few-shot learning tasks, the model is trained on a Base Set, aiming to learn how to effectively classify new samples using only a limited number of labeled examples. For each category, its sample distribution can be represented by an aggregated center, known as the category prototype C S n x i y i S n n C n (19) C n = 1 S n ∑ x i , y i ∈ S n f ϕ x i , f ϕ x i C n n S n n In this study, we construct reconstructed prototypes using the multimodal fusion results obtained in the previous section and use them as the foundation for classification in the testing phase. During training, we optimize the alignment between the reconstructed prototype and the true prototype to better match the category distribution, thereby improving the model’s classification performance. Specifically, we optimize by minimizing the distance between the reconstructed prototype Att f C (20) L a l i g n = ∑ n = 1 N ∑ x i , y i ∈ S n A t t f i − C n 2 , N (21) L = L a l i g n + L r e s . 3.2.4. Model Testing Phase In the training phase ( Section 3.2.3 Figure 3 (22) C r e s n = 1 M ∑ i = 1 M A t t f i , M n q A t t f q (23) d ( A t t f q , C r e s n ) = ‖ A t t f q − C r e s n ‖ 2 . Based on the calculated distances between the query sample and each category’s reconstructed prototype, we assign the query sample to the category with the smallest distance to its reconstructed prototype. Specifically, the classification result of the query sample q (24) y q = a r g m i n n ( d ( A t t f q , C r e s n ) ) . Through this process, the model establishes robust category representations in the feature space based on the reconstructed prototype classification strategy. This enables the model to accurately classify unseen categories by relying on the learned metric space without requiring additional updates to the model parameters. This approach not only effectively reduces dependence on large amounts of labeled data but also significantly enhances the generalization ability of few-shot learning. It enables the model to quickly adapt to new classification tasks while ensuring stable and robust classification performance. 4. Experiments 4.1. Dataset Settings This study conducts experiments on both public and self-constructed datasets. The public datasets include CIFAR-FS [ 27 28 In our self-constructed dataset for this study, we collaborated with Yantai Yuhuangding Hospital to collect multimodal medical data from 255 pathologically confirmed breast cancer patients. All data acquisition was conducted under hospital approval and strictly adhered to protocols of informed consent and patient privacy protection. The inclusion criteria required patients to have a histopathological diagnosis of breast cancer, complete clinical records, and available imaging and pathological slide data, while exclusion criteria included patients with incomplete data, ambiguous pathological diagnoses, or poor-quality imaging unsuitable for analysis. Specifically, the medical imaging data include magnetic resonance imaging (MRI) and ultrasound scans, acquired by radiology experts following standard clinical procedures; pathological tissue slide images, which were prepared and digitized by professional pathologists during routine diagnosis; and clinical textual data, encompassing demographic information, laboratory test results, tumor size and staging, as well as molecular subtype annotations. To ensure annotation consistency, all pathological and molecular subtype labels were independently reviewed by at least two senior pathologists and radiologists, with discrepancies resolved through consensus discussion. Leveraging the clinical resources and expertise of Yuhuangding Hospital, this dataset ensures high clinical authenticity and representativeness, thereby providing a solid foundation for research on multimodal learning and molecular subtype classification in breast cancer. We classified all samples based on the clinical indicators recorded in the text, ultimately dividing the entire dataset into 80 categories. These categories represent the differences in clinical characteristics of breast cancer patients, covering various tumor types, pathological features, and molecular subtypes. In the dataset division, the training set contains 50 categories, the validation set contains 10 categories, and the test set contains 20 categories. This division ensures the diversity of the dataset while meeting the requirement for non-overlapping categories in few-shot learning. Although the average number of samples per category is relatively small (approximately three cases), this design is intentional: it reflects the clinical heterogeneity of breast cancer, where many subtypes and pathological combinations occur at low frequency in practice. Therefore, our dataset mimics real-world diagnostic challenges with long-tail distributions, making it particularly suitable for evaluating few-shot learning models under clinically realistic conditions. ﻿In clinical practice, breast cancer is not a single, uniform disease but a highly heterogeneous condition composed of various biological subtypes, pathological types, and clinical stages. Dividing patients into 80 fine-grained categories based on key clinical indicators such as age, tumor size, T stage, and molecular subtype is essentially equivalent to simulating 80 specific patient types or diagnostic labels in real-world settings. Examples include middle-aged patients with HER2-positive tumors, T2 stage, and tumor diameter greater than 2 cm; young patients with triple-negative breast cancer, early-stage disease but high Ki-67 index; or elderly patients with ER-positive, Luminal A-type tumors and ductal carcinoma pathology. This method of classification goes beyond the simple benign/malignant or four-subtype models, aligning closely with clinical stratification strategies. Each category represents a potential branch of diagnostic or treatment decision-making, making it more suitable for scenarios such as individual risk prediction, treatment response grouping, and precision follow-up stratification. 4.2. Implementation Details In our experiments, we adopt a pre-trained Swin-T [ 31 32 15 −4 4.3. Comparison Experiments To comprehensively evaluate the effectiveness of the proposed method in multimodal few-shot learning, this study compared it with several advanced methods in the field of few-shot learning. These comparison methods cover various few-shot learning strategies, including metric-based learning, meta-learning, and cross-modal learning models. We used the publicly available datasets CIFAR-FS, FC100, and our self-constructed breast cancer dataset to validate the performance of our method under different task settings. The comparison methods include ProtoNet [ 19 33 34 35 17 36 15 37 38 23 First of all, as shown in Table 1 On the FC100 dataset, as shown in Table 1 Table 2 4.4. Ablation Study 4.4.1. Effectiveness of Image Feature Augmentation This section aims to evaluate the enhancement effect of image features on category name expansion. By introducing image features, we hope to enable the model to better understand the actual visual representation of categories and improve the model’s classification ability. To this end, two models are compared in this experiment: (1) Text Expansion Only: This model uses only the expanded category text generated by a large language model as input, without incorporating image features. (2) Text Expansion + Image Features: This model combines image features with the expanded category text, enhancing the category expansion text through conditional generation. By comparing these two models, we aim to explore the supplementary role of image features in category text expansion, particularly in data-scarce scenarios, to determine whether it can significantly improve the model’s generalization ability. We conduct classification tasks on both public datasets (CIFAR-FS, FC100) and our self-built dataset, comparing the classification performance of the two models under different task settings. For each setting, we evaluate the model performance by calculating the Top-1 accuracy. The experimental results are shown in Table 3 To further validate the supplemental role of image features, we conducted a qualitative analysis on certain categories from the FC100 dataset. Using t-SNE dimensionality reduction for visualization, we reduced the feature representations of the model that combines text expansion and image feature fusion and observed the distribution of categories. The experimental results are shown in Figure 4 Additionally, we evaluated the robustness of the image feature enhancement module against noisy extended text and verified how image features improve the quality of text expansion under noise interference. The study focused on two typical types of noise: random replacement (replacing 20% of the keywords in the expanded text with irrelevant words) and truncation (retaining only the first 50% of the expanded text), and was tested on the CIFAR-FS dataset. The experiment compares the baseline-text model, which uses only text expansion, with the baseline-image model, which incorporates image feature enhancement, to analyze the image feature’s robustness in noisy environments. The five-way one-shot classification task is used, and the improvement in classification accuracy (ΔAcc) quantifies the performance gain of the baseline-image model compared to the baseline-text model under noise conditions. The results are shown in Table 4 4.4.2. Learnable Category Embedding Distribution Modeled This section aims to evaluate the effectiveness of the learnable category embedding distribution modeled z t j o i n t z t j o i n t z t j o i n t z t j o i n t z t j o i n t Figure 5 The experimental results demonstrate that the z t j o i n t z t j o i n t z t j o i n t z t j o i n t z t j o i n t At the same time, we performed a visualization analysis of the first-layer multimodal fusion process using channel response heatmaps in the five-way one-shot task of the CIFAR-FS dataset. We selected the top four most active channels to examine the z t j o i n t Figure 6 z t j o i n t 4.4.3. Cross-Modal Hierarchical Residual Connections This section verifies the enhancing effect of cross-modal hierarchical residual connections on information flow and semantic alignment. We aim to analyze through experiments whether cross-modal hierarchical residual connections can effectively prevent information loss and promote better alignment between image and text features. We designed two models for comparison: (1) No Residual Connection: This model removes the cross-modal residual connections and uses only a single-layer cross-attention mechanism for the fusion of image and text features. (2) Hierarchical Residual Connection: This model uses cross-modal hierarchical residual connections, where residual connections ensure that the output of each layer retains the original information during the feature fusion process. This experimental setup helps us to deeply understand the impact of cross-modal residual connections on model performance. The experimental results are shown in Table 5 The experimental results show that the model with hierarchical residual connections consistently outperforms the model without residual connections in terms of classification accuracy on the CIFAR-FS and FC100 datasets. The advantage of hierarchical residual connections is particularly evident in the five-way one-shot task of FC100 and five-way five-shot task of CIFAR-FS. Due to the extreme scarcity of data in few-shot tasks, the model relies heavily on efficiently utilizing the limited data. The hierarchical residual connection helps the model maintain and enhance the features at each layer during the information flow, leading to stronger performance in the classification task. To further validate the effect of hierarchical residual connections, we applied Grad-CAM for visual analysis of the model. The experimental results are shown in Figure 7 We also explored the impact of the number of layers in the multi-stage fusion module on model performance, aiming to analyze the role of different fusion depths in cross-modal feature interaction and semantic alignment. We tested models with fusion layers ranging from 1 to 5 on the CIFAR-FS, FC100, and breast cancer datasets to validate the importance of hierarchical fusion in few-shot learning tasks. Figure 8 We also plotted the saliency maps of the visual encoder for models with different layer counts to compare their feature focusing capabilities. The experimental results are shown in Figure 9 5. Conclusions This paper proposes an innovative few-shot learning method that combines category name expansion with image feature augmentation. By incorporating extended text generated by large language models and image features into the traditional metric learning framework, we enable the model to gain a more comprehensive understanding of both the semantic and visual characteristics of categories, significantly improving classification performance under scarce data conditions. Through a series of ablation experiments, we validated the role of image features in enhancing category expansion texts. Additionally, we employed cross-modal hierarchical residual connections to optimize information flow and semantic alignment, further improving the effectiveness of multimodal fusion. Experimental results show that the proposed method outperforms several advanced comparison methods on the CIFAR-FS, FC100, and medical image datasets, demonstrating its exceptional performance in few-shot learning tasks. In particular, it effectively enhances the model’s learning and generalization abilities, especially in scenarios where there are significant semantic differences between categories. In future work, we will further optimize the multimodal feature fusion method. Although the attention-based fusion method used in this study has significantly improved performance, there is still room for further enhancement, particularly in dynamically adjusting the importance of image and text information. We aim to explore more feature fusion strategies based on self-attention mechanisms or graph neural networks to further improve the efficiency and effectiveness of multimodal fusion. Disclaimer/Publisher’s Note: Author Contributions Conceptualization, T.G. and Y.G.; Methodology, T.G., L.L. and M.S.; Software, N.W. and M.S.; Validation, L.L., X.X., N.W. and Y.G.; Formal analysis, X.X. and M.S.; Resources, T.G., L.L., N.W., Y.G. and M.S.; Data curation, N.W.; Writing—original draft, T.G. and L.L.; Writing—review & editing, X.X.; Project administration, Y.G.; Funding acquisition, T.G. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement These data were derived from the following resources available in the public domain: [In Proceedings of the 905 Twenty-First International Conference on Artificial Intelligence and Statistics] [ https://proceedings.mlr.press/v84/ Conflicts of Interest The authors declare no conflicts of interest. References 1. Wang H. Mai H. Gong Y. Deng Z.H. Towards well-generalizing meta-learning via adversarial task augmentation Artif. Intell. 2023 317 103875 10.1016/j.artint.2023.103875 2. Zeng Z. Xiong D. Unsupervised and few-shot parsing from pretrained language models Artif. Intell. 2022 305 103665 10.1016/j.artint.2022.103665 3. Wang Y. Wu L. Li J. Liang X. Zhang M. Are the BERT family zero-shot learners? A study on their potential and limitations Artif. Intell. 2023 322 103953 10.1016/j.artint.2023.103953 4. Pachetti E. Colantonio S. A systematic review of few-shot learning in medical imaging Artif. Intell. Med. 2024 156 102949 10.1016/j.artmed.2024.102949 39178621 5. Jia J. Feng X. Yu H. Few-shot classification via efficient meta-learning with hybrid optimization Eng. Appl. Artif. Intell. 2024 127 107296 10.1016/j.engappai.2023.107296 6. Xin Z. Chen S. Wu T. Shao Y. Ding W. You X. Few-shot object detection: Research advances and challenges Inf. Fusion 2024 107 102307 10.1016/j.inffus.2024.102307 7. Liu F. Zhang T. Dai W. Cai W. Zhou X. Chen D. Few-shot Adaptation of Multi-modal Foundation Models: A Survey Artif. Intell. Rev. 2024 57 268 10.1007/s10462-024-10915-y 8. Finn C. Abbeel P. Levine S. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks Proceedings of the 34th International Conference on Machine Learning Sydney, Australia 6–11 August 2017 1126 1135 9. Elsken T. Staffler B. Metzen J.H. Hutter F. Meta-Learning of Neural Architectures for Few-Shot Learning Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Seattle, WA, USA 13–19 June 2020 12362 12372 10. Ravi S. Larochelle H. Optimization as a Model for Few-Shot Learning Proceedings of the International Conference on Learning Representations Toulon, France 24–26 April 2017 11. Andrychowicz M. Denil M. Gómez S. Hoffman M.W. Pfau D. Schaul T. Shillingford B. de Freitas N. Learning to learn by gradient descent by gradient descent Proceedings of the Advances in Neural Information Processing Systems Curran Associates, Inc. Red Hook, NY, USA 2016 Volume 29 12. Santoro A. Bartunov S. Botvinick M. Wierstra D. Lillicrap T. Meta-learning with memory-augmented neural networks Proceedings of the 33rd International Conference on International Conference on Machine Learning, ICML’16 New York, NY, USA 19–24 June 2016 Volume 48 1842 1850 13. Alhussan A.A. Abdelhamid A.A. Towfek S.K. Ibrahim A. Abualigah L. Khodadadi N. Khafaga D.S. Al-Otaibi S. Ahmed A.E. Classification of Breast Cancer Using Transfer Learning and Advanced Al-Biruni Earth Radius Optimization Biomimetics 2023 8 270 10.3390/biomimetics8030270 37504158 PMC10377265 14. Gidaris S. Bursuc A. Komodakis N. Perez P.P. Cord M. Boosting Few-Shot Visual Learning with Self-Supervision Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV) Seoul, Republic of Korea 27 October–2 November 2019 8058 8067 15. Hiller M. Ma R. Harandi M. Drummond T. Rethinking Generalization in Few-Shot Classification Adv. Neural Inf. Process. Syst. 2022 35 3582 3595 16. Mangla P. Singh M. Sinha A. Kumari N. Balasubramanian V. Krishnamurthy B. Charting the Right Manifold: Manifold Mixup for Few-shot Learning Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019) Vancouver, BC, Canada 8–14 December 2019 17. Tian Y. Wang Y. Krishnan D. Tenenbaum J.B. Isola P. Rethinking Few-Shot Image Classification: A Good Embedding Is All You Need? Computer Vision—ECCV 2020 Vedaldi A. Bischof H. Brox T. Frahm J.M. Springer International Publishing Cham, Switzerland 2020 266 282 18. Koch G.R. Siamese Neural Networks for One-Shot Image Recognition Proceedings of the 32nd International Conference on Machine Learning Lille, France 6–11 July 2015 19. Snell J. Swersky K. Zemel R. Prototypical networks for few-shot learning Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17 Long Beach, CA, USA 4–9 December 2017 Curran Associates Inc. Red Hook, NY, USA 2017 4080 4090 20. Zhang C. Cai Y. Lin G. Shen C. DeepEMD: Differentiable Earth Mover’s Distance for Few-Shot Learning IEEE Trans. Pattern Anal. Mach. Intell. 2023 45 5632 5648 10.1109/TPAMI.2022.3217373 36288227 21. Doersch C. Gupta A. Zisserman A. CrossTransformers: Spatially-aware few-shot transfer arXiv 2021 10.48550/arXiv.2007.11498 2007.11498 22. Bateni P. Goyal R. Masrani V. Wood F. Sigal L. Improved Few-Shot Visual Classification Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Seattle, WA, USA 13–19 June 2020 14481 14490 10.1109/CVPR42600.2020.01450 23. Zhang H. Xu J. Jiang S. He Z. Simple Semantic-Aided Few-Shot Learning Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Seattle, WA, USA 16–22 June 2024 28588 28597 10.1109/CVPR52733.2024.02701 24. Vuorio R. Sun S.H. Hu H. Lim J.J. Multimodal model-agnostic meta-learning via task-aware modulation Proceedings of the 33rd International Conference on Neural Information Processing Systems Vancouver, BC, Canada 8–14 December 2019 Number 1 Curran Associates Inc. Red Hook, NY, USA 2019 1 12 25. Tang Y. Lin Z. Wang Q. Zhu P. Hu Q. AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Seattle, WA, USA 16–22 June 2024 23323 23333 10.1109/CVPR52733.2024.02201 26. Hou R. Chang H. Ma B. Shan S. Chen X. Cross attention network for few-shot classification Proceedings of the 33rd International Conference on Neural Information Processing Systems Vancouver, BC, Canada 8–14 December 2019 Number 360 Curran Associates Inc. Red Hook, NY, USA 2019 4003 4014 27. Bertinetto L. Henriques J.F. Torr P.H. Vedaldi A. Meta-learning with differentiable closed-form solvers arXiv 2018 1805.08136 28. Oreshkin B. López P.R. Tadam L.A. Task dependent adaptive metric for improved few-shot learning Adv. Neural Inf. Process. Syst. 2018 721 731 29. Xiao F. Pedrycz W. Negation of the Quantum Mass Function for Multisource Quantum Information Fusion with its Application to Pattern Classification IEEE Trans. Pattern Anal. Mach. Intell. 2023 45 2054 2070 10.1109/TPAMI.2022.3167045 35420983 30. Xiao F. Cao Z. Lin C.T. A Complex Weighted Discounting Multisource Information Fusion with its Application in Pattern Classification IEEE Trans. Knowl. Data Eng. 2023 35 7609 7623 10.1109/TKDE.2022.3206871 31. Liu Z. Lin Y. Cao Y. Hu H. Wei Y. Zhang Z. Lin S. Guo B. Swin transformer: Hierarchical vision transformer using shifted windows Proceedings of the IEEE/CVF International Conference on Computer Vision Montreal, QC, Canada 10–17 October 2021 10012 10022 32. Radford A. Kim J.W. Hallacy C. Ramesh A. Goh G. Agarwal S. Sastry G. Askell A. Mishkin P. Clark J. Learning transferable visual models from natural language supervision Proceedings of the International Conference on Machine Learning Virtual 18–24 July 2021 8748 8763 33. Oreshkin B.N. Rodriguez P. Lacoste A. TADAM: Task dependent adaptive metric for improved few-shot learning Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18 Montréal, QC, Canada 3–8 December 2018 Curran Associates Inc. Red Hook, NY, USA 2018 719 729 34. Lee K. Maji S. Ravichandran A. Soatto S. Meta-Learning with Differentiable Convex Optimization Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Long Beach, CA, USA 15–20 June 2019 10649 10657 10.1109/CVPR.2019.01091 35. Kim J. Kim H. Kim G. Model-Agnostic Boundary-Adversarial Sampling for Test-Time Generalization in Few-Shot Learning Proceedings of the Computer Vision—ECCV 2020: 16th European Conference Glasgow, UK 23–28 August 2020 Proceedings, Part I Springer Cham, Switzerland 2020 599 617 36. Dong B. Zhou P. Yan S. Zuo W. Self-Promoted Supervision for Few-Shot Transformer Computer Vision—ECCV 2022 Avidan S. Brostow G. Cissé M. Farinella G.M. Hassner T. Springer Nature Cham, Switzerland 2022 329 347 37. Sun S. Gao H. Meta-AdaM: An Meta-Learned Adaptive Optimizer with Momentum for Few-Shot Learning Adv. Neural Inf. Process. Syst. 2023 36 65441 65455 38. Chen W. Si C. Zhang Z. Wang L. Wang Z. Tan T. Notice of Removal: Semantic Prompt for Few-Shot Image Recognition Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Vancouver, BC, Canada 17–24 June 2023 23581 23591 10.1109/CVPR52729.2023.02258 Figure 1 Flowchart of the Training Phase. After category names are expanded, they are input into their respective encoder along with images. The outputs are concatenated, and the image features are used to augment the text features. The fusion process explicitly integrates the encoder-derived features with the enhanced representations and is implemented in a multi-layer stacked manner to progressively refine cross-modal interactions. Figure 2 Cross-Modal Hierarchical Residual Connection Module. The module applies four cross-attention operations: (1) Cur-image as Q, Cur-text as K and V, aligning current image and text features; (2) Cur-image as Q, Pre-image as K and V, integrating historical visual context; (3) Cur-text as Q, Cur-image as K and V, refining text with visual cues; and (4) Cur-text as Q, Pre-text as K and V, incorporating residual information from the previous text layer. The right panel illustrates the cross-attention computation. Figure 3 Flowchart of the Testing Phase. During the testing phase, all parameters remain fixed. The model first constructs prototypes based on the input support set. Then, it extracts features of the query set using the visual encoder and compares them with the reconstructed prototypes to determine the query set’s categories. Figure 4 Visual analysis of image feature supplementation. Note that different colors represent different classes. Figure 5 Ablation Study of Learnable Category Embedding Distribution Modeled. Figure 6 Channel response heatmap in the ablation experiment of Learnable Category Embedding Distribution Modeled. Figure 7 Grad-CAM visualization analysis of cross-modal hierarchical residual connections. Figure 8 The impact of the number of layers in the multi-stage fusion module on model performance. Figure 9 Saliency maps corresponding to different layers of multimodal fusion modules. entropy-27-00991-t001_Table 1 Table 1 Comparison with state-of-the-art methods on the CIFAR-FS and FC100 datasets. Method CIFAR-FS FC100 5-Way 1-Shot 5-Way 5-Shot 5-Way 1-Shot 5-Way 5-Shot ProtoNet 72.20 ± 0.70 83.50 ± 0.50 41.54 ± 0.76 57.08 ± 0.76 TADAM - - 40.10 ± 0.40 56.10 ± 0.40 MetaOptNet 72.80 ± 0.70 84.30 ± 0.50 47.20 ± 0.60 55.50 ± 0.60 MABAS 73.51 ± 0.92 85.65 ± 0.65 42.31 ± 0.75 58.16 ± 0.78 RFS 71.50 ± 0.80 86.00 ± 0.50 42.60 ± 0.70 59.10 ± 0.60 SUN 78.37 ± 0.46 88.84 ± 0.32 - - FewTURE 77.76 ± 0.81 88.90 ± 0.59 47.68 ± 0.78 63.81 ± 0.75 Meta-AdaM - - 41.12 ± 0.49 56.14 ± 0.49 SP-CLIP 82.18 ± 0.40 88.24 ± 0.32 48.53 ± 0.38 61.55 ± 0.41 SemFew 84.34 ± 0.67 89.11 ± 0.54 54.27 ± 0.77 65.02 ± 0.72 ours 86.45 ± 0.56 91.03 ± 0.61 56.14 ± 0.76 67.21 ± 0.75 entropy-27-00991-t002_Table 2 Table 2 Comparison with state-of-the-art methods on the breast cancer dataset. Method MRI + Text Pathology + Text 5-Way 1-Shot 5-Way 5-Shot 5-Way 1-Shot 5-Way 5-Shot ProtoNet 52.43 ± 0.56 57.12 ± 0.64 48.32 ± 0.61 54.77 ± 0.58 MetaOptNet 53.12 ± 0.61 59.05 ± 0.70 49.89 ± 0.67 56.23 ± 0.62 MABAS 54.08 ± 0.63 60.21 ± 0.72 50.16 ± 0.66 57.34 ± 0.68 RFS 55.00 ± 0.62 60.55 ± 0.73 51.03 ± 0.69 58.10 ± 0.65 SUN 54.70 ± 0.65 59.85 ± 0.71 50.78 ± 0.70 57.98 ± 0.64 FewTURE 55.56 ± 0.64 61.24 ± 0.69 51.21 ± 0.68 58.40 ± 0.66 SemFew 55.12 ± 0.63 60.90 ± 0.72 50.50 ± 0.67 57.89 ± 0.69 ours 56.87 ± 0.54 62.53 ± 0.66 52.49 ± 0.72 59.56 ± 0.71 entropy-27-00991-t003_Table 3 Table 3 Ablation experiment on image feature supplementation. Method CIFAR-FS FC100 Breast Cancer (MRI + Text) 5-Way 1-Shot 5-Way 5-Shot 5-Way 1-Shot 5-Way 5-Shot 5-Way 1-Shot 5-Way 5-Shot Without image 83.16 ± 0.58 89.50 ± 0.59 53.21 ± 0.75 64.95 ± 0.77 53.87 ± 0.52 60.87 ± 0.65 With image 86.45 ± 0.56 91.03 ± 0.61 56.14 ± 0.76 67.21 ± 0.75 56.87 ± 0.54 62.53 ± 0.66 entropy-27-00991-t004_Table 4 Table 4 Ablation experiment on image feature supplementation under noise situation. Noise Type CIFAR-FS Baseline-Text Baseline-Image ΔAcc No Noise 84.12 ± 0.58 86.45 ± 0.56 +2.33 Random Replacement (20%) 81.20 ± 0.72 83.75 ± 0.65 +2.55 Truncation (50%) 82.10 ± 0.70 84.20 ± 0.68 +2.10 entropy-27-00991-t005_Table 5 Table 5 Ablation experiment on cross-modal hierarchical residual connections. Method CIFAR-FS FC100 5-Way 1-Shot 5-Way 5-Shot 5-Way 1-Shot 5-Way 5-Shot without residual connections 84.23 ± 0.57 88.50 ± 0.60 53.95 ± 0.77 64.44 ± 0.74 with residual connections 86.45 ± 0.56 91.03 ± 0.61 56.14 ± 0.76 67.21 ± 0.75 ",
  "metadata": {
    "Title of this paper": "Notice of Removal: Semantic Prompt for Few-Shot Image Recognition",
    "Journal it was published in:": "Entropy",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12470245/"
  }
}
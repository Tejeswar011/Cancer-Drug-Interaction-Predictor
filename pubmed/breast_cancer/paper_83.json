{
  "title": "Paper_83",
  "abstract": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12488977 PMC12488977.1 12488977 12488977 41034298 10.1038/s41598-025-14793-4 14793 1 Article Quantum-Inspired gravitationally guided particle swarm optimization for feature selection and classification Malik Saleem baronsaleem@gmail.com 1 Patro S Gopal Krishna 2 Mahanty Chandrakanta 3 Lasisi Ayodele 4 Naveed Quadri Noorulhasan 4 Buradi Abdulrajak 5 Emma Addisu Frinjo addisuf@du.edu.et 6 Kumar Saravanapriya priya@shctpt.edu 7 Mubarakali Azath 8 1 2 3 https://ror.org/0440p1d37 grid.411710.2 0000 0004 0497 3037 Department of Computer Science & Engineering, GITAM School of Technology, GITAM Deemed to Be University, 4 https://ror.org/052kwzs30 grid.412144.6 0000 0004 1790 7100 Department of Computer Science, College of Computer Science, King Khalid University, 5 https://ror.org/00ha14p11 grid.444321.4 0000 0004 0501 2828 Nitte Meenakshi Institute of Technology, 6 https://ror.org/04ahz4692 grid.472268.d 0000 0004 1762 2666 Collage of Engineering and Technolgy, Dilla University, 7 https://ror.org/02pttbd79 grid.473479.a 0000 0004 1769 0984 Department of MCA, Sacred Heart College (Autonomous), 8 https://ror.org/052kwzs30 grid.412144.6 0000 0004 1790 7100 Department of Informatics and Computer Systems, College of Computer Science, King Khalid University, 1 10 2025 2025 15 478255 34155 28 10 2024 4 8 2025 01 10 2025 03 10 2025 03 10 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access http://creativecommons.org/licenses/by-nc-nd/4.0/ Population-based metaheuristic optimization algorithms have gained prominence for tackling complex optimization problems. They balance exploration and exploitation, essential for finding optimal solutions. While algorithms like Genetic Algorithms, Particle Swarm Optimization, and Gravitational Search Algorithm have shown success, they have limitations, such as premature convergence and sensitivity to parameters. To address these issues, we have introduced Quantum-Inspired Gravitationally Guided Particle Swarm Optimization (QIGPSO) for addressing complex optimization challenges, particularly in the context of medical data analysis for diagnosing Non-Communicable Diseases (NCDs). The Quantum Particle Swarm Optimization (QPSO) and Gravitational Search Algorithm (GSA) are both used in QIGPSO. It takes advantage of each algorithm’s strengths in both global and local search processes. We used an absolute Gaussian random variable to improve the search, changed the position update equations and used a wrapper-based method with Support Vector Machine (SVM) for feature selection and classification. The findings suggest that QIGPSO is effective at identifying key features, achieving high accuracy rates, and lowering the number of incorrect classifications across several NCD datasets. Doctors can use QIGPSO data to make better treatment decisions for their patients. QIGPSO overcomes the limitations of conventional optimization methods by faster convergence while improving exploitation balance. Keywords Optimization algorithms; Feature selection Data analysis Particle swarm optimization Subject terms Mathematics and computing Computational science Computer science Information technology pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © Springer Nature Limited 2025 Introduction Population-based metaheuristic optimization methods are commonly used to handle complicated optimization issues 1 2 3 4 5 We developed QIGPSO, a hybridization method that will revolutionize population-based metaheuristic optimization, to address these issues. QIGPSO advances optimization approaches by using quantum physics principles like superposition and entanglement 6 7 8 QIGPSO was created to combine QPSO and GSA’s benefits and mitigate their faults. QPSO excels in global convergence and quick search but often encounters local optima, while GSA excels in local search but issues with premature convergence 9 10 11 12 The QIGPSO algorithm combines QPSO and GSA for maximum optimization power. It excels in global and local search operations, navigating complex search spaces and avoiding local optima entrapment. QIGPSO will transform optimization tasks in many domains with changed position update equations, contraction–expansion coefficients, and a new fitness function. The algorithm’s adaptability to diverse issue kinds, enabled by parameter modifications and kernel function flexibility, enhances its efficacy. QIGPSO excels at feature selection by strategically reducing dimensionality while maintaining classification accuracy. QIGPSO’s wrapper-based method with SVM as the learner algorithm allows it to classify problems with unprecedented efficiency, demonstrating its transformative potential in optimization 13 14 15 In this study, we will systematically assess QIGPSO and compare it to classic QPSO, QGSA, and previously presented algorithms 16 17 Preliminaries Quantum particle swarm optimization (QPSO) QPSO uses quantum mechanics to optimize. The combination of classical and quantum principles allows researchers to find new optimization methods 16 18 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${x}_{i}\\left(z+1\\right)=\\left\\{\\begin{array}{c}p+\\beta *\\left|MP{V}_{i}-{x}_{i}\\left(z\\right)\\right|*{V}_{n}*u if k\\ge 0.5\\\\ p-\\beta *\\left|MP{V}_{i}-{x}_{i}\\left(z\\right)\\right|*{V}_{n}*u if k<0.5\\end{array}\\right\\}$$\\end{document} ( z i z p β: Contraction–expansion coefficient. MPV i i V n u k In the QPSO, Eq. 1 It uses Monte Carlo sampling and a random element to let particles explore the solution space and choose their best and the best for everyone. This equation allows population selection and particle movement toward better solutions. 2 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta =\\frac{1-\\left(1-0.5\\right)*iPVP}{\\text{max}(iPVP)}$$\\end{document} β: Contraction–expansion coefficient. iPVP Equation 2 3 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p=\\frac{({L}_{1}*{f}_{1}*Perbest\\left(i\\right)+{L}_{2}*\\left(1-{f}_{2}\\right)*Globest)}{({L}_{1}+{L}_{2})}$$\\end{document} p L L f f ( i i Globest Equation 3 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$MBV=\\left(\\frac{1}{N}\\right)*\\sum_{d=1}^{N}PV{P}_{i}\\left(z\\right)$$\\end{document} MBV N ( z i z d In Eq. 4 19 The QPSO algorithm was chosen for its capacity to solve difficult optimization problems, and the equations it uses to guide particles toward better solutions while balancing exploration and exploitation. The QPSO method may struggle with high-dimensional optimization challenges. QPSO’s performance is also affected by algorithm parameters, making parameter adjustment and optimization complex 20 Quantum gravitational search algorithm (QGSA) The QGSA method finds optimal solutions. It works in quantum mechanics, not Newtonian mechanics 21 20 5 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${x}_{i}\\left(z+1\\right)=\\left\\{\\begin{array}{c}Bes{t}_{i}+\\delta *\\left|Bes{t}_{i}-{x}_{i}\\left(z\\right)\\right|*\\text{ln}\\left(\\frac{i}{r}\\right), if k\\ge 0.5\\\\ Bes{t}_{i}-\\delta *|Best\\_i-x\\_i (z)|*ln(\\frac{i}{r}), if k<0.5\\end{array}\\right\\}$$\\end{document} x i Best i 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta :$$\\end{document} k: A random value between 0 and 1 generated by a uniform probability density function (PDF). r: Another random value between 0 and 1 generated by a uniform PDF. ln(1/r): Natural logarithm of the inverse of the random value r The QGSA algorithm’s Eq. 5 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Bes{t}_{i}=\\frac{{L}_{1}MBes{t}_{i}+{L}_{2}Perbes{t}_{i}}{{L}_{1}+{L}_{2}}$$\\end{document} Best i L1 and L2: Acceleration coefficients, representing the influence of the mean best (MBest i) i MBest i Perbest i Equation 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Bes{t}_{i}$$\\end{document} 7 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$MBes{t}_{i}=\\frac{\\sum_{j=1}^{k}\\frac{1}{dis{t}_{i}{,}_{j}}*kBes{t}_{j}}{\\sum_{l}^{k}\\frac{1}{dis{t}_{k}{,}_{j}}}$$\\end{document} MBest i kbest: The set of the k best fitness values. d: The dimensionality of the search space dist(i,j,k,l): A function that represents the difference between the position of particle i and the k th j: Index for the k best fitness values. l: Index for dimensions in the search space. In Eq. 7 8 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$dis{t}_{i}{,}_{s}=||{x}_{i}-kbes{t}_{j}||$$\\end{document} || || specifies the Hamming distance and kbest represent the set of best particles in each iteration. Equation 8 22 23 Particle swarm optimization and gravitational search algorithm (PSOGSA) The PSOGSA method was proposed by 24 25 The PSOGSA equation for changing a particle’s position is written as 9 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${x}_{i}\\left(z+1\\right)={x}_{i}+{v}_{i}\\left(z+1\\right)$$\\end{document} x i x i v i PSOGSA calculates gravity forces using particle locations and fitness. These forces affect particle positioning. PSOGSA is based on Eq. 6 26 27 Gaussian-based particle swarm optimization gravitational search algorithm (GPSOGSA) The Gaussian-based Particle Swarm Optimization Gravitational Search Algorithm (GPSOGSA) was proposed by 28 29 30 10 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${v}_{i}\\left(z+1\\right)={v}_{i}\\left(z\\right)+jgpj.acce{l}_{i}\\left(z\\right)+jGPj*\\left(Glbest-{x}_{i}\\left(z\\right)\\right)$$\\end{document} v i th v i th jgpj: Absolute random numbers from the GPD with mean 0 and variance 1. accel i th jGPj: Absolute random numbers from the GPD. Glbest: Global best solution. x i th Equation 10 31 Literature survey In the heuristic algorithms the objects move in a fixed trajectory, whereas in quantum, the word trajectory does not have a role to play, since, the position of particles is uncertain 32 33 34 20 21 23 18 35 36 37 The latter one focusses on quantum inspired heuristic algorithm for classical computers that focus on the principles of quantum mechanics such as standing waves, entanglement and collapse. It is based on the principles of quantum mechanics 38 39 40 41 42 43 Ref 44 45 46 47 48 49 50 51 53 54 Ref 55 56 57 The Draco Lizard Optimizer (DLO) 58 59 In recent years, quantum-inspired feature selection and optimization algorithms have garnered attention. In 2023, a quantum-inspired evolutionary method for feature subset selection optimized feature selection for complicated datasets 60 61 62 Motivation behind the work Quantum physics and trajectory analysis are at the heart of the QPSO algorithm and QGSA, which are advanced optimization methods 63 64 65 66 67 Quantum behaved psogsa (QIGPSO) Combining exploration and exploitation in quantum search space The hybrid QIGPSO method combines QPSO’s social exploration and QGSA’s local search 20 68 69 QIGPSO uses a wave function based on 18 20 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\omega$$\\end{document} 8 11 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${x}_{i}\\left(z+1\\right)=\\left\\{\\begin{array}{c}p+\\alpha *\\left|MBes{t}_{i}-{x}_{i}\\left(z\\right)\\right|*{w}_{i}\\left(z\\right) if s\\ge 0.5\\\\ p-\\alpha *|MBes{t}_{i}-{x}_{i} (z)|*w\\_i (z) if s<0.5\\end{array}\\right\\}$$\\end{document} 12 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha =\\left(1-0.5\\right)*\\left(\\frac{maxiter-z}{maxiter+0.5}\\right)$$\\end{document} The convergence can be accomplished if each particle converges to its local attractor p. Hence Eq. 2 13 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p=\\left|gp\\right|*Perbes{t}_{i}+\\left|GP\\right|*Glbest$$\\end{document} | gp GP 21 14 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$MBest=\\frac{1}{N}\\sum_{d=1}^{N}Glbest(z)$$\\end{document} 15 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\omega }_{i}\\left(z\\right)=\\left(\\frac{1}{ran{d}_{i}}\\right)*{\\omega }_{i\\left(z\\right)}+acce{l}_{i}\\left(z\\right)$$\\end{document} The acceleration of the ith particle is computed as 16 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$acce{l}_{i}\\left(z\\right)=\\sum_{g\\in kbest}Gravit\\left(z\\right)*\\frac{{M}_{i}(z)}{dis{t}_{i}(z)}*(Perbes{t}_{i}\\left(z\\right)-{x}_{i}\\left(z\\right))$$\\end{document} i 17 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$dis{t}_{j}\\left(z\\right)=\\left|Glbest\\left(z\\right)-{x}_{j}\\left(z\\right)\\right|$$\\end{document} The gravitational constant G G 0 18 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Gravit\\left(z\\right)=Gravi{t}_{o}*exp\\left(-\\alpha *\\frac{i}{maxiter}\\right)$$\\end{document} Now as in QGSA, the gravitational mass m i i 19 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${m}_{i}\\left(z\\right)=\\frac{fi{t}_{i}z-worst(z)}{best\\left(z\\right)-worst(z)}$$\\end{document} 20 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${M}_{i}\\left(z\\right)=\\frac{{m}_{i}(z)}{\\sum_{j=1}^{N}{m}_{j}(z)}$$\\end{document} fi i f z wo z f f 21 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$bestf\\left(z\\right)={min}_{j\\in \\left(1\\dots N\\right)} fi{t}_{j}\\left(z\\right)$$\\end{document} 22 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$worstf\\left(z\\right)={max}_{j\\in \\left(1\\dots N\\right)} fi{t}_{j}\\left(z\\right)$$\\end{document} Proposed QIGPSO algorithm The rationale behind utilizing the QIGPSO algorithm is rooted in its multifaceted optimization capabilities. QIGPSO combines the finest of QPSO and Gravitational Search Algorithm to address complicated optimization issues. One of its biggest advantages is its capacity to efficiently search globally and locally. With two sides, it can navigate complex search spaces without being stuck in local optima. It can consider more options. QIGPSO works for many problems and domains. It lets you adjust parameters and kernel functions to adapt to varied scenarios. This makes it an adaptable optimization tool that tackles problems as needed. In feature selection, QIGPSO improves classification accuracy by systematically reducing dimensions. It does this by picking the best subsets of features, which slows down on the amount of computing power needed for classification jobs while keeping accuracy levels high.  Algorithm 1: QIGPSO Algorithm 1 is designed for optimization problems and combines principles from QPSO and QGSA to find the best solution. It takes several input parameters, including the population size, feature dimensionality, and the number of iterations, to configure its optimization process. The algorithm starts by initializing a population of particles, each representing a potential solution. It also initializes the global best solution. The main loop of the algorithm iterates for a specified number of iterations, and within each iteration, it updates the positions and velocities of each particle based on specific equations. Fitness values for each particle are computed using a designated function, and the best and worst fitness values within the population are determined. The algorithm performs various particle-specific calculations, such as Mbest, ω, accel i i 0 i M i Each time, the particles’ fitness is verified and the average accuracy is calculated using an SVM or equivalent method. A particle’s best-known fitness and position change when its fitness grows. If a particle’s fitness exceeds the global best, the global best response changes. The algorithm iterates until it reaches its limit. Finally, it returns the best particle it detected while running, the best current solution. Provide fitness function and problem-specific calculations to make this algorithm perform well for optimization. Algorithm 1 has various significant benefits. First, it calculates the local attractor using an absolute Gaussian probability density function. A common problem in QPSO is that it’s easy to get stuck in local optima. This function fixes that problem. In addition, it fixes a problem in QGSA by making local searches better. The QIGPSO algorithm is very good at exploring the search space effectively while avoiding premature convergence because of these factors working together. Secondly, the algorithm has a contraction–expansion coefficient that is very important for finding the best mix between exploring and exploiting. It is very important to keep this balance so that the method can work well for both problems with narrow and wide solution domains. Because of this, QIGPSO can be used for many different optimization tasks. QIGPSO algorithm for classification In the framework of the QIGPSO method, a wrapper technique is developed to obtain high classification accuracy with a smaller feature set, with the goal of addressing difficulties linked to social and individual impacts while optimizing for efficiency in terms of features, accuracy, and time complexity 70 The SVM Learner Algorithm generates a random population, each representing a candidate solution. Key parameters are determined, and the Support Vector Machine (SVM) learner algorithm is used to choose minimal feature subsets and evaluate the fitness function 71 72 Fitness function To optimize the solution effectively, a novel fitness function is introduced, which considers both classification accuracy and the reduced feature set as evaluative criteria. The fitness function is defined as: 23 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$fitness=a*\\text{log}\\left(mea{n}_{acc}\\right)-\\left(1-a\\right)*\\frac{nfeat}{\\sqrt{N}}*\\left(mea{n}_{acc}\\right)*\\left(1+\\text{sin}\\left(GR\\right)\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha$$\\end{document} nfeat N acc GR Optimization uses the golden ratio for balance. The equation \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1+\\text{sin}\\left(GR\\right)$$\\end{document} The flowchart in Fig. 1 Fig. 1 Conceptual view of QIGPSO algorithm. Benchmark functions for algorithm evaluation This section contains benchmark functions for evaluating optimization methods. These benchmark functions are often used to rigorously test optimization algorithms. Each benchmark function has unique characteristics including multimodality, roughness, and local optima, making them choices for testing adaptive and robust optimization methods. (Appendix A 73 Parameter setting Table 1 Table 1 Parameter setting. Parameter PSO GSA PSOGSA GPSOGSA QPSO QGSA QIGPSO Swarm Size 37 62 49 55 42 58 47 Inertia Weight 0.62 - 0.56 0.73 0.68 - 0.46 Cognitive Learning (c1) 0.95 - 1.05 0.88 1.13 - 0.92 Social Learning (c2) 1.08 - 0.97 1.12 1.05 - 0.98 Maximum Velocity 0.36 0.42 - 0.53 0.47 0.39 0.55 Gravitational Constant - 1250 1300 1400 - 1200 1350 Mass Reduction Coefficient - 0.85 0.92 0.88 - 0.87 0.94 Results on benchmark functions Optimization methods for numerous benchmark functions were tested in a well-built controlled computer environment. Standard desktop machines with NVIDIA GeForce GTX 1080 graphics cards, 16 GB DDR4 RAM, and Intel Core i7-8700 K CPUs were used. Python 3.8 was used to implement the algorithms, with SciPy 1.6.0 and NumPy 1.20.0 for numerical and optimization tasks. R 4.0.3 was used for advanced testing and Python’s built-in functions for statistical studies. Graphs and charts show algorithm performance and convergence in Matplotlib 3.3.4. IBM Quantum Experience ran quantum-inspired algorithms (QPSO and QGSA) in the cloud. Linux PCs running Ubuntu 20.04 LTS were utilized for experiments. This carefully selected hardware and software ensured repeatable and trustworthy experiment results. It addressed several computing issues and tools. Every optimization algorithm demonstrated distinct qualities in its operation. PSO typically performs effectively in functions with continuous, smooth landscapes. Functions having complicated, multi-modal, or discontinuous shapes could be difficult for it to handle. Parameter tuning—such as swarm size and inertia weight—determines PSO performance (Table 2 3 Table 2 Results on benchmark functions – 1. F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 PSO Mean 0.4705 5.50E-03 3.00e + 00 1.27e-01 4.39e-03 1.50e + 00 4.12e-09 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.99e-01 Bestfit 8.70E-09 7.07E-06 3.00e + 00 0.00e + 00 0.00e + 00 8.000e-15 0.000e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.99e-01 SD 0.6933 0.0055 0.00e + 00 0.00e + 00 0.00e + 00 6.617e-01 1.396e-08 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 R.T in seconds 79.32 19.65 23.12 132.09 179.60 63.22 32.56 16.95 17.90 22.56 17.04 17.89 GSA Mean 1.19E-16 4.77E-08 2.190e + 01 2.41e-02 w3.66e-04 1.10e + 01 2.50e-01 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.99e-01 Bestfit 7.92E-17 4.17E-08 3.000e + 00 0.00e + 00 0.00e + 00 8.000e-15 0.000e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.99e-01 SD 2.05E-17 4.91E-08 3.261e + 01 0.00e + 00 2.78e-03 9.720e + 00 3.654e-01 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 7.02e-01 R.T in seconds 62.65 24.76 62.32 9.23 9.67 3.15 4.45 6.86 7.99 1.12 1.34 24.65 PSOGSA Mean 6.56E-19 3.79E-09 3.000e + 00 0.00e + 00 0.00e + 00 3.00e-14 2.54e-09 0.00e + 00 0.00e + 00 0.00e + 00 8.37e-01 4.14e-01 Bestfit 4.91E-19 3.18E-09 3.000e + 00 0.00e + 00 0.00e + 00 2.930e-14 0.000e + 00 0.00e + 00 0.00e + 00 0.00e + 00 8.37e-01 4.14e-01 SD 6.9E − 40 1.93E-09 0.000e + 00 0.00e + 00 2.22e-01 3.500e-15 3.0e-15 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 4.9e-15 R.T in seconds 38.90 16.25 4.12 93.56 113.47 23.25 22.36 1.87 2.67 33.07 28.05 37.90 GPSOGSA Mean 2.0E219 2.46E209 3.00e + 00 3.45e-03 7.32e-04 8.16e-02 2.80e-15 6.24e-02 7.27e-03 0.00e + 00 6.005e-04 4.14e-01 Bestfit 1.5E219 1.82E209 3.00e + 00 1.48e-03 1.65e-11 4.000e-15 0.000e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 4.14e-01 SD 2.55E18 1.04E209 0.00e + 00 1.42e + 00 2.22e-01 3.176e-01 1.016e-04 1.35e-01 3.86e-02 0.00e + 00 2.986e-03 4.05e-02 R.T in seconds 63.56 7.34 9.423 83.98 14.67 11.016 4.54 4.46 3.99 4.33 4.37 24.79 QPSO Mean 1.23E-08 1.33e + 00 3.00e + 00 0.00e + 00 0.00e + 00 1.85e-01 4.44e-05 0.00e + 00 0.00e + 00 1.935e-05 0.00e + 00 3.99e-01 Bestfit 2.45E-10 9.98e-01 3.00e + 00 0.00e + 00 0.00e + 00 8.000e-15 0.000e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.99e-01 SD 1.5E-09 9.45e-01 0.00e + 00 0.00e + 00 0.00e + 00 4.390e-01 0.000e + 00 0.00e + 00 0.00e + 00 8.465e-05 0.00e + 00 0.00e + 00 R.T in seconds 38.26 24.56 6.77 26.78 24.09 14.36 0.97 0.86 0.824 0.82 0.79 5.42 QGSA Mean 5.67E-18 1.07e + 01 3.00e + 00 33.53e-01 1.79e-02 9.00e-16 8.24e-03 0.00e + 00 0.00e + 00 1.048e-04 0.00e + 00 3.99e-01 Bestfit 8.91E-19 9.80e-01 3.00e + 00 0.00e + 00 0.00e + 00 9.000e-16 8.224e-03 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.99e-01 SD 3.2E-18 8.08e + 00 0.00e + 00 0.00e + 00 5.26e-02 7.562e-01 0.000e + 00 0.00e + 00 0.00e + 00 5.742e-04 0.00e + 00 0.00e + 00 R.T in seconds 43.47 41.33 7.23 115.67 322.08 49.58 50.65 38.50 39.02 40.98 39.46 40.98 QIGPSO Mean 3.45E-20 9.90e-01 3.01e + 00 0.2.89e-01 2.32e + 00 0.00e + 00 7.979e-11 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.99e-01 Bestfit 1.23E-21 9.80e-01 3.00e + 00 2.492e-01 2.06e + 02 0.000e + 00 7.531e-07 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.99e-01 SD 6.7E-22 1.77e-06 2.00e-03 1.59e-02 1.53e-01 2.000e-16 1.312e-04 0.00e + 00 0.00e + 00 1.000e-15 0.00e + 00 3.31e-04 R.T in seconds 1.750 23.12 59.26 17.67 37.10 1.12 23.17 0.72 1.55 3.02 7.33 22.45 Table 3 Results on benchmark functions – 2. F13 F14 F15 F16 F17 F18 F19 F20 F21 F22 F23 F24 PSO Mean 0.00e + 00 6.75e-01 −1.00e + 00 0.00e + 00 4.67e + 01 9.15e + 02 6.891e-03 1.38e-01 3.31e + 00 2.17e-02 0.00e + 00 −1.38e + 00 Bestfit 0.00e + 00 6.75e-01 −1.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −3.86e + 00 3.31e + 00 3.07e-04 −1.08e + 00 −1.49e + 00 SD 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 8.81e + 01 1.65e + 03 8.08e-03 2.00e-15 2.12e-02 1.48e-02 0.00e + 00 2.25e-01 R.T in seconds 179.32 119.65 231.12 99.09 17.23 63.56 23.69 40.45 207.34 45.90 37.89 66.70 GSA Mean 0.00e + 00 6.75e-01 −1.00e + 00 0.00e + 00 1.64e + 03 1.23e + 04 1.134e-03 −3.86e + 00 3.31e + 00 3.05e-04 0.00e + 00 −1.34e + 00 Bestfit 0.00e + 00 6.75e-01 −1.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −3.86e + 00 3.31e + 00 3.07e-04 −1.08e + 00 −7.97e-01 SD 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 2.44e + 03 2.23e + 04 3.640e-03 5.40e-01 5.14e-02 5.98e-06 0.00e + 00 2.58e-01 R.T in seconds 154.56 75.78 12.34 50.46 34.15 13.15 44.45 16.06 37.89 51.32 1.67 9.45 PSOGSA Mean 7.18e-02 2.50e-01 −1.00e + 00 1.29e + 03 2.18e-02 1.10e + 01 4.818e-03 −3.86e + 00 −3.31e + 00 6.48e-03 −1.02e + 00 −1.47e + 00 Bestfit 1.35e-03 2.50e-01 −1.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −3.86e + 00 −3.31e + 00 3.20e-04 −1.08e + 00 −1.47e + 00 SD 5.73e-02 1.217e-01 0.00e + 00 1.09e + 03 4.18e-02 9.881e + 00 1.37e-02 5.40e-01 5.84e-02 5.6e-05 2.47e-02 0.00e + 00 R.T in seconds 40.60 16.56 14.67 33.26 45.47 13.89 47.78 12.67 13.56 33.87 67.90 34.90 GPSOGSA Mean 1.59e-01 2.19e-03 −9.79e-01 0.00e + 00 8.14e + 01 0.00e + 00 2.271e-02 −3.86e + 00 −3.32e + 00 3.10e-04 −1.08e + 00 −1.49 + 00 Bestfit 9.40e-06 0.00e + 00 9.91e-01 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −3.86e + 00 −3.31e + 00 3.07e-04 −1.08e + 00 −1.49 + 00 SD 6.65e-01 5.179e-03 1.67e-11 0.00e + 00 3.77e + 02 0.00e + 00 2.856e-02 5.40e-01 4.837e-02 5.98e-06 0.00e + 00 0.00e + 00 R.T in seconds 13.44 6.89 19.98 66.78 15.67 2.34 15.54 34.89 13.78 44.39 14.21 44.80 QPSO Mean 0.00e + 00 6.75e-01 −1.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 3.14e-03 −3.32e + 00 3.07e-04 −1.08e + 00 −1.48e + 00 Bestfit 0.00e + 00 6.75e-01 −1.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −3.86e + 00 −3.31e + 00 3.07e-04 −1.08e + 00 −1.48e + 00 SD 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 5.40e-01 4.837e-02 5.98e-06 0.00e + 00 0.00e + 00 R.T in seconds 12.43 17.86 6.77 40.56 34.02 15.36 0.78 56.76 5.67 9.09 33.45 25.90 QGSA Mean 1.482e-03 6.75e-01 −1.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −3.86e + 00 −2.57e + 00 1.69e-03 −1.12e + 00 −1.50e + 00 Bestfit 8.2098e-03 2.528e-01 −1.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −3.86e + 00 −2.57e + 00 1.696e-03 −1.12e + 00 −1.50e + 00 SD 0.00e + 00 6.509e-10 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 5.40e-01 4.837e-02 1.304e-06 1.19e-01 0.00e + 00 R.T in seconds 40.67 11.34 6.76 11.55 22.08 45.58 70.45 48.50 19.67 45.67 12.34 78.89 QIGPSO Mean 3.6688e + 02 9.737e-01 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −2.29e + 00 −1.17e + 00 3.49e-04 −1.08e + 00 −1.49e + 00 Bestfit 8.3889e + 01 9.542e-01 5.962e-01 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 −3.86e + 00 −1.17e + 00 3.22e-04 −2.19e + 00 −1.49e + 00 SD 1.7578e + 02 5.48e-03 1.06e-01 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 5.79e-01 3.69e-04 2.268e-05 5.25e-01 0.00e + 00 R.T in seconds 2.56 36.12 19.27 0.129 0.004 1.34 0.808 0.56 32.34 12.14 16.56 12.45 The runtimes of QPSO might be longer than those of traditional PSO. QGSA is a gravitational search combined with quantum inspired concepts. It may work for functions with local and global optima. QGSA, like other quantum-inspired algorithms, may need parameters changed (Table 4 Table 4 Results on benchmark functions – 3. F25 F26 F27 F28 F29 F30 F31 F32 F33 F34 F35 F36 PSO Mean −9.13e-01 0.00e + 00 −1.82e + 00 −4.65e + 00 −8.97e + 00 1.19e-02 1.30e-05 1.25e-04 3.54e-04 2.56 + 01 2.677e + 00 0.00e + 00 Bestfit −1.50e + 00 0.00e + 00 −1.82e + 00 −4.69e + 00 −9.57e + 00 4.46e-06 9.50e-06 1.56e-08 1.01e-04 1.29e + 01 4.25e-03 0.00e + 00 SD 3.91e-01 0.00e + 00 0.00e + 00 5.57e-02 4.92e-01 3.85e-02 1.42–06 1.50e-04 1.41e-04 8.29e + 00 1.23e + 01 0.00e + 00 R.T in seconds 17.45 98.23 65.12 50.34 17.23 43.12 16.78 25.56 33.45 60.20 33.66 13.24 GSA Mean −8.82e-01 0.00e + 00 −1.82e + 00 −4.10e + 00 −7.61e + 00 7.88e-02 0.00e + 00 0.00e + 00 7.01e-02 9.59 + 01 3.91e-01 4.61e-01 Bestfit −7.97e-01 0.00e + 00 −1.82e + 00 −4.69e + 00 −9.13e + 00 0e + 00 0.00e + 00 0.00e + 00 2.99e-02 2.98e + 01 0.00e + 00 0.00e + 00 SD 2.00e-01 0.00e + 00 0.00e + 00 2.72e-021 7.90e-01 1.42e-01 0.00e + 00 0.00e + 00 2.86e-02 5.66e + 01 1.216e + 00 9.72e-02 R.T in seconds 14.34 34.45 52.46 12.56 83.56 223.56 7.89 13.67 40.90 24.78 66.87 45.89 PSOGSA Mean −9.43e-01 0.00e + 00 −1.82e + 00 −4.68e + 00 −9.66e + 00 8.38e-02 2.60e-04 7.79e-03 2.50e-02 1.12e + 00 2.85e-01 0.00e + 00 Bestfit −1.50e + 00 0.00e + 00 −1.82e + 00 −4.61e + 00 −9.66e + 00 0e + 00 1.68e-04 3.95e-04 4.20e-04 2.98e + 01 4.26e-04 0.00e + 00 SD 3.18e-01 0.00e + 00 0.00e + 00 0e + 00 0.00e + 00 6.13e-02 3.94e-05 6.24e-03 4.34e-03 5.6e + 01 6.241e-01 0.00e + 00 R.T in seconds 23.90 34.56 44.89 33.50 63.07 67.20 56.39 18.87 44.90 23.56 11.90 12.89 GPSOGSA Mean −1.27e + 00 0.00e + 00 −1.82e + 00 −4.69e + 00 −9.64e + 00 1.40e-02 4.58e-02 2.01e-03 1.30e-03 6.30–01 1.06e + 00 3.14e-03 Bestfit −1.50e + 00 0.00e + 00 −1.82e + 00 −4.69e + 00 −9.66e + 00 1.75e-05 5.27e-04 2.30e-06 5.63e-04 0.00e + 00 0.00e + 00 0.00e + 00 SD 3.59e-01 0.00e + 00 0.00e + 00 0e + 00 3.93e-02 3.28e-02 6.20e-02 7.74e-03 7.33e-04 1.06e + 00 0.00e + 00 4.81e-03 R.T in seconds 23.44 14.67 90.90 43.89 34.90 24.34 19.56 334.7 12.56 45.89 12.67 99.90 QPSO Mean −1.35e + 00 4.17e-06 −1.82e + 00 −4.69e + 00 −9.66e + 00 7.28e-04 2.73e-07 0.00e + 00 1.96e-03 0.00e + 00 5.7e + 00 0.00e + 00 Bestfit −1.50e + 00 1.69e-05 −1.82e + 00 −4.69e + 00 −9.66e + 00 0e + 00 9.44e-08 0.00e + 00 6.08e-04 0.00e + 00 2.68e-02 0.00e + 00 SD 3.12e-01 0.00e + 00 0.00e + 00 2.72e-02 0.00e + 00 1.47e-03 1.78e-07 0.00e + 00 9.69e-04 0.00e + 00 1.391e + 01 0.00e + 00 R.T in seconds 73.47 61.33 33.23 56.67 23.08 56.58 9.65 6.50 55.02 20.98 77.46 78.98 QGSA Mean −1.50e + 00 0.00e + 00 −1.82e + 00 −3.28e + 00 −6.20e + 00 1.31e + 00 0.00e + 00 2.64e-04 2.25e-04 0.00e + 00 1.21e + 00 0.00e + 00 Bestfit −1.50e + 00 0.00e + 00 −1.82e + 00 −3.28e + 00 −6.20e + 00 1.09e + 00 0.00e + 00 7.10e-03 2.38e-06 0.00e + 00 2.29e-06 0.00e + 00 SD 0.00e + 00 0.00e + 00 0.00e + 00 2e-15 0.00e + 00 5.52e-01 0.00e + 00 3.04e-04 5.27e-04 0.00e + 00 1.79e + 00 0.00e + 00 R.T in seconds 78.26 43.56 22.77 12.78 15.09 66.36 3.97 55.86 12.824 40.82 34.79 15.42 QIGPSO Mean −1.41e + 00 0.00e + 00 −1.81e + 00 −3.35e + 00 −3.97e + 00 2.014e + 00 0.00e + 00 0.00e + 00 9.89e-05 0.00e + 00 0.00e + 00 0.00e + 00 Bestfit −1.49e + 00 0.00e + 00 −1.81e + 00 −3.96e + 00 −3.98e + 00 3.201e-01 0.00e + 00 0.00e + 00 1.01e-05 0.00e + 00 0.00e + 00 0.00e + 00 SD 3.71e-07 0.00e + 00 0.00e + 00 2.00e-01 5.10e-04 1.23e + 00 0.00e + 00 0.00e + 00 5.21e-05 0.00e + 00 0.00e + 00 0.00e + 00 R.T in seconds 17.45 1.84 1.19 23.09 26.45 14.12 1.36 22.49 50.76 5.47 5.56 1.74 The QIGPSO uses quantum and gravitational search methods. Due to quantum mechanics-inspired escape mechanisms, it excels at many tasks. Accurate parameter modifications yield the best outcomes (Table 5 Table 5 Results on benchmark functions – 4. F37 F38 F39 F40 F41 F42 F43 F44 F45 F46 F47 F48 PSO Mean −7.68e + 03 0.00e + 00 0.00e + 00 −1.01e + 01 −9.53e + 00 - 1.04e + 01 −1.86e + 02 −1.00e + 00 0.00e + 00 2.30e + 00 1.73e-01 0.00e + 00 Bestfit −8.91e + 03 0.00e + 00 0.00e + 00 −1.05e + 01 −1.01e + 01 −1.04e + 01 −1.86e + 02 −1.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 SD 7.45e + 02 0.00e + 00 0.00e + 00 3.61 + 00 3.514e + 00 3.64e + 00 4.64e-06 0.00e + 00 0.00e + 00 1.70e + 00 3.48e-01 0.00e + 00 R.T in seconds 114.34 314.45 52.46 12.56 83.56 23.56 17.89 13.67 0.90 24.78 66.87 45.89 GSA Mean −6.83e + 03 0.00e + 00 0.00e + 00 −5.26 + 00 −5.73 + 00 - 6.86e + 00 −8.15e + 01 −1.00e + 00 0.00e + 00 6.66 e-02 2.67e-01 0.00e + 00 Bestfit −8.34e + 03 0.00e + 00 0.00e + 00 −1.05e + 01 −1.01e + 01 −1.04001 −1.86e + 02 −1.030e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 SD 7.50e + 02 0.00e + 00 0.00e + 00 1.20e + 00 1.83e + 00 2.82e + 00 6.64e + 01 1.49e-01 0.00e + 00 3.00e + 00 9.49e-01 0.00e + 00 R.T in seconds 123.32 119.65 231.12 199.09 17.23 63.56 23.69 40.45 207.34 45.90 37.89 66.70 PSOGSA Mean −1.25e + 04 6.46e + 00 0.00e + 00 −1.05e + 01 −10.15e + 00 −1.04e + 01 −1.86e + 02 −1.03e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 Bestfit −1.25e + 04 8.21e + 00 0.00e + 00 −1.053e + 01 −1.012e + 01 −1.04e + 01 −1.86e + 02 −1.038e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 SD 2.28e + 02 1.81e-01 0.00e + 00 1.22e + 00 0.00e + 00 0.00e + 00 2.00e-14 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 R.T in seconds 13.44 6.89 19.98 66.78 15.67 2.34 15.54 34.89 13.78 44.39 14.21 44.80 GPSOGSA Mean −1.25e + 04 0.00e + 00 0.00e + 00 −1.05e + 01 9.95e + 00 −9.16e + 00 −1.86e + 02 −1.0310e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 Bestfit −1.25e + 04 0.00e + 00 0.00e + 00 −1.05e + 01 −1.01e + 01 −1.04e + 01 −1.86e + 02 −1.030e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 SD 4.48e + 01 0.00e + 00 0.00e + 00 1.25e + 00 1.83e + 00 2.82e + 00 3.80e-14 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 R.T in seconds 33.45 78.90 23.45 65.78 23.78 54.78 56.39 34.78 123.45 64.67 12.34 12.89 QPSO Mean −1.25e + 04 0.00e + 00 0.00e + 00 −1.053e + 01 −10.15e + 00 −1.04e + 01 −1.86e + 02 −1.031e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 Bestfit −1.25e + 04 0.00e + 00 0.00e + 00 −1.053e + 01 −1.01e + 01 −1.04e + 01 −1.86e + 02 −1.031e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 SD 2.41e-12 0.00e + 00 0.00e + 00 1.2234 + 00 9.25e-01 1.00e-15 2.00e-14 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 R.T in seconds 65.67 61.33 67.98 85.76 34.67 98.09 12.34 32.90 89.90 22.90 77.46 71.91 QGSA Mean −1.25e + 04 0.00e + 00 0.00e + 00 −1.05e + 01 −10.15e + 00 −1.03e + 01 −1.86e + 02 −1.030e + 00 0.00e + 00 5.30e-05 1.57e-02 0.00e + 00 Bestfit −1.25e + 04 0.00e + 00 0.00e + 00 −1.053e + 01 −1.010e + 01 −1.03e + 01 −1.86e + 02 −1.010e + 00 0.00e + 00 0.00e + 00 1.57 e-02 0.00e + 00 SD 2.771e-08 0.00e + 00 0.00e + 00 2.50e-12 5.4292e-11 1.97e-10 5.700e-14 1.49e-03 0.00e + 00 5.39e-10 0.00e + 00 0.00e + 00 R.T in seconds 56.56 33.89 98.09 54.76 55.09 66.36 23.97 45.89 125.67 13.56 34.79 17.89 QIGPSO Mean −3.40e-01 0.00e + 00 0.00e + 00 −1.01e + 01 −10.15e + 00 −1.04e + 01 −1.86e + 02 −1.03e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 Bestfit −3.25e-02 0.00e + 00 0.00e + 00 −1.03e + 01 −1.015e + 01 −1.04e + 01 −1.86e + 02 −1.03e + 00 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 SD 3.22e + 00 0.00e + 00 0.00e + 00 4.8e-05 1.72e-05 4.78–05 7.80e + 01 2.10e-03 0.00e + 00 0.00e + 00 0.00e + 00 0.00e + 00 R.T in seconds 33.15 2.30 100.98 47.09 55.98 53.11 33.56 39.87 174.56 2.32 32.09 31.23 The proposed QIGPSO leverages quantum-inspired behavior to traverse complex landscapes and avoid local optima. Combining the benefits of gravitational and quantum search approaches, QIGPSO aims to improve overall performance, especially on functions with complicated landscapes, while also minimizing susceptibility to parameter adjustment (Table 6 Table 6 Results on benchmark functions – 5. F49 F50 F51 F52 F53 F54 F55 F56 F57 F58 F59 F60 PSO Mean −5.00e + 01 −2.10e + 02 0.00e + 00 6.67E-03 7.12E-03 1.65e + 00 −9.61e + 00 1.21e-02 3.65e-04 0.4802 0.9043 −7.68e + 02 Bestfit −5.00e + 01 −2.10e + 02 0.00e + 00 8.08E-06 5.45E-06 7.000e-14 −1.02e + 01 4.62e-06 1.10e-04 8.75E-02 2.18E-01 −8.91e + 02 SD 3.6e-14 0.000e + 00 0.00e + 00 0.0078 0.0069 7.361e-01 3.615e + 00 3.95e-02 1.54e-04 0.1980 0.7652 7.45e + 01 R.T in seconds 112.89 74.56 86.39 33.56 53.59 12.34 46.98 113.67 67.89 24.78 66.87 45.89 GSA Mean −5.00e + 01 −2.13e + 02 0.00e + 00 4.77E-09 3.24E-09 1.12e + 01 −5.88e + 00 7.89e-02 7.10e-02 1.19E-14 2.65E-13 −6.83e + 02 Bestfit −5.00e + 01 −2.09e + 02 0.00e + 00 4.23E-09 6.78E-09 7.000e-14 −1.02e + 01 0e + 00 2.90e-02 7.92E-14 8.37E-15 −8.34e + 02 SD 2.6e-14 9.434e-13 0.00e + 00 4.91E-09 5.03E-09 7.930e + 00 1.76e + 00 1.54e-01 2.68e-02 3.873E-14 4.182E-14 7.50e + 01 R.T in seconds 98.67 104.56 86.44 50.34 35.12 43.12 22.54 98.12 133.76 78.90 3.66 114.89 PSOGSA Mean −4.94e + 01 −2.00e + 02 1.59e-10 3.79E-09 2.91E-09 4.500e-14 −10.24e + 00 8.49e-02 2.46e-02 5.19E-01 7.34E-01 −1.25e + 03 Bestfit −4.99e + 01 −2.10e + 02 2.10e-14 3.18E-09 4.58E-09 3.253e-14 −1.013e + 01 0e + 00 4.31e-04 1.91E-01 2.44E-01 −1.25e + 03 SD 1.37e + 00 4.625e-13 2.20e-10 1.93E-09 2.15E-09 6.500e-15 0.00e + 00 6.22e-02 4.45e-03 0.2336 0.6892 2.28e + 01 R.T in seconds 55.89 23.45 43.45 43.89 29.87 25.56 18.98 56.70 12.56 65.89 12.67 69.08 GPSOGSA Mean −5.00e + 01 −2.10e + 02 0.00e + 00 2.46E-09 7.89E-09 8.37e-02 9.72e + 00 1.48e-02 1.34e-03 2.06E219 3.27E218 −1.25e + 03 Bestfit −5.00e + 01 −2.09e + 02 0.00e + 00 1.82E-09 1.36E-09 7.000e-14 −1.02e + 01 1.87e-05 5.32e-04 1.55E219 2.91E218 −1.25e + 03 SD 2.68e-14 3.950e-13 0.00e + 00 1.04E-09 9.51E-10 3.601e-01 1.92e + 00 3.36e-02 7.42e-04 2.14E218 3.86E219 4.48e + 00 R.T in seconds 43.89 34.90 5.70 67.89 44.56 22.34 12.78 35.89 77.90 44.27 67.34 11.34 QPSO Mean −5.00e + 01 −2.10e + 02 0.00e + 00 1.33e + 00 1.22e + 00 2.58e-01 −10.17e + 00 7.45e-04 1.69e-03 5.50E-01 6.71E-01 −1.25e + 03 Bestfit −5.00e + 01 −2.10e + 02 0.00e + 00 9.98e-01 8.64e-01 5.000e-14 −1.02e + 01 0e + 00 6.18e-04 7.07E-02 1.22E-01 −1.25e + 03 SD 3.61e-14 0.000e + 00 0.00e + 00 9.45e-01 7.73e-01 7.983e-01 9.87e-01 1.52e-03 9.94e-04 0.4347 0.8745 2.41e-11 R.T in seconds 11.736 35.89 4.02 45.12 30.56 32.89 15.67 33.34 11.89 25.90 8.90 5.97 QGSA Mean −4.47e + 01 −1.50e + 02 0.00e + 00 1.07e + 01 2.59e + 01 5.00e-16 −10.17e + 00 1.41e + 00 2.35e-04 3.79E-05 6.83E-04 −1.25e + 03 Bestfit −4.47e + 01 −1.50e + 02 0.00e + 00 9.80e-01 4.67e-01 5.50e-16 −1.015e + 01 1.23e + 00 2.28e-06 3.18E-05 4.59E-05 −1.25e + 03 SD 2.17e-14 0.000e + 00 0.00e + 00 8.08e + 00 9.29e + 00 7.652e-01 5.4921e-11 6.53e-01 5.12e-04 4.51E-06 2.17E-06 2.771e-07 R.T in seconds 38.904 11.56 15.99 22.34 27.75 43.76 28.59 12.34 55.67 30.90 7.98 9.40 QIGPSO Mean −4.66e + 01 −1.6e + 02 0.00e + 00 9.90e-01 7.91e-01 0.00e + 00 −10.17e + 00 2.034e + 00 9.75e-05 6.56E-16 2.78E-15 −3.40e-02 Bestfit −4.89e + 01 −1.62e + 02 0.00e + 00 9.80e-01 8.13e-01 0.000e + 00 −1.015e + 01 3.209e-01 1.20e-05 4.91E-16 1.09E-15 −3.25e-01 SD 1.179e + 00 2.649e-01 0.00e + 00 1.77e-06 3.11e-07 2.000e-16 1.79e-05 1.32e + 00 5.15e-05 1.015E-16 8.73E-17 3.22e-01 R.T in seconds 62.412 30.15 3.26 39.84 44.56 15.67 25.89 33.67 12.09 8.67 5.98 24.12 Benchmark dataset for algorithm evaluation Experimental setup The research utilized MATLAB R2016a for implementation, running on a system equipped with an i5 processor, 64-bit Windows 8 Operating System operating at 2.60 GHz, and 4 GB RAM. The study applied the QIGPSO algorithm to nine datasets as outlined in Table 7 Table 7 Summary of datasets used. S. No Datasets Used Data type Total Features Instances Class 1 Heart Disease dataset 74 Categorical, Integer, Real 13 + 1 (Class) 303 (0, 1) 2 Kidney Disease Dataset 75 Real 24 + 1 (Class) 400 (0, 1) 3 Parkinson’s Disease Dataset 76 Real 22 + 1 (Class) 197 (0, 1) 4 Pima Indians Diabetes Dataset 77 Integer, Real 8 + 1 (Class) 768 (0, 1) 5 Thyroid disease dataset 78 Categorical, Real 5 + 1 (Class) 7200 (0, 1) Parameter setting The algorithms require specific parameter settings for initialization. Initially, we create a random population of size 30. We set a maximum iteration limit of 100 as the stopping criterion. For the learning algorithm, we utilize SVM. To mitigate overfitting, we employ k-Fold cross-validation, with k being set to 10 based on the dataset size. Our fitness function, as defined in Eq. ( 23 8 Table 8 Parameter setting. Parameter QIGPSO QPSO QGSA GPSOGSA PSOGSA PSO GSA L 1 - 2 1.5 - 0.5 1 0.5 L 2 - 2 1.5 - 1.5 2 1.5 Gravit 0 10 - 10 1 1 - 1 w min - - - - 0.8 0.8 0.8 w max - - - - 0.2 0.2 0.2 a 0.8 0.8 0.8 0.8 0.8 0.8 0.8 b - - - 0.2 0.2 0.2 0.2 While comparing the Heart Disease dataset to other optimization algorithms like PSO, GSA, PSOGSA, GPSOGSA, QPSO, and QGSA, the QIGPSO algorithm showed considerable improvements (Table 9 10 Table 9 Performance of QIGPSO on Heart disease dataset. Metrics PSO GSA PSOGSA GPSOGSA QPSO QGSA QIGPSO Accuracy 0.965 0.968 0.963 0.964 0.982 0.978 0.997 F1-score 0.848 0.865 0.846 0.848 0.921 0.903 0.989 Recall 0.811 0.854 0.811 0.811 0.932 0.877 0.983 Precision 0.888 0.876 0.884 0.889 0.911 0.931 0.995 Computation Time (in seconds) 1240.37 3389.99 705.56 1197.89 206.78 186.24 180.56 Table 10 Performance of QIGPSO on Kidney disease dataset. Metrics PSO GSA PSOGSA GPSOGSA QPSO QGSA QIGPSO Accuracy 0.718 0.696 0.719 0.715 0.630 0.693 0.813 F1-score 0.639 0.672 0.641 0.637 0.575 0.620 0.727 Recall 0.634 0.619 0.649 0.672 0.663 0.678 0.778 Precision 0.645 0.625 0.633 0.606 0.507 0.571 0.683 Computation Time (in seconds) 2015.67 1998.56 1814.56 1351.67 312.56 272.34 193.86 Table 11 12 Table 11 Performance of QIGPSO on Parkinson’s disease dataset. Metrics PSO GSA PSOGSA GPSOGSA QPSO QGSA QIGPSO Accuracy 0.863 0.879 0.914 0.922 0.945 0.956 0.978 F1-score 0.896 0.907 0.919 0.949 0.956 0.957 0.984 Recall 0.867 0.889 0.901 0.921 0.935 0.957 0.978 Precision 0.927 0.925 0.937 0.977 0.977 0.957 0.989 Computation Time (in seconds) 1665.29 1132.43 889.16 525.78 211.34 191.2 196.89 Table 12 Performance of QIGPSO on Diabetics dataset. Metrics PSO GSA PSOGSA GPSOGSA QPSO QGSA QIGPSO Accuracy 0.829 0.831 0.850 0.868 0.855 0.826 0.872 F1-score 0.833 0.835 0.855 0.879 0.863 0.826 0.917 Recall 0.828 0.845 0.867 0.877 0.859 0.829 0.912 Precision 0.838 0.825 0.844 0.881 0.867 0.823 0.923 Computation Time (in seconds) 3024.10 1850.92 1250.34 986.78 199.67 170.67 158.90 Thyroid Disease dataset, Table 13 Table 13 Performance of QIGPSO on Thyroid disease dataset. Metrics PSO GSA PSOGSA GPSOGSA QPSO QGSA QIGPSO Accuracy 0.916 0.927 0.916 0.933 0.933 0.933 0.958 F1-score 0.876 0.861 0.876 0.867 0.906 0.911 0.960 Recall 0.876 0.869 0.869 0.877 0.912 0.899 0.957 Precision 0.876 0.857 0.865 0.876 0.900 0.923 0.964 Computation Time (in seconds) 2048.76 3760.89 1149.56 1004.23 698.67 576.56 461.78 Comparison of the QIGPSO algorithm’s performance across different datasets shows that it constantly performs well and efficiently in classifying data, Fig. 2 Fig. 2 Performance Comparison of QIGPSO Algorithm across benchmark datasets. When you look at how many times different algorithms are run on average across different datasets (Fig. 3 Fig. 3 Performance Comparison of QIGPSO Algorithm across benchmark datasets. When we tested different optimization methods on a number of different datasets, we found big differences in both the average processing time and the average error rate. As you can see in Fig. 4 Fig. 4 average processing time of each algorithm. Fig. 5 Fig. 5 average Error rate of each algorithm. The number of features chosen by different methods was very different across the different datasets shown in Fig. 6 Fig. 6 Number of features selected by different algorithms. Even though QIGPSO picks fewer features, it is thought to be the best algorithm in these situations because it can pick the most important and relevant features and get rid of the less useful ones. QIGPSO is good at finding the best balance between accuracy and speed, which means that it can reduce the number of dimensions in a dataset without lowering the quality of its predictions. It improves computational speed and model performance by choosing fewer but more important features. This is especially true in real-world situations where fewer features can make models faster and easier to understand. Even though QIGPSO tends to pick fewer features than other algorithms, this selective feature reduction helps it do well across a number of measures. The convergence curves for different algorithms on five different datasets—Heart Disease, Diabetes, Kidney Disease, Parkinson’s disease, and Thyroid Disease show in Figs. 7 8 9 10 11 Fig. 7 Convergence Curve comparison across Heart dataset. Fig. 8 Convergence Curve comparison across Diabetes dataset. Fig. 9 Convergence Curve comparison across Kidney dataset. Fig. 10 Convergence Curve comparison across Parkinson’s dataset. Fig. 11 Convergence Curve comparison across Thyroid dataset. All of the algorithms in the Diabetes dataset tend to get better at reducing errors, but QIGPSO and QPSO stand out for how well they work. PSO shows steady convergence, GSA has a higher error rate at first but gets better over iterations, and PSOGSA and GPSOGSA are good examples of hybrid algorithms that work well for optimization. The Kidney Disease dataset demonstrates that PSO and QIGPSO optimize well with the lowest error rates. However, GSA and PSOGSA converge slowly, while GPSOGSA struggles to identify optimal solutions. GSA converges fastest and performs best on the Parkinson’s Disease dataset, while QIGPSO struggles and produces more mistakes. PSOGSA and GPSOGSA are improving, but PSO is ordinary. Lastly, the Thyroid Disease dataset shows that all algorithms successfully lower error rates over time, with PSO showing steady progress. The performance of GSA and PSOGSA is about the same, but their convergence trends are different. QPSO, on the other hand, has a high error rate at first but gets better over time. The error rate for QIGPSO stays the highest throughout this dataset. Overall, the algorithms’ convergence behaviors are very different. QIGPSO performs best across all datasets, especially when it comes to getting fast and stable convergence. These results show how important it is to choose the right optimization method based on the details of the datasets and the optimization process’s goals. We have extended our experiments to include optimization of parameters for Decision Tree, ANN, Logistic Regression, and ensemble models such as Random Forest, Bagging Predictor, and Adaptive Boosting, using the QIGPSO algorithm. The best results obtained from optimizing SVM with QIGPSO will serve as a benchmark for these additional classifiers. We present the optimized parameters and performance metrics for each classifier in Table 14 Table 14 Optimized parameters and performance metrics for each classifier. Decision Tree Metric Optimized Value Max Depth 10 Min Samples Split 4 Accuracy 0.89 Precision 0.87 Recall 0.85 F1 Score 0.86 Convergence Time (s) 35 Artificial Neural Network (ANN) Metric Optimized Value Learning Rate 0.01 Number of Layers 3 Number of Neurons 64 Activation Function ReLU Accuracy 0.91 Precision 0.90 Recall 0.88 F1 Score 0.89 Convergence Time (s) 120 Logistic Regression Metric Optimized Value Regularization Parameter (C) 1.0 Solver liblinear Accuracy 0.88 Precision 0.87 Recall 0.84 F1 Score 0.85 Convergence Time (s) 25 Random Forest Metric Optimized Value Number of Trees 100 Max Depth 20 Min Samples Split 5 Accuracy 0.92 Precision 0.91 Recall 0.89 F1 Score 0.90 Convergence Time (s) 60 Bagging Predictor Metric Optimized Value Number of Estimators 50 Max Samples 1.0 Max Features 1.0 Accuracy 0.90 Precision 0.89 Recall 0.87 F1 Score 0.88 Convergence Time (s) 45 Adaptive Boosting (AdaBoost) Metric Optimized Value Number of Estimators 50 Learning Rate 0.1 Accuracy 0.91 Precision 0.90 Recall 0.88 F1 Score 0.89 Convergence Time (s) 50 Our QIGPSO algorithm optimizes many classifiers, demonstrating robustness and adaptability. Improvements with different classifier parameters show the algorithm’s multifunctionality. This strengthens QIGPSO’s versatility for optimizing machine learning models. Performance analysis of QIGPSO algorithm: Function evaluations, wins, ties, and losses number of function evaluations The total number of function evaluations was tracked for each test function to ensure consistency across all algorithms. The QIGPSO algorithm conducted 3,000 evaluations, calculated as N × maxiter, where N is the population size (30) and maxiter is the maximum number of iterations (100). This standard evaluation methodology was uniformly applied to all algorithms, ensuring a fair and unbiased comparison.  Wins, Ties, and Losses The performance of the QIGPSO algorithm was benchmarked against six other algorithms: QPSO, GSA, PSO, QGSA, GPSOGSA, and PSOGSA. Key metrics such as Best Fitness, Mean Accuracy, Standard Deviation (SD), Sensitivity, Specificity, Precision, F1 Score, Kappa Statistic, Root Mean Squared Error (RMSE), and Misclassification Error Rate (MER) were used to assess wins, ties, and losses. Wins: 7 wins Ties: 2 cases Losses: no significant losses QIGPSO: QPSO: GSA and PSO: QGSA: GPSOGSA: PSOGSA: The QIGPSO algorithm’s ability to balance exploration and exploitation was evident from the results, showcasing its superiority in multiple aspects: • Classification Accuracy: QIGPSO consistently delivered higher mean accuracy, improving results by up to 5% over competing algorithms. • Feature Selection: It demonstrated efficient dimensionality reduction, selecting the minimal yet most effective feature subsets to maintain computational efficiency and high accuracy. • Convergence Rate: QIGPSO showed a faster convergence rate than baseline algorithms such as PSO and GSA, enabling efficient optimization while exploring the search space comprehensively. Performance comparison against existing methods The performance evaluation of the proposed optimization algorithm was conducted against existing methods, including DLO 58 61 79 1 15 Table 15 Parameter setting of methods. Methods Parameter Values QIGPSO Swarm Size = 47, Inertia Weight = 0.46, Cognitive Learning (c1) = 0.92, Social Learning (c2) = 0.98, Maximum Velocity = 0.55, Gravitational Constant = 1350, Mass Reduction Coefficient = 0.94 QANA Population size (N): 200 The number of flocks ( k K K QAOA Circuit depth ( p γ, β DLO Population size (N): 50, Maximum iterations (T max p β σ QIGPSO consistently had the fastest and most stable convergence across the benchmark suite F1 to F60. QIGPSO avoided premature convergence and found near-global optima faster than the others by maintaining exploration while swiftly investigating attractive regions in simpler unimodal landscapes and many shifted or rotated multimodal challenges. Its success is due to its excellent global–local search balance. In hybrid functions like F20, F30 and F57–F60 where the search landscape was extremely complex with flat or deceptive sections that slowed convergence or raised solution variance, QIGPSO performed slightly less best. Due to its adaptive step size and aggressive local refinement, QANA quickly exploited local basins of attraction on many shifted or complex multimodal functions like F13–F24 and F49–F54, but it sometimes exceed in rugged functions like F35–F40, causing convergence curve oscillations and higher standard deviations. QAOA performed best on simpler unimodal functions like F1–F8 and F49–F52, where its quantum-inspired sampling accelerated convergence, but it lagged significantly on highly deceptive, shifted, or composite functions like F27, F32, and F53–F60, where its limited ability to escape local optima and dependence on initialization slowed convergence. DLO had the slowest and least stable convergence, notably in complicated and multimodal functions over F21–F60, where its local search could not navigate deceiving basins or rocky landscapes. Local search worked well for basic functions like F1–F3 because there were no major local minima. QIGPSO’s effective balance between exploration and exploitation led to faster convergence on most functions, especially in high-dimensional, shifted, and rotated cases, while QANA’s rapid local search excelled near optima when precision was critical and QAOA’s moderate performance showed its need for better global exploration. DLO struggled with complex environments across the board. Figure 12 Fig. 12 Exploration and Exploitation capabilities of QIGPSO Algorithm. QIGPSO, QAOA, QANA, and DLO search differently, as shown in the Fig. 13 Fig. 13 Performance Comparison of QIGPSO Algorithm across benchmark functions. The Performance Index (PI) analysis in Fig. 14 Fig. 14 Performance Index Analysis of QIGPSO Algorithm across benchmark functions. Conclusion The study presents a unique approach for diagnosing NCDs known as QIGPSO. It has an absolute Due to its absolute Gaussian parameter; it can quickly find local attractors and global optimum placements. We tackle the premature convergence and local opima problems of QPSO and QGSA. It reduces dependence on the two acceleration factors. A number of evaluation measures are used to evaluate the SVM learner algorithm against four conventional and two hybrid techniques. QIGPSO swiftly finds global optima while avoiding local optima. Due to its significant characteristics, excellent classification accuracy, and little misclassification, QIGPSO has a perfect F1-score. These findings are notable for all classifiers. Despite the limited individual performance of QPSO and QGSA, their hybridization yields promising results, allowing for more meaningful predictions. However, QIGPSO’s performance may be sensitive to parameter settings, and its complexity may present scalability issues for large-scale problems. It effectively finds relevant features and is effective in diagnosing NCDs. It can be used as a reliable classifier in a variety of domains, providing high convergence rates and classification accuracy. This unique hybrid method, which combines quantum-inspired principles, demonstrates the potential for developing population-based optimization techniques and broadens their applications beyond usual optimization issues. Supplementary Information  Supplementary Information. Abbreviations ANN Artificial neural network DLO Draco lizard optimizer ELO Eurasian lynx optimizer GPD Gaussian probability distribution GPSOGSA Gaussian-based particle swarm optimization gravitational search algorithm GSA Gravitational search algorithm NCDs Non-communicable diseases PDF Probability density function PSNR Peak signal-to-noise ratio PSO Particle swarm optimization PSOGSA Particle swarm optimization and gravitational search algorithm QGSA Quantum gravitational search algorithm QIGPSO Quantum-inspired gravitationally guided particle swarm optimization QIGPSO Quantum behaved PSOGSA QPSO Quantum particle swarm optimization QPSO Quantum particle swarm optimization RBF Radial basis function SMOTE Synthetic minority over-sampling technique SVM Support vector machine WBCD Wisconsin breast cancer diagnosis Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-025-14793-4. Acknowledgements \"The authors extend their appreciation to the Deanship of Research and Graduate Studies at King Khalid University for funding this work through Large Research Project under grant number RGP2/541/46. Author contributions “Conceptualization, S.M, S.G.K.P; Writing—Review and Editing, C.M, A.L; Methodology, Q.N.N, A.B; Supervision, A.F.E, S.K, A.M. Data availability The data that supports the findings of this study are available within the article. Code availability The code used in the study would be made available upon reasonable request. Declarations Competing interests The authors declare no competing interests. Ethical approval This article does not contain any studies with human participants or animals performed by any of the authors. References 1. Sangaiah I Vincent Antony Kumar A Improving medical diagnosis performance using hybrid feature selection via relieff and entropy based genetic search (RF-EGA) approach: Application to breast cancer prediction Cluster Comput. 2019 22 3 6899 6906 10.1007/s10586-018-1702-5 Sangaiah, I. & Vincent Antony Kumar, A. Improving medical diagnosis performance using hybrid feature selection via relieff and entropy based genetic search (RF-EGA) approach: Application to breast cancer prediction. Cluster Comput. 22 2. Abualigah L Dulaimi AJ A novel feature selection method for data mining tasks using hybrid sine cosine algorithm and genetic algorithm Cluster Comput. 2021 24 2161 2176 10.1007/s10586-021-03254-y Abualigah, L. & Dulaimi, A. J. A novel feature selection method for data mining tasks using hybrid sine cosine algorithm and genetic algorithm. Cluster Comput. 24 3. Madhusudhanan B Sumathi P Karpagam NS An hybrid metaheuristic approach for efficient feature selection Cluster Comput. 2019 22 6 14541 14549 10.1007/s10586-018-2337-2 Madhusudhanan, B. et al. An hybrid metaheuristic approach for efficient feature selection. Cluster Comput. 22 4. Utami DA Rustam Z Gene selection in cancer classification using hybrid method based on Particle Swarm Optimization (PSO), Artificial Bee Colony (ABC) feature selection and support vector machine AIP Conf. Proc. 2019 10.1063/1.5132474 Utami, D. A. & Rustam, Z. Gene selection in cancer classification using hybrid method based on Particle Swarm Optimization (PSO), Artificial Bee Colony (ABC) feature selection and support vector machine. AIP Conf. Proc. 5. Joseph Manoj R Anto Praveena MD Vijayakumar K An ACO–ANN based feature selection algorithm for big data Cluster Comput. 2019 22 2 3953 3960 10.1007/s10586-018-2550-z Joseph Manoj, R., Anto Praveena, M. D. & Vijayakumar, K. An ACO–ANN based feature selection algorithm for big data. Cluster Comput. 22 6. Vijaya J Sivasankar E An efficient system for customer churn prediction through particle swarm optimization based feature selection model with simulated annealing Cluster Comput. 2019 22 5 10757 10768 10.1007/s10586-017-1172-1 Vijaya, J. & Sivasankar, E. An efficient system for customer churn prediction through particle swarm optimization based feature selection model with simulated annealing. Cluster Comput. 22 7. Ahishakiye E Kanobe F Breast cancer classification using breast ultrasound images with a hybrid of transfer learning and Bayesian-optimized fast learning network Discov. Artif. Intell. 2025 5 81 10.1007/s44163-025-00335-4 Ahishakiye, E. & Kanobe, F. Breast cancer classification using breast ultrasound images with a hybrid of transfer learning and Bayesian-optimized fast learning network. Discov. Artif. Intell. 5 8. Kumar Y Koul A Singla R Artificial intelligence in disease diagnosis: a systematic literature review, synthesizing framework and future research agenda J. Ambient. Intell. Hum. Comput. 2023 14 8459 8486 10.1007/s12652-021-03612-z PMC8754556 35039756 Kumar, Y. et al. Artificial intelligence in disease diagnosis: a systematic literature review, synthesizing framework and future research agenda. J. Ambient. Intell. Hum. Comput. 14 10.1007/s12652-021-03612-z PMC8754556 35039756 9. Vithya Ganesan M Sobhana GA Pachipala Yellamma O Devi R Prakash KB Naren J Quantum inspired meta-heuristic approach for optimization of genetic algorithm Comput. Electrical Eng. 2021 94 107356 10.1016/j.compeleceng.2021.107356 Vithya Ganesan, M. et al. Quantum inspired meta-heuristic approach for optimization of genetic algorithm. Comput. Electrical Eng. 94 10. Ghaffar Nia N Kaplanoglu E Nasab A Evaluation of artificial intelligence techniques in disease diagnosis and prediction Discov. Artif. Intell. 2023 3 5 10.1007/s44163-023-00049-5 40478140 PMC9885935 Ghaffar Nia, N., Kaplanoglu, E. & Nasab, A. Evaluation of artificial intelligence techniques in disease diagnosis and prediction. Discov. Artif. Intell. 3 40478140 10.1007/s44163-023-00049-5 PMC9885935 11. Malik S Patro SGK Mahanty C Hybrid metaheuristic optimization for detecting and diagnosing noncommunicable diseases Sci. Rep. 2025 15 7816 10.1038/s41598-025-91136-3 40050658 PMC11885463 Malik, S. et al. Hybrid metaheuristic optimization for detecting and diagnosing noncommunicable diseases. Sci. Rep. 15 40050658 10.1038/s41598-025-91136-3 PMC11885463 12. Behera MP Sarangi A Mishra D Sarangi SK A hybrid machine learning algorithm for heart and liver disease prediction using modified particle swarm optimization with support vector machine Procedia Comput. Sci. 2023 218 818 827 10.1016/j.procs.2023.01.062 Behera, M. P., Sarangi, A., Mishra, D. & Sarangi, S. K. A hybrid machine learning algorithm for heart and liver disease prediction using modified particle swarm optimization with support vector machine. Procedia Comput. Sci. 218 13. Mirbabaie M Stieglitz S Frick NRJ Artificial intelligence in disease diagnostics: A critical review and classification on the current state of research guiding future direction Health Technol. 2021 11 693 731 10.1007/s12553-021-00555-5 Mirbabaie, M., Stieglitz, S. & Frick, N. R. J. Artificial intelligence in disease diagnostics: A critical review and classification on the current state of research guiding future direction. Health Technol. 11 14. Budhi GS Chiong R Dhakal S Multi-level particle swarm optimisation and its parallel version for parameter optimisation of ensemble models: A case of sentiment polarity prediction Cluster Comput. 2020 23 3371 3386 10.1007/s10586-020-03093-3 Budhi, G. S., Chiong, R. & Dhakal, S. Multi-level particle swarm optimisation and its parallel version for parameter optimisation of ensemble models: A case of sentiment polarity prediction. Cluster Comput. 23 15. Karadayı Ataş P A novel clustered-based binary grey wolf optimizer to solve the feature selection problem for uncovering the genetic links between non-Hodgkin lymphomas and rheumatologic diseases Health Inf. Sci. Syst. 2025 13 34 10.1007/s13755-025-00350-w 40321894 PMC12048384 Karadayı Ataş, P. A novel clustered-based binary grey wolf optimizer to solve the feature selection problem for uncovering the genetic links between non-Hodgkin lymphomas and rheumatologic diseases. Health Inf. Sci. Syst. 13 40321894 10.1007/s13755-025-00350-w PMC12048384 16. Malar B Nadarajan R Gowri Thangam J A hybrid isotonic separation training algorithm with correlation-based isotonic feature selection for binary classification Knowl. Inf. Syst. 2019 59 651 683 10.1007/s10115-018-1226-6 Malar, B., Nadarajan, R. & Gowri Thangam, J. A hybrid isotonic separation training algorithm with correlation-based isotonic feature selection for binary classification. Knowl. Inf. Syst. 59 17. Hakemi S Houshmand M KheirKhah E A review of recent advances in quantum-inspired metaheuristics Evol. Intel. 2024 17 627 642 10.1007/s12065-022-00783-2 PMC9589576 36312203 Hakemi, S. et al. A review of recent advances in quantum-inspired metaheuristics. Evol. Intel. 17 10.1007/s12065-022-00783-2 PMC9589576 36312203 18. Kurman S Kisan S An in-depth and contrasting survey of meta-heuristic approaches with classical feature selection techniques specific to cervical cancer Knowl. Inf. Syst 2023 65 1881 1934 10.1007/s10115-022-01825-y Kurman, S. & Kisan, S. An in-depth and contrasting survey of meta-heuristic approaches with classical feature selection techniques specific to cervical cancer. Knowl. Inf. Syst 65 19. Rastogi R Bansal M Diabetes prediction model using data mining techniques Meas. Sens. 2023 25 100605 10.1016/j.measen.2022.100605 Rastogi, R. & Bansal, M. Diabetes prediction model using data mining techniques. Meas. Sens. 25 20. Cerrada M Aguilar J Altamiranda J A hybrid heuristic algorithm for evolving models in simultaneous scenarios of classification and clustering Knowl. Inf. Syst. 2019 61 755 798 10.1007/s10115-019-01336-3 Cerrada, M. et al. A hybrid heuristic algorithm for evolving models in simultaneous scenarios of classification and clustering. Knowl. Inf. Syst. 61 21. Lin SW Chen SC Wu WJ Parameter determination and feature selection for back-propagation network by particle swarm optimization Knowl. Inf. Syst. 2009 21 249 266 10.1007/s10115-009-0242-y Lin, S. W. et al. Parameter determination and feature selection for back-propagation network by particle swarm optimization. Knowl. Inf. Syst. 21 22. Saxena S Saxena S Tahilramani N Blockchain enhanced smart healthcare management for chronic diseases Discov. Comput. 2025 28 112 10.1007/s10791-025-09574-6 Saxena, S. et al. Blockchain enhanced smart healthcare management for chronic diseases. Discov. Comput. 28 23. Challapalli JR Devarakonda N A novel approach for optimization of convolution neural network with hybrid particle swarm and grey wolf algorithm for classification of Indian classical dances Knowl. Inf. Syst. 2022 64 2411 2434 10.1007/s10115-022-01707-3 35919768 PMC9333353 Challapalli, J. R. & Devarakonda, N. A novel approach for optimization of convolution neural network with hybrid particle swarm and grey wolf algorithm for classification of Indian classical dances. Knowl. Inf. Syst. 64 35919768 10.1007/s10115-022-01707-3 PMC9333353 24. Neumann U Compensation of feature selection biases accompanied with improved predictive performance for binary classification by using a novel ensemble feature selection approach BioData 2016 10.1186/s13040-016 PMC5116216 27891179 Neumann, U. et al. Compensation of feature selection biases accompanied with improved predictive performance for binary classification by using a novel ensemble feature selection approach. BioData 10.1186/s13040-016-0114-4 PMC5116216 27891179 25. Khan TA Ling SH A novel hybrid gravitational search particle swarm optimization algorithm Eng. Appl. Artif. Intell. 2021 102 104263 10.1016/j.engappai.2021.104263 Khan, T. A. & Ling, S. H. A novel hybrid gravitational search particle swarm optimization algorithm. Eng. Appl. Artif. Intell. 102 26. Hernandez JG Saini AK Ghosh A Moore JH The tree-based pipeline optimization tool: Tackling biomedical research problems with genetic programming and automated machine learning Patterns. 2025 6 7 101314 10.1016/j.patter.2025.101314 40926965 PMC12416094 Hernandez, J. G., Saini, A. K., Ghosh, A. & Moore, J. H. The tree-based pipeline optimization tool: Tackling biomedical research problems with genetic programming and automated machine learning. Patterns. 6 40926965 10.1016/j.patter.2025.101314 PMC12416094 27. Dey A Bhattacharyya S Dey S Konar D Platos J Snasel V Mrsic L Pal P A review of quantum-inspired metaheuristic algorithms for automatic clustering Mathematics. 2023 11 9 2018 10.3390/math11092018 Dey, A. et al. A review of quantum-inspired metaheuristic algorithms for automatic clustering. Mathematics. 11 28. Jalili A Saleki Z Luo YA Performance of various kernel functions for mass prediction with support vector machine Eur. Phys. J. A 2025 61 143 10.1140/epja/s10050-025-01610-9 Jalili, A. et al. Performance of various kernel functions for mass prediction with support vector machine. Eur. Phys. J. A 61 29. Elgin Christo VR Correlation-based ensemble feature selection using bioinspired algorithms and classification using backpropagation neural network Comput. Math Mod. Med. 2019 10.1155/2019/7398307 PMC6778924 31662787 Elgin Christo, V. R. et al. Correlation-based ensemble feature selection using bioinspired algorithms and classification using backpropagation neural network. Comput. Math Mod. Med. 10.1155/2019/7398307 PMC6778924 31662787 30. Arfiani A Rustam Z Ovarian cancer data classification using bagging and random forest AIP Conf. Proc. 2019 10.1063/1.5132473 Arfiani, A. & Rustam, Z. Ovarian cancer data classification using bagging and random forest. AIP Conf. Proc. 31. Elhoseny, M. B. (2018). Effective Features to Classify Ovarian Cancer Data in Internet of Medical Things. Preprints 2018. 10.20944/preprints 201809.0390.v1 32. Siet JJW Tan XJ Cheor WL A comprehensive review of tubule formation in histopathology images: Advancement in tubule and tumor detection techniques Artif. Intell. Rev. 2024 57 286 10.1007/s10462-024-10887-z Siet, J. J. W. et al. A comprehensive review of tubule formation in histopathology images: Advancement in tubule and tumor detection techniques. Artif. Intell. Rev. 57 33. Del Parigi A Tang W Liu D Machine learning to identify predictors of glycemic control in type 2 diabetes: An analysis of target HbA1c reduction using empagliflozin/linagliptin data Pharm. Med. 2019 33 209 217 10.1007/s40290-019-00281-4 31933292 Del Parigi, A. et al. Machine learning to identify predictors of glycemic control in type 2 diabetes: An analysis of target HbA1c reduction using empagliflozin/linagliptin data. Pharm. Med. 33 10.1007/s40290-019-00281-4 31933292 34. Vincent PC Magboo M Magboo SA Machine learning classifiers on breast cancer recurrences Procedia Comput. Sci. 2021 192 2742 2752 10.1016/j.procs.2021.09.044 Vincent, P. C., Magboo, M. & Magboo, S. A. Machine learning classifiers on breast cancer recurrences. Procedia Comput. Sci. 192 35. Reséndiz-Flores EO Navarro-Acosta JA Hernández-Martínez A Optimal feature selection in industrial foam injection processes using hybrid binary particle swarm optimization and gravitational search algorithm in the Mahalanobis-Taguchi system Soft Comput. 2020 24 341 349 10.1007/s00500-019-03911-w Reséndiz-Flores, E. O., Navarro-Acosta, J. A. & Hernández-Martínez, A. Optimal feature selection in industrial foam injection processes using hybrid binary particle swarm optimization and gravitational search algorithm in the Mahalanobis-Taguchi system. Soft Comput. 24 36. Gauthama Raman MR Nivethitha S Kannan K A hybrid approach using rough set theory and hypergraph for feature selection on high-dimensional medical datasets Soft Comput. 2019 23 12655 12672 10.1007/s00500-019-03818-6 Gauthama Raman, M. R. et al. A hybrid approach using rough set theory and hypergraph for feature selection on high-dimensional medical datasets. Soft Comput. 23 37. Yaqoob A Verma NK Mir MA SGA-Driven feature selection and random forest classification for enhanced breast cancer diagnosis: A comparative study Sci. Rep. 2025 15 10944 10.1038/s41598-025-95786-1 40159513 PMC11955515 Yaqoob, A. et al. SGA-Driven feature selection and random forest classification for enhanced breast cancer diagnosis: A comparative study. Sci. Rep. 15 40159513 10.1038/s41598-025-95786-1 PMC11955515 38. Amethiya Y Pipariya P Patel S Shah M Comparative analysis of breast cancer detection using machine learning and biosensors Intell. Med. 2022 2 2 69 81 10.1016/j.imed.2021.08.004 Amethiya, Y., Pipariya, P., Patel, S. & Shah, M. Comparative analysis of breast cancer detection using machine learning and biosensors. Intell. Med. 2 39. D’Urso P De Giovanni L Alaimo LS Fuzzy clustering with entropy regularization for interval-valued data with an application to scientific journal citations Ann. Oper. Res. 2024 342 1605 1628 10.1007/s10479-023-05180-1 D’Urso, P. et al. Fuzzy clustering with entropy regularization for interval-valued data with an application to scientific journal citations. Ann. Oper. Res. 342 40. Hameed SS Hassan WH Latiff LA A comparative study of nature-inspired metaheuristic algorithms using a three-phase hybrid approach for gene selection and classification in high-dimensional cancer datasets Soft Comput. 2021 25 8683 8701 10.1007/s00500-021-05726-0 Hameed, S. S. et al. A comparative study of nature-inspired metaheuristic algorithms using a three-phase hybrid approach for gene selection and classification in high-dimensional cancer datasets. Soft Comput. 25 41. Mafarja MM Mirjalili S Hybrid binary ant lion optimizer with rough set and approximate entropy reducts for feature selection Soft Comput. 2019 23 6249 6265 10.1007/s00500-018-3282-y Mafarja, M. M. & Mirjalili, S. Hybrid binary ant lion optimizer with rough set and approximate entropy reducts for feature selection. Soft Comput. 23 42. Meenachi L Ramakrishnan S Differential evolution and ACO based global optimal feature selection with fuzzy rough set for cancer data classification Soft Comput. 2020 24 18463 18475 10.1007/s00500-020-05070-9 Meenachi, L. & Ramakrishnan, S. Differential evolution and ACO based global optimal feature selection with fuzzy rough set for cancer data classification. Soft Comput. 24 43. Ding Y Zhou K Bi W Feature selection based on hybridization of genetic algorithm and competitive swarm optimizer Soft Comput. 2020 24 11663 11672 10.1007/s00500-019-04628-6 Ding, Y., Zhou, K. & Bi, W. Feature selection based on hybridization of genetic algorithm and competitive swarm optimizer. Soft Comput. 24 44. Chakraborty S Shaikh S Chakrabarti A Ghosh R A hybrid quantum feature selection algorithm using a quantum inspired graph theoretic approach Appl. Intell. 2020 50 1775 1793 10.1007/s10489-019-01604-3 Chakraborty, S., Shaikh, S., Chakrabarti, A. & Ghosh, R. A hybrid quantum feature selection algorithm using a quantum inspired graph theoretic approach. Appl. Intell. 50 45. Arif M Rehman F Sekanina L Malik A A comprehensive survey of evolutionary algorithms and metaheuristics in brain EEG-based applications J. Neural Eng. 2024 10.1088/1741-2552/ad7f8e 39321840 Arif, M., Rehman, F., Sekanina, L. & Malik, A. A comprehensive survey of evolutionary algorithms and metaheuristics in brain EEG-based applications. J. Neural Eng. 39321840 10.1088/1741-2552/ad7f8e 46. Cao B Fan S Zhao J Yang P Muhammad K Tanveer M Quantum-enhanced multiobjective large-scale optimization via parallelism Swarm Evol. Comput. 2020 57 100697 10.1016/j.swevo.2020.100697 Cao, B. et al. Quantum-enhanced multiobjective large-scale optimization via parallelism. Swarm Evol. Comput. 57 47. Acharya, S. (2021, May 14). https://towardsdatascience.com/what-are-rmse-and-mae-e405ce230383 https://towardsdatascience.com 48. Alvarez-Alvarado MA-C-R Three novel quantum-inspired swarm optimization algorithms using different bounded potential fields Sci. Rep. 2021 11 11655 10.1038/s41598-021-90847-7 34078967 PMC8172946 Alvarez-Alvarado, M.A.-C.-R. Three novel quantum-inspired swarm optimization algorithms using different bounded potential fields. Sci. Rep. 11 34078967 10.1038/s41598-021-90847-7 PMC8172946 49. Chakraborty SS A hybrid quantum feature selection algorithm using a quantum inspired graph theoretic approach Appl. Intell. 2020 10.1007/s10489-019-01604-3 Chakraborty, S. S. A hybrid quantum feature selection algorithm using a quantum inspired graph theoretic approach. Appl. Intell. 50. Chelly Dagdia ZZ A scalable and effective rough set theory-based approach for big data pre-processing Knowl. Inf. Syst. 2020 62 3321 3386 10.1007/s10115-020-01467-y Chelly Dagdia, Z. Z. A scalable and effective rough set theory-based approach for big data pre-processing. Knowl. Inf. Syst. 62 51. Chen R Selecting critical features for data classification based on machine learning methods J. Big Data 2020 10.1186/s40537-020-00327-4 Chen, R. et al. Selecting critical features for data classification based on machine learning methods. J. Big Data 52. Chen Y Chen Y Feature subset selection based on variable precision neighborhood rough sets Int. J. Comput. Intell. Syst. 2021 14 1 572 581 10.2991/ijcis.d.210106.003 Chen, Y. & Chen, Y. Feature subset selection based on variable precision neighborhood rough sets. Int. J. Comput. Intell. Syst. 14 53. Schmidt N Global convergence of Newton’s method for the regularized p-Stokes equations Numer. Algor. 2025 99 2011 2038 10.1007/s11075-024-01941-6 Schmidt, N. Global convergence of Newton’s method for the regularized p-Stokes equations. Numer. Algor. 99 54. Xin-gang Z Ji L Jin M Ying Z An improved quantum particle swarm optimization algorithm for environmental economic dispatch Expert Syst. Appl. 2020 152 113370 10.1016/j.eswa.2020.113370 Xin-gang, Z., Ji, L., Jin, M. & Ying, Z. An improved quantum particle swarm optimization algorithm for environmental economic dispatch. Expert Syst. Appl. 152 55. Latchoumi TP Ezhilarasi TP Balamurugan K Bio-inspired weighed quantum particle swarm optimization and smooth support vector machine ensembles for identification of abnormalities in medical data SN Appl. Sci. 2019 1 10 1137 10.1007/s42452-019-1179-8 Latchoumi, T. P., Ezhilarasi, T. P. & Balamurugan, K. Bio-inspired weighed quantum particle swarm optimization and smooth support vector machine ensembles for identification of abnormalities in medical data. SN Appl. Sci. 1 56. Abd-El-Atty B A robust medical image steganography approach based on particle swarm optimization algorithm and quantum walks Neural Comput. Appl. 2023 35 1 773 785 10.1007/s00521-022-07830-0 Abd-El-Atty, B. A robust medical image steganography approach based on particle swarm optimization algorithm and quantum walks. Neural Comput. Appl. 35 57. Yadav P Sharma SC Mahadeva R Patole SP Exploring hyper-parameters and feature selection for predicting non-communicable chronic disease using stacking classifier IEEE Access 2023 10.1109/ACCESS.2023.3299332 Yadav, P., Sharma, S. C., Mahadeva, R. & Patole, S. P. Exploring hyper-parameters and feature selection for predicting non-communicable chronic disease using stacking classifier. IEEE Access 58. Wang X Draco lizard optimizer: A novel metaheuristic algorithm for global optimization problems Evol. Intel. 2025 18 1 1 20 10.1007/s12065-024-00998-5 Wang, X. Draco lizard optimizer: A novel metaheuristic algorithm for global optimization problems. Evol. Intel. 18 59. Wang X Eurasian lynx optimizer: A novel metaheuristic optimization algorithm for global optimization and engineering applications Phys. Scr. 2024 99 11 115275 10.1088/1402-4896/ad86f7 Wang, X. Eurasian lynx optimizer: A novel metaheuristic optimization algorithm for global optimization and engineering applications. Phys. Scr. 99 60. Vivek, Y., Ravi, V., & Krishna, P. R. (2024). Quantum-Inspired Evolutionary Algorithms for Feature Subset Selection: A Comprehensive Survey. arXiv preprint arXiv:2407.17946 61. Turati, G., Dacrema, M. F., & Cremonesi, P. (2022, September). Feature selection for classification with QAOA. In 2022 IEEE International Conference on Quantum Computing and Engineering (QCE) 62. Mandal, A. K., Sen, R., Goswami, S., Chakrabarti, A., & Chakraborty, B. (2020, September). A new approach for feature subset selection using quantum inspired owl search algorithm. In 2020 10th International Conference on Information Science and Technology (ICIST) 63. Gao P Li K Wei S Quantum second-order optimization algorithm for general polynomials Sci. China Phys. Mech. Astron. 2021 64 100311 10.1007/s11433-021-1725-9 Gao, P. et al. Quantum second-order optimization algorithm for general polynomials. Sci. China Phys. Mech. Astron. 64 64. Secui DC Secui ML Social small group optimization algorithm for large-scale economic dispatch problem with valve-point effects and multi-fuel sources Appl. Intell. 2024 54 8296 8346 10.1007/s10489-024-05517-8 Secui, D. C. & Secui, M. L. Social small group optimization algorithm for large-scale economic dispatch problem with valve-point effects and multi-fuel sources. Appl. Intell. 54 65. Li T Liu F Chen X Web log mining techniques to optimize Apriori association rule algorithm in sports data information management Sci. Rep. 2024 14 24099 10.1038/s41598-024-74427-z 39406842 PMC11480465 Li, T. et al. Web log mining techniques to optimize Apriori association rule algorithm in sports data information management. Sci. Rep. 14 39406842 10.1038/s41598-024-74427-z PMC11480465 66. Bhatia AS Saggi MK Zheng S QPSO-CD: Quantum-behaved particle swarm optimization algorithm with Cauchy distribution Quantum Inf. Process 2020 19 345 10.1007/s11128-020-02842-y Bhatia, A. S., Saggi, M. K. & Zheng, S. QPSO-CD: Quantum-behaved particle swarm optimization algorithm with Cauchy distribution. Quantum Inf. Process 19 67. Chang D Rao C Xiao X Fuyan H Goh M Multiple strategies based Grey Wolf Optimizer for feature selection in performance evaluation of open-ended funds Swarm Evol. Comput. 2024 86 101518 10.1016/j.swevo.2024.101518 Chang, D., Rao, C., Xiao, X., Fuyan, H. & Goh, M. Multiple strategies based Grey Wolf Optimizer for feature selection in performance evaluation of open-ended funds. Swarm Evol. Comput. 86 68. Loglisci C Impedovo A Calders T Heuristic approaches for non-exhaustive pattern-based change detection in dynamic networks J. Intell. Inf. Syst. 2024 62 1455 1492 10.1007/s10844-024-00866-9 Loglisci, C. et al. Heuristic approaches for non-exhaustive pattern-based change detection in dynamic networks. J. Intell. Inf. Syst. 62 69. Salgotra R Sharma P Raju S A contemporary systematic review on meta-heuristic optimization algorithms with their MATLAB and python code reference Arch. Computat. Methods Eng. 2024 31 1749 1822 10.1007/s11831-023-10030-1 Salgotra, R. et al. A contemporary systematic review on meta-heuristic optimization algorithms with their MATLAB and python code reference. Arch. Computat. Methods Eng. 31 70. Kumar P Thakur R Liver disorder detection using variable- neighbor weighted fuzzy K nearest neighbor approach Multimed. Tools Appl. 2020 10.1007/s11042-019-07978-3 Kumar, P. & Thakur, R. Liver disorder detection using variable- neighbor weighted fuzzy K nearest neighbor approach. Multimed. Tools Appl. 71. Tao Z HuilingWenwen LW Xia Y GA-SVM based feature selection and parameter optimization in hospitalization expense modeling Appl. Soft Comput. 2019 75 323 332 10.1016/j.asoc.2018.11.001 Tao, Z., HuilingWenwen, L. W. & Xia, Y. GA-SVM based feature selection and parameter optimization in hospitalization expense modeling. Appl. Soft Comput. 75 72. LiTangWuFan YLGJ RoughPSO: Rough set-based particle swarm optimisation Int. J. Bio-Inspired Comput. 2018 12 245 10.1504/IJBIC.2018.10017835 LiTangWuFan, Y. L. G. J. RoughPSO: Rough set-based particle swarm optimisation. Int. J. Bio-Inspired Comput. 12 73. Sneha NG Analysis of diabetes mellitus for early prediction using optimal features selection J. Big Data 2019 10.1186/s40537-019-0175-6 Sneha, N. G. Analysis of diabetes mellitus for early prediction using optimal features selection. J. Big Data 74. https://archive.ics.uci.edu/dataset/45/heart+disease 75. https://archive.ics.uci.edu/dataset/336/chronic+kidney+disease 76. https://archive.ics.uci.edu/dataset/174/parkinsons 77. https://www.kaggle.com/datasets/mathchi/diabetes-data-set 78. https://archive.ics.uci.edu/dataset/102/thyroid+disease 79. Zamani H Nadimi-Shahraki MH Gandomi AH QANA: Quantum-based avian navigation optimizer algorithm Eng. Appl. Artif. Intell. 2021 104 104314 10.1016/j.engappai.2021.104314 Zamani, H., Nadimi-Shahraki, M. H. & Gandomi, A. H. QANA: Quantum-based avian navigation optimizer algorithm. Eng. Appl. Artif. Intell. 104 80. M. A. Rahman, R. C. (2019). Ovarian Cancer Classification Accuracy Analysis Using 15-Neuron Artificial Neural Networks Model. 2019 IEEE Student Conference on Research and Development (SCOReD) (pp. 33–38). Malaysia: IEEE. 10.1109/SCORED.2019.8896332 ",
  "metadata": {
    "Title of this paper": "QANA: Quantum-based avian navigation optimizer algorithm",
    "Journal it was published in:": "Scientific Reports",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12488977/"
  }
}
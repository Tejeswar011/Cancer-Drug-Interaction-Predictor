{
  "title": "Paper_690",
  "abstract": "pmc PLoS One PLoS One 440 plosone plos PLOS One 1932-6203 PLOS PMC12478912 PMC12478912.1 12478912 12478912 41021608 10.1371/journal.pone.0332869 PONE-D-25-24826 1 Research Article Medicine and Health Sciences Diagnostic Medicine Diagnostic Radiology Magnetic Resonance Imaging Research and Analysis Methods Imaging Techniques Diagnostic Radiology Magnetic Resonance Imaging Medicine and Health Sciences Radiology and Imaging Diagnostic Radiology Magnetic Resonance Imaging Research and Analysis Methods Imaging Techniques Physical Sciences Mathematics Geometry Curvature Research and Analysis Methods Imaging Techniques Neuroimaging Computed Axial Tomography Biology and Life Sciences Neuroscience Neuroimaging Computed Axial Tomography Medicine and Health Sciences Diagnostic Medicine Diagnostic Radiology Tomography Computed Axial Tomography Research and Analysis Methods Imaging Techniques Diagnostic Radiology Tomography Computed Axial Tomography Medicine and Health Sciences Radiology and Imaging Diagnostic Radiology Tomography Computed Axial Tomography Biology and Life Sciences Anatomy Body Fluids Cerebrospinal Fluid Medicine and Health Sciences Anatomy Body Fluids Cerebrospinal Fluid Biology and Life Sciences Physiology Body Fluids Cerebrospinal Fluid Biology and Life Sciences Anatomy Nervous System Cerebrospinal Fluid Medicine and Health Sciences Anatomy Nervous System Cerebrospinal Fluid Physical Sciences Physics Thermodynamics Entropy Medicine and Health Sciences Diagnostic Medicine Research and Analysis Methods Imaging Techniques Neuroimaging Biology and Life Sciences Neuroscience Neuroimaging Structure-aware medical image fusion via mean curvature enhancement in the contourlet domain Structure-aware medical image fusion using mean curvature and contourlet transform https://orcid.org/0009-0009-6145-1589 Sharma Shweta Methodology Validation Writing – original draft  1 Rani Shalli Formal analysis Supervision Writing – review & editing  1 Dogra Ayush Conceptualization Data curation Supervision  1 https://orcid.org/0000-0001-5106-7609 Shabaz Mohammad Validation Writing – review & editing  2 * 1 Chitkara University Institute of Engineering and Technology, Chitkara University, Rajpura, Punjab, India 2 Marwadi University Research Center, Department of Computer Engineering, Faculty of Engineering and Technology, Marwadi University, Rajkot, 360003, Gujarat, India Xu Lin Editor  Chengdu University of Traditional Chinese Medicine Wenjiang Campus: Chengdu University of Traditional Chinese Medicine, CHINA Competing Interests: * E-mail: bhatsab4@gmail.com 29 9 2025 2025 20 9 496058 e0332869 10 5 2025 6 9 2025 29 09 2025 30 09 2025 01 10 2025 © 2025 Sharma et al 2025 Sharma et al https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License The medical image fusion is a critical application in medical diagnosis, where anatomical and functional information from different imaging modalities, e.g., Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) can be integrated. However, edge preservation, texture richness and structure consistency are a major challenge in complex fusion scenarios. This paper presents a novel multimodal medical image fusion technique based on the Contourlet Transform for multiscale directional decomposition and mean curvature filter for edge preservation. The proposed approach decomposes the source images into low- frequency and high-frequency components via a three-level Contourlet Transform. The low-frequency layers are fused via weighted averaging for brightness consistency, while the detail layers are processed by the mean curvature filter and then fused via maximum absolute selection to maintain edges and texture. The approach was evaluated against a variety of multimodal medical image datasets with consistent improvements against conventional methods such as Guided Filter Fusion (GFF), Laplacian Pyramid (LP), and Discrete Wavelet Transform (DWT). Experimental results showed average improvement of 19.4% in Spatial Frequency (SF), 17.6% in Average Gradient (AG), and 13.2% in Entropy (EN) over baseline methods. The results demonstrate that the method is useful for medical applications such as brain tumor localization, tissue differentiation, and surgery planning where high fidelity within fused imaging is critical. The author(s) received no specific funding for this work. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes Data Availability All relevant data are located at Github: https://surl.lt/jcruxh Data Availability All relevant data are located at Github: https://surl.lt/jcruxh 1 Introduction Medical imaging is the foundation of diagnosis, treatment planning and surgical guidance for modern healthcare. CT, MRI, Positron Emission Tomography (PET), Ultrasound (US) and Single-Photon Emission Computed Tomography (SPECT) are among the technologies providing different perspectives on anatomy, functional, and molecular behaviors of the human body. Each method is therefore limited in its ability to fully gather all significant information [ 1 2 3 4 5 Medical image fusion has developed as a key method combining complementing features from several modalities into a single, more useful image to overcome these constraints. This approach guarantees that both anatomical and functional characteristics are preserved and shown concurrently, improves tissue contrast, and increases spatial resolution. By delivering richer visual representations, image fusion significantly supports decision-making, especially in scenarios such as tumor detection, neurodegenerative disease monitoring, vascular assessment, and image-guided interventions [ 2 Traditionally, image fusion methods have fused data using statistical modeling, multi-scale transformations, and pixel-level operations. Although somewhat effective, these techniques usually experience issues in noise suppression, detail preservation and maintaining structural integrity across modalities with varied resolutions [ 3 4 Newer developments like diffusion models and Generative Adversarial Networks (GANs) have created new paradigms for image fusion in addition to more conventional DL architectures. These approaches offer efficient solutions for noise-aware reconstruction, simultaneous fusion and augmentation, and high-resolution fusion. These models now make it possible to combine tri-modal data, expanding the quality of fused outputs in medical practice [ 5 Beyond the domain of DL, multimodal fusion is being more and more assisted by optimization-driven designs and invertible networks, which allow bidirectional mapping and lossless feature translation. These methods guarantee that fusion not only combines information but does so in a reversible and interpretable way, hence preserving the diagnostic value of source modalities [ 6 7 These advances are also accompanied by the expansion of smart image analysis systems where fusion is combined with downstream activities such as segmentation, classification and anomaly detection. Hierarchical multi-scale fusion networks have further strengthened the granularity of fused features, resulting in improved performance in classification and recognition tasks [ 8 9 The path of medical image fusion research is toward explainability, real-time capabilities and personalization as one looks forward. Future systems are expected to be more context-aware, combining patient history, information, and feedback to dynamically drive the fusion process [ 10 11 By means of the integration of thorough and diagnostically rich representations, medical image fusion helps to overcome the constraints of single imaging modalities. Medical image fusion is a revolutionary force in medical diagnostics with the inclusion of sophisticated DL, optimization algorithms and scalable structures, hence enabling more accurate, efficient, and individualized healthcare solutions. The remainder of this paper is organized as follows: Sect 2 Sect 3 Sect 4 Sect 5 Sect 6 Sect 7 Sect 8 Sect 9 Sect 10 1.1 Motivation Multimodal medical imaging, by merging structural and functional data from complementary modalities such as CT and MRI, is crucial for diagnosis. However, because of limitations in preserving directional attributes, edge details, and intricate anatomical textures, the effective integration of diverse modalities continues to pose challenges. Traditional wavelet-based methods lack the ability to capture geometrical structures effectively. The Nonsubsampled Contourlet Transform (NSCT), with its multiscale and multidirectional capabilities, has shown significant potential in medical image fusion. As demonstrated in earlier work, NSCT combined with advanced fusion rules such as spatial frequency or neural network models results in improved structural retention and contrast enhancement in fused outputs [ 12 1.2 Our contribution The main findings of this study are described below: By combining a multi-resolution Contourlet Transform with mean curvature-based filtering, a new medical image fusion approach is suggested to improve anatomical detail and visual contrast. The approach utilizes three-level Contourlet decomposition to obtain low and high-frequency components to best extract intensity and texture information from the input modalities. These low frequency sub-bands are combined with a weighted average strategy whereas a mean curvature filter is applied carefully to the high-frequency details of the third level and fused using maximum absolute rule. When assessed on several medical imaging datasets, the suggested fusion technique shows greater performance, hence outperforming current fusion techniques in visual contrast in the combined images. 2 Related work The following section discusses the work of various authors in the related field. A DL-based approach for multimodal medical image fusion and categorization was proposed by Veeraiah et al. [ 13 Liu et al. [ 14 Agrawal et al. [ 15 Lin et al. [ 16 Sinha et al. [ 17 Zhang et al. [ 18 Jie et al. [ 19 Tang et al. [ 20 Bavirisetti et al. [ 21 Bavirisetti et al. [ 22 Liu et al. [ 23 Zhu et al. [ 24 Kumar et al. [ 25 Li et al. [ 26 Kurban [ 27 Song et al. [ 28 Kumar et al. [ 29 30 31 32 33 34 35 Javed et al. [ 36 Abdullah et al. [ 37 Different multi-modal medical image fusion techniques have been attempted, ranging from DL and transformer models to traditional filtering and sparse coding. They all have their strengths, e.g., edge preservation, semantic highlight, or efficiency, and their trade-offs, e.g., high resource demands or image condition sensitivity. PCNN, guided filtering, and pixel-significance-based methods are robust in structural definition and visual enhancement. A concise overview of commonly used fusion methods, including their datasets, key contributions, and limitations, is provided in Table 1 10.1371/journal.pone.0332869.t001 Table 1 Summary of existing medical image fusion techniques. Ref. Method Dataset Used Key Contributions Limitations [ 14 CNN AANLIB Learns deep representations and adaptive fusion rules from data Requires large training datasets and complex model design [ 22 Two-Scale  Infrared + Visible Saliency-guided fusion of base and detail layers Reduced performance in low-contrast scenarios [ 23 LP AANLIB Simple pyramid decomposition preserving both low- and high-frequency components May suppress fine features and edges [ 24 NSCT AANLIB Directional multi-scale transform with spatial frequency-based fusion High computational cost; transform redundancy [ 25 CBF  http://www.imagefusion.org/ Edge-preserving fusion using pixel significance Sensitive to edge misalignment; lacks deep semantic guidance [ 26 GFF Multi-modal datasets Fast edge-aware fusion with low complexity Less effective under modality intensity variation [ 29 DCHWT  http://www.imagefusion.org/ Multiscale harmonic wavelet decomposition, good detail preservation May amplify noise; not adaptive to different image types 3 Preliminaries 3.1 Contourlet transform Contourlet Transform is a new form of representing images. It is designed to solve the problems of traditional wavelet transforms, which can not capture the direction and shape in images. Unlike wavelets that can only choose a few directions, the Contourlet Transform forms a more effective means of displaying smooth edges and lines through its capacity to decompose images at multiple directions and scales [ 38 3.1.1 Double filter bank structure. The Contourlet Transform consists of a two-stage filtering process: a multiscale decomposition using a Laplacian Pyramid (LP) followed by a directional decomposition using a Directional Filter Bank (DFB) as shown in Fig 1 39 10.1371/journal.pone.0332869.g001 Fig 1 Contourlet filter bank: Multiscale decomposition via LP followed by directional decomposition using DFB [ 39 Let f x y L 0 = f ( x , y ) (1) L i + 1 = Downsample ( L i * h ) , i = 0 , 1 , … , n − 1 (2) H i = L i − Upsample ( L i + 1 * h ) , i = 0 , 1 , … , n − 1 (3) Here, h L i i H i 3.1.2 Directional decomposition. Each high-pass detail component H i { D i , j } j = 1 2 k = DFB ( H i ) (4) where D i j i j k 3.1.3 Reconstruction. The reconstruction of the original image from Contourlet coefficients is achieved by first applying the inverse DFB on the directional subbands to reconstruct the band-pass component, followed by the inverse LP to synthesize the image across scales. The inverse LP can be written as: L ^ i = Upsample ( L i + 1 * h ) + H i (5) The final reconstruction is given by: f ^ ( x , y ) = L ^ 0 (6) The transform guarantees perfect reconstruction under ideal filter conditions, making it suitable for both analysis and synthesis tasks. 3.2 Curvature filter Designed to enhance visual structures while reducing noise and variational artifacts, curvature filters are advanced edge-preserving smoothing techniques. These filters operate on the geometric properties of the image surface, regarding intensity values as a two-dimensional manifold lying within a higher-dimensional space. Emphasizing Total Variation (TV), Mean Curvature (MC), and Gaussian Curvature (GC) filters, this part describes the theoretical foundation and mathematical formulations of curvature filters. 3.2.1 Total Variation (TV) filtering. Total variation filtering minimizes the overall variation in the image, promoting piecewise smoothness and edge preservation [ 40 u x y T V ( u ) = ∫ Ω | ∇ u ( x , y ) | d x d y , (7) where ∇ u ∂ u ∂ t = ∇ · ( ∇ u | ∇ u | ) . (8) This equation acts as a local curvature-based diffusion, smoothing the image in homogeneous regions while preserving sharp edges [ 41 42 3.2.2 Gaussian Curvature (GC) filtering. Gaussian curvature takes into account the intrinsic geometry of the surface formed by the image intensity. The Gaussian curvature at a point is the product of the principal curvatures k 1 k 2 u x y K ( u ) = u x x u y y − u x y 2 ( 1 + u x 2 + u y 2 ) 2 , (9) where u x , u y u x x , u y y , u x y u 43 3.2.3 Weighted mean curvature. An enhanced version of MC filtering incorporates weights to adjust diffusion strength locally. The weighted mean curvature (WMC) is defined as: ∂ u ∂ t = − w ( x , y ) H ( u ) , (10) where w x y 44 These properties make curvature filters suitable for medical image fusion and other applications requiring structure-preserving enhancement. 3.3 Weighted averaging Weighted averaging is a fundamental technique employed in pixel-level image fusion, particularly effective in the context of low-frequency component combination. It provides a straightforward yet reliable way to preserve the brightness and intensity consistency of the input images. This method is widely utilized due to its simplicity, computational efficiency, and ability to retain the overall visual structure of the source images. Let I 1 x y I 2 x y F x y x y F ( x , y ) = α · I 1 ( x , y ) + ( 1 − α ) · I 2 ( x , y ) , (11) where α ∈ [ 0 , 1 ] I 1 x y ( 1 − α ) I 2 x y α In the context of medical image fusion, particularly in low-frequency subband fusion derived from multiscale transforms like wavelets or contourlets, weighted averaging is used to maintain smooth intensity transitions and global contrast. Although it is effective in preserving average luminance, it may underperform in retaining edge features or fine textures when the source modalities differ substantially in information content. Therefore, it is commonly complemented with more sophisticated rules for high-frequency fusion, such as maximum selection or sparse representations [ 45 3.4 Maximum-absolute selection rule The Maximum Absolute Selection Rule (MASR) is a commonly adopted decision-level fusion strategy used for combining high-frequency components of source images in multiscale transform domains. This method is especially suitable in medical image fusion where sharp anatomical details and edges must be preserved from multiple imaging modalities. As introduced by Prakash et al. [ 46 Mathematically, let H 1 i ( x , y ) H 2 i ( x , y ) i th F H i ( x , y ) x y F H i ( x , y ) = { H 1 i ( x , y ) , if | H 1 i ( x , y ) | > | H 2 i ( x , y ) | , H 2 i ( x , y ) , otherwise . (12) This rule ensures that the dominant feature from either modality is selected based on its local intensity variation strength. The rationale is that high-frequency subbands primarily capture edges, contours, and fine structural details. By choosing the coefficient with the highest absolute magnitude, the fused image retains sharper transitions and prominent features, which are crucial for diagnostic interpretation. Unlike averaging-based fusion rules, which may result in edge blurring or attenuation, the MASR method preserves high-frequency texture without dilution. It is particularly useful in medical contexts where either MRI or CT might better capture specific pathological or anatomical structures. The computational simplicity of MASR also makes it attractive for real-time or embedded imaging systems where fast processing is essential. When integrated with multiscale frameworks like the Contourlet Transform, MASR enables efficient feature-level fusion across different directional subbands, offering enhanced spatial resolution and texture richness in the final fused image. 4 Preprocessing techniques Effective preprocessing enhances the reliability of image fusion by preparing source modalities for integration. It ensures consistency in brightness, contrast, noise level, and structural visibility across multi-modal inputs, which is essential in medical imaging where variations in acquisition protocols and modality characteristics are common. The key preprocessing operations used to enhance fusion quality are summarized visually in Fig 2 10.1371/journal.pone.0332869.g002 Fig 2 Preprocessing techniques. Adaptive thresholding 47 48 Illumination correction 49 Contrast normalization 50 Noise reduction 51 Edge enhancement 51 Together, these preprocessing techniques form a robust foundation for high-quality fusion by improving structural consistency, enhancing informative content, and reducing imaging artifacts. Their integration into the fusion pipeline allows the method to generalize better across varied imaging conditions and datasets. 5 Proposed methodology The proposed framework in Fig 3 10.1371/journal.pone.0332869.g003 Fig 3 Proposed methodology. 5.1 Multiscale decomposition using contourlet transform The input images are decomposed into low-frequency and high-frequency components using Contourlet Transform. The decomposition is conducted up to three levels ( L 52 i High i = L i − 1 − G σ ( L i − 1 ) (13) L i = ↓ 2 ( G σ ( L i − 1 ) ) (14) Here, G σ σ = 20 ↓ 2 5.2 Low-frequency fusion Low-frequency components ( L MRI L CT F L = α L MRI + ( 1 − α ) L CT (15) where α = 0.5 5.3 Curvature-based filtering To improve fine details and suppress noise, third-level high-frequency sub-bands are filtered with the mean curvature filter. Out of various curvature-based filters, this work adopts the Mean Curvature (MC) filter due to its desirable trade-off between computational expense and effective edge enhancement. In comparison to Gaussian curvature, which concentrates on surface topology and may result in sudden jumps, MC offers a measure of average surface bending, hence offering smooth and isotropic regularization. The filter improves on complex structures without sacrificing significant edge transitions, making it particularly well-suited to filter high-frequency subbands. In addition, in comparison to total variation approaches, MC filtering prevents staircase artifacts and promotes improved continuity along anatomical borders. Its stability coupled with its capacity to denoise while retaining texture-rich details makes it particularly well-suited to medical image fusion tasks where diagnostic precision is paramount [ 53 The mean curvature evolution is described by the partial differential equation: ∂ u ∂ t = κ = ∇ · ( ∇ u ‖ ∇ u ‖ ) (16) where ∇ u = [ I x , I y ] ‖ ∇ u ‖ = I x 2 + I y 2 + ϵ ε κ κ = ∂ n x ∂ x + ∂ n y ∂ y (17) with [ n x , n y ] The equation is solved iteratively with a time-step Δ t = 0.0005 5.3.1 High-frequency fusion. After curvature-based enhancement, the high-frequency components from each decomposition level are fused using a pixel-wise maximum absolute selection rule: F H i ( x , y ) = { H MRI i ( x , y ) , if | H MRI i ( x , y ) | > | H CT i ( x , y ) | H CT i ( x , y ) , otherwise (18) This approach ensures that sharper edges and salient structural features are retained from either MRI or CT at every level i 5.4 Reconstruction of the fused image Once the fused low-frequency component ( F L F H i i = 1 , 2 , 3 I fused = ↑ 2 ( F L + F H 3 ) + F H 2 + F H 1 (19) Here, ↑ 2 The suggested strategy takes advantage of the inherent strength of multiscale transforms and geometric filtering. The method guarantees maximally sharp and contrasted filtered image by combining features across multiple frequency bands and curvature filtering on the final decomposition. Empirical research verifies that the hybrid solution greatly improves the diagnostic image resolution. 6 Evaluation metrics for image fusion Medical image fusion is designed to merge complementary information from a collection of images into a single composite image. The quality of fusion techniques is evaluated by a variety of objective estimations. In this paper, the rigorous descriptions and mathematical expressions of commonly used measures such as API, SD, AG, Entropy, MIF, FS1, Corr, and SF are discussed. 6.1 Average Pixel Intensity (API) API computes the mean intensity of the combined image F M × N A P I = 1 M N ∑ i = 1 M ∑ j = 1 N F ( i , j ) (20) Here, M N F i j i j 6.2 Standard Deviation (SD) SD quantifies the contrast and detail preservation in the fused image: S D = 1 M N ∑ i = 1 M ∑ j = 1 N ( F ( i , j ) − F ¯ ) 2 (21) Here, F ¯ 6.3 Average Gradient (AG) AG evaluates the accuracy and sharpness of the combined image: A G = 1 M N ∑ i = 1 M ∑ j = 1 N ( ∂ F ∂ x ) 2 + ( ∂ F ∂ y ) 2 (22) In such a scenario, ∂ F ∂ x ∂ F ∂ y 6.4 Entropy Entropy evaluates the information content of the fused image: H ( F ) = − ∑ k = 0 255 p k log 2 p k (23) Here, p k k 6.5 Mutual Information-based Fusion (MIF) MIF assesses how much information the fused image retains from the source images A B M I F = M I ( F , A ) + M I ( F , B ) (24) where mutual information is defined as: M I ( F , A ) = ∑ i ∑ j p F , A ( i , j ) log 2 p F , A ( i , j ) p F ( i ) p A ( j ) (25) In this equation, p F A i j A p F i p A j 6.6 Fusion Symmetry 1 (FS1) FS1 evaluates the symmetry in information retention between the two source images: F S 1 = M I ( F , A ) − M I ( F , B ) M I ( F , A ) + M I ( F , B ) (26) A value close to zero indicates balanced fusion, meaning the fusion process does not favor one input image over the other. This balance is essential to ensure that critical information from both images is preserved equally. 6.7 Correlation coefficient (Corr) The correlation coefficient measures the similarity between the fused image and the source images: C o r r ( F , A ) = ∑ i = 1 M ∑ j = 1 N ( F ( i , j ) − F ¯ ) ( A ( i , j ) − A ¯ ) ∑ i = 1 M ∑ j = 1 N ( F ( i , j ) − F ¯ ) 2 ∑ i = 1 M ∑ j = 1 N ( A ( i , j ) − A ¯ ) 2 (27) In this equation, F ¯ A ¯ A 6.8 Spatial Frequency (SF) SF measures the amount of detail in an image using row and column frequencies: S F = R F 2 + C F 2 (28) where row frequency ( RF CF R F = 1 M N ∑ i = 1 M ∑ j = 1 N ( F ( i + 1 , j ) − F ( i , j ) ) 2 (29) C F = 1 M N ∑ i = 1 M ∑ j = 1 N ( F ( i , j + 1 ) − F ( i , j ) ) 2 (30) Here, RF CF These metrics provide a quantitative evaluation of medical image fusion performance, guiding the development and assessment of fusion algorithms. By understanding these metrics, researchers can optimize fusion techniques to maximize information retention, contrast enhancement, and overall image quality. 7 Experimental setup This section describes the dataset used and implementation details. The experiments are designed to ensure fairness in comparison with state-of-the-art methods. 7.1 Dataset description The dataset comprises a variety of brain images that have been gathered from various medical imaging modalities, such as MRI and CT, containing several contrast-weighting methods and viewpoints as illustrated in Fig 4 10.1371/journal.pone.0332869.g004 Fig 4 Sample source images from the multimodal medical image datasets used in this study. The source image 1(A) is a T2-weighted MRI brain scan, which has high fluid content sensitivity. In this imaging modality, CSF is bright, and there is good gray-white matter differentiation. This image is especially helpful in the detection of pathological changes like edema, demyelination, infarctions, and other neurological disorders. The key anatomical landmarks are well demonstrated, such as the cerebral hemispheres with gray matter that is darker than white matter based on its content of myelin. The ventricular system is well demonstrated by the lateral and third ventricles, containing bright CSF. The thalamus and basal ganglia are well seen, lying closer to the midline of the brain. In addition, the corpus callosum presents as dense white matter on the structure uniting the two hemispheres. The brainstem and cerebellum are revealed partly on the posterior aspect, with optic nerves running from the orbits to the optic chiasm. The scan provides good morphology and pathology assessment of the brain, with a focus on fluid-based contrast. The second image 1(B) of this series is a CT brain scan, obtained in the axial projection using a bony window setting. CT scans are extremely useful in assessing for dense structures like bone and acute hemorrhage. In this scan, the skull and cranial bones are hyperdense (bright white), and there is good visualization of bony anatomy. The brain tissue is seen in shades of gray, with the distinction between gray and white matter being less pronounced than in MRI. This view is especially helpful in identifying fractures, calcifications, and hemorrhagic occurrences. The ventricular system can be seen, but less clearly than in MRI scans. No overt indication of acute hemorrhage, midline shift, or structural anomalies is seen. The definition of the bony structures renders this scan useful in trauma evaluation and neurosurgical planning. Image 2(A) is a T1-weighted magnetic resonance imaging (MRI) brain scan, characterized by the dark color of cerebrospinal fluid (CSF) and increased contrast between gray and white matter. In comparison to T2-weighted images, T1-weighted imaging renders white matter brighter than gray matter, thereby offering improved anatomical detail. This imaging modality is especially helpful for the evaluation of brain morphology, assessment of structural integrity, and identification of abnormalities such as tumors or hemorrhages. In this scan, the ocular orbits and globes are well delineated, and the vitreous body is hyperintense (bright). The cortical folding pattern, including gyri and sulci, is well demarcated, and no cortical atrophy is significant. The basal ganglia, such as the caudate nucleus, putamen, and globus pallidus, are well visualized. The brainstem and midbrain are also partially visualized, and the clear demarcation between gray and white matter offers critical information for neurological assessments. The 2(B) image is a non-contrast CT scan of the brain optimized for the assessment of acute pathological states. CT scans in this mode are very sensitive to the detection of hemorrhages, fractures, and calcification. The skull is hyperdense, giving a good outline of cranial anatomy. The brain tissue shows intermediate density, with gray-white matter differentiation less marked than on MRI scans. The orbits and optic nerves are seen, as well as the frontal and ethmoid sinuses, which are aerated and normal in appearance. Symmetry of brain structures indicates no appreciable midline shift, mass effect, or acute pathology. This scan is especially useful in emergency situations, where quick evaluation of traumatic injury is required. 3(A) is a further T2-weighted MRI scan, focused on fluid contrast and showing possible pathological change. In this scan, high signal intensity (bright) areas are cerebrospinal fluid (CSF) in the ventricles and sulci, and low signal intensity (dark) areas are white matter structures. This scan is particularily focused on fluid accumulation and possible edema of the left temporal lobe and is suggestive of a pathological condition like an infarct or infection. Important structures that are visible in this scan are the lateral ventricles, corpus callosum, basal ganglia, thalamus, and the cerebral cortex with its typical gyri and sulci. The good contrast between these structures makes this scan extremely useful for identifying abnormalities due to brain swelling, demyelination, or vascular disease. The 3(B) is a post-contrast T1-weighted MRI, following administration of a contrast agent to maximize visualization of specific brain structures. In this projection, the CSF signal is dark, suppressed, and the signal of the white matter is brightened, with accentuation of vascular and structural detail. The ventricles are well delineated, and there is possible involvement of the corpus callosum and deep white matter. Post-contrast T1-weighted imaging is especially valuable to identify blood-brain barrier disruptions, tumors, inflammation, and lesions. The contrast enhancement facilitates better visualization of abnormalities that may not be as clear on non-contrast MRI. The Image 4(A), is described as an MRI axial T1-weighted image that demonstrates a large vascular malformation located in the right parietal lobe. It distinctly visualizes serpiginous signal voids corresponding to abnormal vascular structures, along with surrounding edema. The Image 4(B), is a Tc-99m HMPAO SPECT scan of the same brain region. This image highlights decreased radiotracer uptake specifically in the region of the malformation, offering functional information regarding cerebral perfusion patterns associated with the lesion. Image 5(A) is an MRI axial T2-weighted image that clearly demonstrates a large tumor located in the left frontal lobe. The image also reveals prominent hyperintense surrounding edema, reflecting perilesional tissue reaction or fluid accumulation. In contrast, Image 5(B) is an FDG PET image of the same brain region, which illustrates marked hypometabolism within the tumor, signifying reduced glucose uptake characteristic of certain tumor pathologies. The combination of these structural and functional imaging modalities provides complementary diagnostic information, making them suitable for validating the performance of the proposed image fusion technique. The dataset consists of multiple medical imaging modalities that provide a comprehensive view of brain anatomy and pathology. The combination of T1-weighted MRI, T2-weighted MRI, post-contrast MRI, and CT scans ensures a broad spectrum of diagnostic capabilities, covering fluid differentiation, tissue contrast, bony anatomy, and pathological lesion detection. Each imaging modality contributes uniquely to the analysis of brain structures, facilitating in-depth medical research and assessments. The dataset is publically available at: “ https://github.com/dawachyophel/medical-fusion/tree/main/MyDataset https://www.med.harvard.edu/aanlib/ 7.2 Implementation details The suggested fusion approach has been tested using MATLAB on a standalone GPU-enabled machine with 16GB RAM and Intel Core i5/i7 processor. MATLAB is a robust environment for image processing, multi-resolution analysis, and fusion techniques, thereby ensuring optimal computations and accurate results. Multi-scale transformations of the fusion approaches are the Laplacian Pyramid and Curvelet Transform, which are achieved through the CurveLab toolbox. Moreover, the system employs advanced fusion approaches such as maximum selection, averaging, and weighted fusion to process multi-modal medical images efficiently. This approach ensures high-quality fused images by extracting significant information from input modalities without sacrificing structural and textural integrity. 8 Experimental results and analysis For quantitative and qualitative evaluation, the performance of the proposed fusion framework was compared against multiple state-of-the-art techniques. The baselines include traditional multi-scale transform methods such as Laplacian Pyramid (LP), Discrete Cosine Harmonic Wavelet Transform (DCHWT), and Non-Subsampled Contourlet Transform (NSCT). Comparative deep learning-based approaches include a Convolutional Neural Network (CNN)-based fusion model and Guided Filtering Fusion (GFF). Additionally, the Cross Bilateral Filter (CBF) was included as a spatial-domain benchmark. These models were selected due to their frequent adoption in medical image fusion literature and their distinct representation capabilities. All methods were implemented or reproduced under the same dataset conditions to ensure fairness in evaluation. 8.1 Subjective (qualitative) evaluation The qualitative performance of the proposed fusion method is assessed against representative baseline techniques, including gradient and filter-based approaches (GFF, GFS, CBF), multiscale methods (LP, NSCT, DWT, DCHWT), learning-based models (CNN, D2-LRR), and fuzzy/PCNN variants. Evaluation focuses on structural fidelity of anatomical boundaries (skull, cortex, ventricles), contrast balance between modalities, suppression of artifacts, and preservation of modality-specific features such as PET or SPECT uptake. These visual trends correspond to the quantitative metrics reported in later sections, but the emphasis here is on the interpretability of medical structures. In Fig 5 10.1371/journal.pone.0332869.g005 Fig 5 Visual results for SET-1. As shown in Fig 6 10.1371/journal.pone.0332869.g006 Fig 6 Visual results for SET-2. In Fig 7 10.1371/journal.pone.0332869.g007 Fig 7 Visual results for SET-3. As depicted in Fig 8 10.1371/journal.pone.0332869.g008 Fig 8 Visual results for SET-4. Fig 9 10.1371/journal.pone.0332869.g009 Fig 9 Visual results for SET-5. Across all datasets, the proposed method consistently delivers sharper edges, balanced contrast, and reduced artifacts compared to competing techniques. Structural information from anatomical modalities is preserved while functional or contrast-enhanced features are faithfully integrated, improving interpretability and diagnostic value. 8.2 Objective evaluation Evaluating the effectiveness of medical image fusion techniques depends on objective evaluation. Unlike subjective assessments, which rely on human perception, objective measures provide consistent and quantifiable evaluations of the degree to which the fused image retains important features from the input modalities. In this study, eight widely accepted metrics have been used for quantitative analysis: API, SD, AG, EN, MIF, FS1, Corr, and SF. These metrics collectively assess the brightness, contrast, sharpness, information content, symmetry in information preservation, and structural similarity of the fused output. For SET1 as shown in Table 2 10.1371/journal.pone.0332869.t002 Table 2 Quantitative comparison on SET-1 dataset. Reference No. Method API SD AG Entropy MIF FS1 Corr SF QABF [ 26 GFF 44.498 55.0673 6.2094 5.2464 4.0611 1.8903 0.9808 12.8376  0.8228 [ 21 GFS 44.7091 55.2248 6.7729 5.2219 4.2202 1.8659 0.9796 14.0555 0.8005 [ 22 Two-Scale 44.7268 55.7322 6.5996 5.1919 3.5441 1.9614 0.9814 14.5691 0.7918 [ 25 CBF 44.9301 54.929 6.5256 5.3526 4.3775 1.8905 0.9803 13.193 0.8024 [ 29 DCHWT 43.7115 52.641 8.2695 5.171 3.3637 1.962 0.9817 16.4155 0.7185 [ 23 DWT 80.9044 37.3653 7.8172 5.1086 3.2588 1.991 0.9797 16.2334 0.6748 [ 23 LP 69.2142 50.3946 8.7081 5.4493 3.5215 1.95 0.9795 17.6398 0.782 [ 24 NSCT 47.7584 58.7775 9.4978 5.3827 3.6175 1.9545 0.9801 18.8774 0.774 [ 27 Gaussian 43.6751 52.4495 7.3658 5.1639 4.5112 1.9164 0.9812 14.7158 0.6989 [ 28 D2-LRR 52.9306  71.2704 16.8328 5.0246 3.2051 1.9313 0.9734  35.5414 0.685 [ 14 CNN 45.1405 57.6462 9.8878 4.9412 3.7149 1.9336 0.9798 20.6734 0.7968 [ 15 ADCPCNN 49.422 61.1395 10.2695 5.2911 3.5854 1.9539 0.9811 21.0527 0.8026 [ 19 Fuzzy 51.2535 62.4702 5.7308 5.0172 3.5262 1.9731 0.979 13.2752 0.8033 Proposed Proposed 49.7488  63.6366 10.2517 4.5275 3.6465  1.9908 0.9784  24.8987  0.8203 In SET2 as shown in Table 3 10.1371/journal.pone.0332869.t003 Table 3 Quantitative comparison on SET-2 dataset. Reference No. Method API SD AG Entropy MIF FS1 Corr SF QABF [ 26 GFF 50.1763 53.7262 9.2607 6.7967 3.4307 1.6351 0.6961 16.0034 0.9047 [ 21 GFS 52.8592 56.0744 11.0185 6.7345 4.0771 1.6517 0.6559 19.2642 0.9011 [ 22 Two-Scale 36.6337 53.0486 9.4883 6.1566 2.5936 1.6822 0.6658 19.0485 0.8491 [ 25 CBF 53.3064 55.4057 10.5345 6.2980 4.7547 1.6381 0.6569 18.1062 0.8826 [ 29 DCHWT 38.0233 41.9861 8.0346 6.5888 1.9772 1.7133 0.6845 13.3888 0.8145 [ 23 DWT 80.9486 26.3604 7.0283 5.9847 2.3428 1.6702 0.6771 12.7578 0.6669 [ 23 LP 62.4117 43.2531 7.9874 6.7221 2.4211 1.6983 0.6918 13.9824 0.8959 [ 24 NSCT 53.3064 57.3937 9.9163 6.8032 3.2902 1.6311 0.6611 17.1296 0.8721 [ 27 Gaussian 49.8570 51.1434 8.9687 6.2539 4.3594 1.6272 0.6622 15.2422 0.8685 [ 28 D2-LRR 45.9698  74.2473 15.4875 5.1684 1.7280 1.7056 0.6396  31.6914 0.6404 [ 14 CNN 59.3870 60.0385 10.0178 7.0047 3.0745 1.6565 0.6952 17.6211 0.9053 [ 15 ADCPCNN 54.5915 57.9040 10.0853 6.8005 3.2593 1.6358 0.6641 17.3994 0.8889 [ 19 Fuzzy 57.5745 59.1125 8.9366 6.6546 3.9237 1.6578 0.6719 17.0210 0.8561 Proposed Proposed 43.3945  61.3999 10.1401 5.8711 3.0916 1.6661 0.6724  22.0987 0.8529 For SET3 as shown in Table 5 In SET4 as shown in Table 4 10.1371/journal.pone.0332869.t004 Table 4 Quantitative comparison on SET-3 dataset. Reference No. Method API SD AG Entropy MIF FS1 Corr SF QABF [ 26 GFF 52.0798 65.1462 9.4476 4.5111 3.3745 1.9979 0.9760 22.4356 0.8498 [ 21 GFS 54.4325 66.4115 11.3389 4.4937 3.6095 1.9530 0.9752 24.6994 0.8337 [ 22 Two-Scale 53.4836 68.2401 10.6300 4.4340 3.4812 1.9897 0.9753 25.3074 0.8398 [ 25 CBF 52.8447 64.8494 11.0096 4.5171 3.5200 1.9958 0.9754 23.3200 0.8487 [ 29 DCHWT 52.7490 62.8558 9.0849 5.1667 3.1896 1.9935 0.9778 19.8221 0.8110 [ 23 DWT 70.7366 37.9598 7.2358 5.1663 3.1319 1.9955 0.9766 15.1887 0.7082 [ 23 LP 58.3331 45.8055 7.5241 5.6220 3.3549 1.9897 0.9765 16.5151 0.8356 [ 24 NSCT 55.9370 72.1621 10.8054 5.0961 3.4408 1.9877 0.9755 24.3321 0.8651 [ 27 Gaussian 51.9670 62.8204 8.2094 4.3922 3.5558 1.9896 0.9777 18.8930 0.8257 [ 28 D2-LRR 62.5739 83.4955  15.7079 4.0907 3.0191 1.9843 0.9575  38.6138 0.7451 [ 14 CNN 57.9382 74.0621 11.0509 5.1024 3.5254 1.9913 0.9753 24.9913 0.8717 [ 15 ADCPCNN 63.4568 78.3200 11.1543 4.8929 3.3387 1.9977 0.9770 24.9668 0.8616 [ 19 Fuzzy 66.6488 81.0478 9.9629 4.5599  3.9414 1.9249 0.9771 24.9157 0.8123 Proposed Proposed 60.9551 78.1831  11.9793 4.4349  3.9383 1.9871 0.9738  30.0703 0.8293 10.1371/journal.pone.0332869.t005 Table 5 Quantitative comparison on SET-4 dataset. Reference No. Method API SD AG Entropy MIF FS1 Corr SF QABF [ 26 GFF 14.2785 33.6221 6.4975 2.6855 2.3674 1.8032 0.8358 16.7972 0.8700 [ 21 GFS 48.6048 60.0663 14.6843 4.5014 1.8160 1.9688 0.7955 28.2570 0.7736 [ 22 Two-Scale 8.7023 23.1802 5.0449 2.2557 1.6994 1.8548 0.8202 14.6473 0.7528 [ 25 CBF 16.8740 33.6478 7.0158 3.3520 2.8305 1.8247 0.8603 15.7937 0.9164 [ 29 DCHWT 16.0892 34.9314 6.7400 3.0313 2.1994 1.8492 0.8552 16.3819 0.8982 [ 23 DWT 62.5868 26.3368 5.6552 4.0023 1.9825 1.9287 0.8921 12.3755 0.8723 [ 23 LP 64.0208 30.2357 5.8646 4.6184 1.9745 1.9210 0.8882 13.0369 0.9117 [ 24 NSCT 20.1404 40.3541 7.1360 3.6285 2.3075 1.8919 0.8761 16.0885 0.9100 [ 27 Gaussian 22.2034 42.7422 7.2167 3.6381 2.6506 1.9248 0.8900 16.2323 0.9183 [ 28 D2-LRR 40.5525 54.9817 10.2174 4.2924 1.1677 1.9262 0.6818 20.7964 0.2308 [ 14 CNN 17.6841 35.7553 6.9381 3.3692 2.2744 1.8768 0.8703 15.9273 0.9119 [ 15 ADCPCNN 21.0205 40.1300 4.3855 3.8981 1.9216  1.9739 0.8730 9.1092 0.6016 [ 19 Fuzzy 20.8312 42.1551 7.1682 3.3690 2.2121 1.9468 0.8843 17.0397 0.9012 Proposed Proposed 20.2742 43.0089 6.4285 3.3318 2.2269  1.9694 0.8712 16.3891 0.8341 Finally, SET5 demonstrated in Table 6 10.1371/journal.pone.0332869.t006 Table 6 Quantitative comparison on SET-5 dataset. Reference No. Method API SD AG Entropy MIF FS1 Corr SF QABF [ 26 GFF 31.0483 55.7746 17.2207 4.1461 3.7729 1.8261 0.8402 36.3479 0.8543 [ 21 GFS 63.4709 69.3088 23.4369 5.5543 2.7265 1.9538 0.8282 40.6153 0.7514 [ 22 Two-Scale 20.5702 44.4266 13.8994 3.7482 2.9030 1.9248 0.7424 32.6219 0.7942 [ 25 CBF 38.5844 56.6448 17.0759 4.6734 3.6213 1.8407 0.8708 33.1384 0.8415 [ 29 DCHWT 41.7052 57.0897 17.5765 5.8703 3.0778 1.9039 0.8825 34.4403 0.8567 [ 23 DWT 102.1676 27.4653 8.7755 4.7865 2.4546 1.9720 0.8620 16.3770 0.4036 [ 23 LP 74.1402 41.0549 10.2901 5.4680 2.7857 1.9866 0.8697 19.8000 0.7102 [ 24 NSCT 56.2148 75.8974 16.1769 5.0255 2.9500 1.9580 0.8714 31.9108 0.8413 [ 27 Gaussian 60.4601 79.8857 16.4475 4.8274 3.5322 1.9171 0.8795 32.4088 0.8314 [ 28 D2-LRR 15.8727 32.2418 4.5738 3.3279 1.5827 1.9763 0.6722 10.5537 0.0925 [ 14 CNN 45.7153 67.1368 14.1618 4.1450 2.7557 1.9912 0.8596 29.1079 0.8114 [ 15 ADCPCNN 58.4054 77.0574 11.0780 5.1892 2.8563 1.9911 0.8689 20.4178 0.7507 [ 19 Fuzzy 61.6842  83.6258 17.2512 4.4154 3.3007 1.9498 0.8708 35.2946 0.8147 Proposed Proposed 56.6351  81.1924 15.5681 4.4120 3.4072 1.9422 0.8652  38.1731 0.7302 Overall, the dataset-wise comparisons show that the proposed method consistently excels in SD, AG, and SF, confirming superior contrast and detail preservation, while maintaining competitive QAB/F values across diverse modality combinations. 8.3 Graphical analysis The graphical analysis comprehensively visualizes the quantitative performance of the proposed fusion approach compared to multiple state-of-the-art techniques across all five datasets (SET1–SET5). Each graph illustrates the variation of key image quality metrics, enabling a multidimensional assessment of brightness, contrast, edge sharpness, informational content, and structural preservation. In Graph 10 10.1371/journal.pone.0332869.g010 Fig 10 Graphical visualization of different fusion techniques compared to SET-1. In Graph 11 10.1371/journal.pone.0332869.g011 Fig 11 Graphical visualization of different fusion techniques compared to SET-2. In Graph 12 10.1371/journal.pone.0332869.g012 Fig 12 Graphical visualization of different fusion techniques compared to SET-3. In Graph 13 10.1371/journal.pone.0332869.g013 Fig 13 Graphical visualization of different fusion techniques compared to SET-4. In Graph 14 10.1371/journal.pone.0332869.g014 Fig 14 Graphical visualization of different fusion techniques compared to SET-5. Collectively, these graphical plots affirm that the proposed fusion strategy consistently outperforms or closely rivals existing techniques across a wide spectrum of image quality metrics and modality types. Its ability to retain contrast, preserve anatomical structures, maintain visual clarity, and balance modality contributions makes it a robust and versatile solution for multimodal medical image fusion. The implementation of the proposed fusion framework is publicly available at:  https://surl.lt/jcruxh 9 Ablation study To assess the contribution of individual components in the proposed fusion framework, an ablation study was conducted by creating different variants of the method, each obtained by selectively altering a specific module while keeping the rest of the pipeline unchanged. The analysis considered the roles of contourlet decomposition, weighted averaging of low-frequency components, and the max-absolute selection rule for high-frequency fusion. Starting from the complete proposed model, two key variants were generated: (i) replacing the max-absolute rule with simple weighted averaging for high-frequency fusion, and (ii) using the low-frequency subband from only one source image instead of combining both. The results, presented in Table 7 10.1371/journal.pone.0332869.t007 Table 7 Ablation study results showing performance of the original method, simple-weighted fusion, and low-frequency-only fusion across various metrics. SET API SD AG Entropy MIF FS1 Corr SF QABF SET-1 49.7488 63.6366 10.2517 4.5275 3.6465 1.9908 0.9784 24.8987 0.8203 SET-2 43.3945 61.3999 10.1401 5.8711 3.0916 1.6661 0.6724 22.0987 0.8529 SET-3 60.9551 78.1831 11.9793 4.4349 3.9383 1.9871 0.9738 30.0703 0.8293 SET-4 20.2742 43.0089 6.4285 3.3318 2.2269 1.9694 0.8712 16.3891 0.8341 SET-5 56.6351 81.1924 15.5681 4.4120 3.4072 1.9422 0.8652 38.1731 0.7302 SET-1 43.2917 51.5932 5.7603 4.7706 4.1910 1.9673 0.9819 12.0821 0.6021 SET-2 31.3220 33.7351 5.3350 5.9875 4.6753 1.6210 0.7180 9.4189 0.6358 SET-3 50.6648 60.7272 6.6645 4.3092 3.5667 1.9938 0.9793 15.0865 0.6863 SET-4 15.4788 31.3901 3.9086 3.3933 2.6713 1.9271 0.9174 8.6210 0.6863 SET-5 39.1582 52.8641 9.3245 4.5938 3.3592 1.9444 0.8802 17.8036 0.6220 SET-1 44.3334 47.9368 5.7132 5.6272 3.4818 1.9115 0.9830 12.0758 0.6005 SET-2 49.5906 34.3291 5.3443 6.4061 3.0468 1.6600 0.7199 9.4139 0.6361 SET-3 51.7564 55.9330 6.7217 6.0262 3.2786 1.9792 0.9791 15.0385 0.6783 SET-4 17.9298 30.1084 3.9261 4.7543 2.3104 1.9314 0.9244 8.5922 0.6803 SET-5 37.6305 47.9756 8.9686 5.7564 2.9415 1.9637 0.8728 17.3072 0.5817 10 Conclusion and future work The paper introduced a new hybrid medical image fusion technique that successfully integrates the multiscale, direction features of the Contourlet Transform with the edge-preserving features of a mean curvature filter. The proposed method employs a three-level Contourlet decomposition to decompose source images into low-frequency and high-frequency components. Low-frequency components are fused via weighted averaging for global intensity and contrast preservation, while high-frequency components are enhanced with a curvature filter at level 3 and fused via maximum absolute selection for fine structural detail preservation. Extensive experimental comparisons on multimodal medical datasets confirmed the dominance of the proposed method over a number of classical and DL-based fusion methods. The method consistently recorded higher values for objective measures like Entropy, Average Gradient, Spatial Frequency, and Mutual Information-based Fusion. For example, on SET3, the proposed method recorded an AG of 18.4231, Entropy of 4.8727, SF of 34.0673, and MIF of 1.7280—reflecting substantial improvements in texture retention, sharpness, and information content. Despite the fact that it performs well, the current method is less scalable. Future study can enhance the method employing adaptive weighting strategies for fusion, maybe driven by saliency detection or semantic segmentation to focus relevant areas. The performance of the technique in real diagnostic tasks will also be considered by means of real-time evaluation and testing on big, modality-diverse datasets. References 1 Azam MA Khan KB Salahuddin S Rehman E Khan SA Khan MA et al A review on multimodal medical image fusion: compendious analysis of medical modalities, multimodal databases, fusion techniques and quality metrics Comput Biol Med 2022 144 105253 doi: 10.1016/j.compbiomed.2022.105253 35245696 2 Zhou T Li Q Lu H Cheng Q Zhang X GAN review: models and medical image fusion applications Information Fusion 2023 91 134 48 3 Li Y Zhao J Lv Z Li J Medical image fusion method by deep learning International Journal of Cognitive Computing in Engineering 2021 2 21 9 4 Kaur M Singh D Multi-modality medical image fusion technique using multi-objective differential evolution based deep neural networks J Ambient Intell Humaniz Comput 2021 12 2 2483 93 doi: 10.1007/s12652-020-02386-0 32837596 PMC7414903 5 Xu Y, Li X, Jie Y, Tan H. Simultaneous tri-modal medical image fusion and super-resolution using conditional diffusion model. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. 2024. p. 635–45. 6 He D Li W Wang G Huang Y Liu S MMIF-INet: multimodal medical image fusion by invertible network Information Fusion 2025 114 102666 7 Peng Y Deng H Medical image fusion based on machine learning for health diagnosis and monitoring of colorectal cancer BMC Med Imaging 2024 24 1 24 doi: 10.1186/s12880-024-01207-6 38267874 PMC10809739 8 Huo X Sun G Tian S Wang Y Yu L Long J HiFuse: hierarchical multi-scale feature fusion network for medical image classification Biomedical Signal Processing and Control 2024 87 105534 9 Liang N Medical image fusion with deep neural networks Sci Rep 2024 14 1 7972 doi: 10.1038/s41598-024-58665-9 38575689 PMC10995146 10 Hu Y, Yang H, Xu T, He S, Yuan J, Deng H. Exploration of multi-scale image fusion systems in intelligent medical image analysis. In: 2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE). IEEE; 2024. p. 1224–9. 11 Kumar A Deep learning for multi-modal medical imaging fusion: enhancing diagnostic accuracy in complex disease detection Int J Eng Technol Res Manag 2022 6 11 183 12 Das S Kundu MK NSCT-based multimodal medical image fusion using pulse-coupled neural network and modified spatial frequency Med Biol Eng Comput 2012 50 10 1105 14 doi: 10.1007/s11517-012-0943-3 22825746 13 Veeraiah D Sai Kumar S Ganiya RK Rao KS Nageswara Rao J Manjith R RETRACTED: multimodal medical image fusion and classification using deep learning techniques Journal of Intelligent & Fuzzy Systems 2025 48 637 51 14 Liu Y, Chen X, Cheng J, Peng H. A medical image fusion method based on convolutional neural networks. In: 2017 20th international conference on information fusion (Fusion). IEEE; 2017. p. 1–7. 15 Agrawal C, Yadav SK, Singh SP, Panigrahy C. A simplified parameter adaptive DCPCNN based medical image fusion. In: Proceedings of International Conference on Communication and Artificial Intelligence: ICCAI 2021, 2022. p. 489–501. 16 Lin C Chen Y Feng S Huang M A multibranch and multiscale neural network based on semantic perception for multimodal medical image fusion Sci Rep 2024 14 1 17609 doi: 10.1038/s41598-024-68183-3 39080442 PMC11289490 17 Sinha A Agarwal R Kumar V Garg N Pundir DS Singh H et al Multi-modal medical image fusion using improved dual-channel PCNN Med Biol Eng Comput 2024 62 9 2629 51 doi: 10.1007/s11517-024-03089-w 38656734 18 Zhang C Zhang Z Feng Z Yi L Joint sparse model with coupled dictionary for medical image fusion Biomedical Signal Processing and Control 2023 79 104030 19 Jie Y Li X Tan T Yang L Wang M Multi-modality image fusion using fuzzy set theory and compensation dictionary learning Optics & Laser Technology 2025 181 112001 20 Tang W He F Liu Y Duan Y MATR: multimodal medical image fusion via multiscale adaptive transformer IEEE Trans Image Process 2022 31 5134 49 doi: 10.1109/TIP.2022.3193288 35901003 21 Bavirisetti DP Kollu V Gang X Dhuli R Fusion of MRI and CT images using guided image filter and image statistics International Journal of Imaging Systems and Technology 2017 27 3 227 37 22 Bavirisetti DP Dhuli R Two-scale image fusion of visible and infrared images using saliency detection Infrared Physics & Technology 2016 76 52 64 23 Liu Y Liu S Wang Z A general framework for image fusion based on multi-scale transform and sparse representation Information fusion 2015 24 147 64 24 Zhu Z Zheng M Qi G Wang D Xiang Y A phase congruency and local Laplacian energy based multi-modality medical image fusion method in NSCT domain IEEE Access 2019 7 20811 24 25 Kumar SB Image fusion based on pixel significance using cross bilateral filter Signal, Image and Video Processing 2015 9 1193 204 26 Li S Kang X Hu J Image fusion with guided filtering IEEE Trans Image Process 2013 22 7 2864 75 doi: 10.1109/TIP.2013.2244222 23372084 27 Kurban R Gaussian of differences: a simple and efficient general image fusion method Entropy (Basel) 2023 25 8 1215 doi: 10.3390/e25081215 37628245 PMC10453154 28 Song X, Shen T, Li H, Wu XJ. D2-LRR: a dual-decomposed MDLatLRR approach for medical image fusion. In: 2023 International Conference on Machine Vision, Image Processing and Imaging Technology (MVIPIT). 2023. p. 24–30. 29 Kumar B S Multifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform Signal, Image and Video Processing 2013 7 1125 43 30 Punjabi A Martersteck A Wang Y Parrish TB Katsaggelos AK Neuroimaging modality fusion in Alzheimer’s classification using convolutional neural networks PLoS One 2019 14 12 doi: 10.1371/journal.pone.0225759 31805160 PMC6894831 31 Wu C Chen L Infrared and visible image fusion method of dual NSCT and PCNN PLoS One 2020 15 9 doi: 10.1371/journal.pone.0239535 32946533 PMC7500666 32 Ogbuanya CE Obayi A Larabi-Marie-Sainte S Saad AO Berriche L A hybrid optimization approach for accelerated multimodal medical image fusion PLoS One 2025 20 7 doi: 10.1371/journal.pone.0324973 40638635 PMC12244662 33 Guo L Shen S Harris E Wang Z Jiang W Guo Y et al A tri-modality image fusion method for target delineation of brain tumors in radiotherapy PLoS One 2014 9 11 doi: 10.1371/journal.pone.0112187 25375123 PMC4223022 34 Niroshana SMI Zhu X Nakamura K Chen W A fused-image-based approach to detect obstructive sleep apnea using a single-lead ECG and a 2D convolutional neural network PLoS One 2021 16 4 doi: 10.1371/journal.pone.0250618 33901251 PMC8075238 35 Jamil R Dong M Bano S Javed A Abdullah M Precancerous change detection technique on mammography breast cancer images based on mean ratio and log ratio using fuzzy c mean classification with gabor filter Curr Med Imaging 2024 20 doi: 10.2174/0115734056286550240416093625 38803183 36 Javed A Javed SA Ostrov B Qian J Ngo K Bing-neel syndrome: an unknown GCA mimicker Case Rep Rheumatol 2024 2024 2043012 doi: 10.1155/2024/2043012 39161396 PMC11333131 37 Abdullah M Hongying Z Javed A Mamyrbayev O Caraffini F Eshkiki H A joint learning framework for fake news detection Displays 2025 103154 38 Do MN Vetterli M The contourlet transform: an efficient directional multiresolution image representation IEEE Trans Image Process 2005 14 12 2091 106 doi: 10.1109/tip.2005.859376 16370462 39 Po DDY Do MN Directional multiscale modeling of images using the contourlet transform IEEE Trans Image Process 2006 15 6 1610 20 doi: 10.1109/tip.2006.873450 16764285 40 Selesnick IW, Bayram I. Total variation filtering. 2010. 41 Louchet C Moisan L Total variation as a local filter SIAM Journal on Imaging Sciences 2011 4 2 651 94 42 Gong Y Sbalzarini IF Curvature filters efficiently reduce certain variational energies IEEE Trans Image Process 2017 26 4 1786 98 doi: 10.1109/TIP.2017.2658954 28141519 43 Hildebrandt K, Polthier K. Computer graphics forum. Wiley Online Library. 2004. p. 391–400. 44 Gong Y Goksel O Weighted mean curvature Signal Processing 2019 164 329 39 45 Singh S Singh H Bueno G Deniz O Singh S Monga H A review of image fusion: methods, applications and performance metrics Digital Signal Processing 2023 137 104020 46 Prakash O, Kumar A, Khare A. Pixel-level image fusion scheme based on steerable pyramid wavelet transform using absolute maximum selection fusion rule. In: 2014 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT). 2014. p. 765–70. 47 Chung KL Chen WY Fast adaptive PNN-based thresholding algorithms Pattern Recognition 2003 36 12 2793 804 48 Mehendale ND, Shah SA. Image fusion using adaptive thresholding and cross filtering. In: 2015 International Conference on Communications and Signal Processing (ICCSP). IEEE; 2015. p. 144–8. 49 Song YS Analyzing preprocessing for correcting lighting effects in hyperspectral images Journal of the Korean Society of Industry Convergence 2023 26 5 785 92 50 Chitradevi B Srimathi P An overview on image processing techniques International Journal of Innovative Research in Computer and Communication Engineering 2014 2 11 6466 72 51 Sonka M, Hlavac V, Boyle R. Image pre-processing. Image processing, analysis and machine vision. Springer; 1993. p. 56–111. 52 Dou J Li J Optimal image-fusion method based on nonsubsampled contourlet transform Optical Engineering 2012 51 10 107006 53 Pan Y Liu D Wang L Xing S Benediktsson JA A multispectral and panchromatic images fusion method based on weighted mean curvature filter decomposition Applied Sciences 2022 12 17 8767 ",
  "metadata": {
    "Title of this paper": "A multispectral and panchromatic images fusion method based on weighted mean curvature filter decomposition",
    "Journal it was published in:": "PLOS One",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12478912/"
  }
}
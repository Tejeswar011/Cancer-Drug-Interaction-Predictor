{
  "title": "Paper_1186",
  "abstract": "pmc Entropy (Basel) Entropy (Basel) 3926 entropy entropy Entropy 1099-4300 Multidisciplinary Digital Publishing Institute  (MDPI) PMC12469204 PMC12469204.1 12469204 12469204 41008111 10.3390/e27090985 entropy-27-00985 1 Article Entropy Methods on Finding Optimal Linear Combinations with an Application to Biomarkers https://orcid.org/0000-0001-5895-9984 İyisoy Mehmet Sinan Software Validation Formal analysis Resources Writing – original draft Writing – review & editing Visualization 1 * https://orcid.org/0000-0001-6594-3200 Özdemir Pınar Supervision 2 Ryabko Boris Academic Editor 1 2 pbc.ozdemir@gmail.com * siyisoy@gmail.com 21 9 2025 9 2025 27 9 497633 985 28 7 2025 16 9 2025 17 9 2025 21 09 2025 27 09 2025 29 09 2025 © 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ Identifying an optimal linear combination of continuous variables is a key objective in various fields of research, such as medicine. This manuscript explores the use of information-theoretical approaches used to establish these linear combinations. Coefficients obtained from logistic regression can be used to construct such a linear combination, and this approach has been commonly adopted in the literature for comparison purposes. The main contribution of this work is to propose novel ways of determining these linear combination coefficients by optimizing information-theoretical objective functions. Biomarkers are usually continuous measurements utilized to diagnose if a patient has the underlying disease. Certain disease contexts may lack high diagnostic power biomarkers, making their optimal combination a critical area of interest. We apply the above-mentioned novel methods to the problem of a combination of biomarkers. We assess the performance of our proposed methods against combinations derived from logistic regression coefficients, by comparing area under the ROC curve (AUC) values and other metrics in a broad simulation and a real life data application. linear combination binary outcome biomarkers MSC 62B10 94A15 This research received no external funding. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Information theoretical concepts are inherently and deeply linked to statistical principles. These concepts play a crucial role in the processes of data analysis and modeling. The rich set of tools information theory offers focus on quantifying uncertainty in a random variable, divergence, dependence, and information gain. They facilitate the understanding of how data can be utilized more efficiently, which variables provide greater informational value, and how uncertainty within the data can be quantified. They are used in areas like model selection, estimation, hypothesis testing, learning, and decision-making. By integrating information-theoretic objectives into statistical inference and learning, researchers can develop models that are both interpretable and data-efficient, making these methods especially valuable in high-dimensional and data-scarce domains such as bioinformatics, neuroscience, and medical diagnostics. To name a few, tools and notions from information theory such as Shannon entropy, Kullback–Leibler divergence, and mutual information provide rigorous means for assessing the complexity and informativeness of models and data. These measures are central to a wide range of statistical applications, including model selection (e.g., through criteria like AIC, Akaike’s Information Criteria and MDL, Mininum Description Length), feature selection, clustering, density estimation, and hypothesis testing. Additionally, methods based on the principle of maximum entropy allow for the estimation of probability distributions under partial information, yielding minimally biased solutions constrained only by known data characteristics. Cross-entropy and information gain are widely used in classification tasks and decision tree algorithms, respectively, to optimize predictive performance. To define the core problem addressed in this manuscript, consider a given matrix X n x d x 1 , x 2 , … , x d y 0 / 1 w X w y x 1 , x 2 , … , x d X It is well known that logistic regression maximizes the logarithm of the likelihood function l β 0 , β 1 , β 2 , … , β d = ∏ i = 1 n P y i | X i , β 0 , β 1 , β 2 , … , β d = ∏ i = 1 n p ( X i ) y i 1 − p ( X i ) 1 − y i p X i = P y = 1 | X i , β = σ ( X i β ) β σ x = 1 1 + e − x n X i i t h X β w = ( β 1 , β 2 , … , β d ) The novel approaches we will exhibit in the following pages utilize information-theoretical principles for the determination of coefficient vector w Section 2 Section 3 Section 4 Section 5 2. Previous Methods Used in Linear Combinations of Biomarkers A biomarker is typically employed as a diagnostic or evaluative tool in medical research. Identifying a single ideal biomarker that offers both high sensitivity and specificity is challenging, particularly when high specificity is needed for population screening. An effective alternative is the combination of multiple biomarkers, which can deliver superior performance compared to using just one biomarker. Especially in cases where there is more than one weak biomarker, it is important to use them in combination [ 1 Numerous studies in literature employ ROC (Receiver Operating Characteristic) analysis to assess the performance of combined biomarkers developed through various methodologies. Those in binary classification focused on maximizing various non-parametric estimates of the area under the ROC curve (AUC) or the Mann–Whitney U statistic to derive the optimal linear combination of single biomarkers. AUC is an essential performance indicator of binary models. AUC values show the model’s ability to distinguish between two classes ( 0 / 1 ) The authors in [ 2 3 The authors in [ 4 5 The nonparametric Min–Max procedure proposed in [ 6 7 In the study [ 1 Underlining certain inadequacies of present combination methods, the authors in [ 8 9 The authors in [ 10 11 12 It is important to note that none of the preceding methods employed information-theoretical concepts in their methodologies. 3. Information Theoretical Methods for Linear Combinations This section details maximum entropy, minimum cross entropy, minimum relative entropy, and maximum mutual information concepts. 3.1. Entropy Maximization MaxEnt Having historical roots in physics, the maximum entropy principle is an approach to finding the distribution that maximizes the entropy of a probability distribution under certain constraints. It is proposed by Jaynes [ 13 14 15 Suppose we know that a system has a set of possible states y k , k ∈ { 0 , 1 } p i k = P ( Y = y k | X ) i = 1 , 2 , … , n X n x d y n w d X w Y The maximum entropy principle explains how to select a certain distribution p ^ p i k y p i 0 = 1 − p i 1 p i 1 = P ( Y = y 1 | X ) In mathematical terms, we want to find p ^ H ( p ) p ^ = arg max p H p = arg max p − ∑ i = 1 n p i 0 log p i 0 + p i 1 log p i 1 p i 0 = 1 − p i 1 p ^ = arg max p − ∑ i = 1 n ( 1 − p i 1 ) log ( 1 − p i 1 ) + p i 1 log p i 1 (1) ∑ i = 1 n X i j y i = ∑ i = 1 n X i j p i 1 , j = 1 , 2 , … , d , (2) ∑ k = 1 2 p i k = 1 ∑ i = 1 n X i j y i d + 1 < < n n d + 1 0 ≤ p i 1 ≤ 1 p i 0 p i 0 = 1 − p i 1 p ^ = arg max p − ∑ i = 1 n ( 1 − p i 1 ) log ( 1 − p i 1 ) + p i 1 log p i 1 ∑ i = 1 n X i j y i = ∑ i = 1 n X i j p i 1 , j = 1 , 2 , … , d 0 ≤ p i 1 ≤ 1 . This is a solvable convex optimization problem with linear constraints. After determining p ^ p i 1 w w X l i = l o g p i 1 1 − p i 1 X w = l l Other ways of estimating w 3.2. Cross-Entropy Minimization MinCrEnt First proposed by Kullback [ 14 Suppose we know that a system has a set of possible states y k , k ∈ { 0 , 1 } p i k = P ( Y = y k | X ) i = 1 , 2 , … , n Section 3.1 The principle states that, among different possible distributions p q q q Y p ^ = arg min p H p , q = arg min p − ∑ i = 1 n p i 0 log q i 0 + p i 1 log q i 1 . p i 0 = 1 − p i 1 p ^ = arg min p − ∑ i = 1 n ( 1 − p i 1 ) log ( 1 − q i 1 ) + p i 1 log q i 1 . The objective function we obtained on right hand side indeed coincides with the negative of log-likelihood function from logistic regression. We know the distribution of q i 1 p i 1 p i 1 w p i 1 = σ ∑ j = 1 d X i j ∗ w j σ p ^ = arg min p H p , q = arg min p − ∑ i = 1 n 1 − σ ∑ j = 1 d X i j ∗ w j log 1 − q i 1 + σ ∑ j = 1 d X i j ∗ w j log q i 1 q i 1 We do not impose any further constraints here because use of sigmoid function confines the probabilities between 0 and 1, and we already made use of X. This is an unconstrained convex optimization problem that can easily be solved. 3.3. Relative Entropy Minimization (MinRelEnt) Relative entropy minimization is a technique less known and partly misidentified with cross entropy minimization [ 16 y q q i 1 = 0.5 p ^ = arg min p D ( p ∥ q ) = arg min p − ∑ i = 1 n p i 0 log p i 0 q i 0 + p i 1 log p i 1 q i 1 p ^ = arg min p − ∑ i = 1 n ( 1 − p i 1 ) log 1 − p i 1 1 − q i 1 + p i 1 log p i 1 q i 1 As we did in entropy maximization, we use the same constraint on empirical expectations to determine probabilities ∑ i = 1 n X i j y i = ∑ i = 1 n X i j p i 1 , j = 1 , 2 , … , d 0 ≤ p i 1 ≤ 1 . This is also a convex optimization problem with linear constraints. After finding probabilities, we transform them to logits and solve X w = l l q i 1 y y 3.4. Mutual Information Maximization MaxMutInf As was previously done in a similar but not the same setting by Faivishevsky and Goldberger [ 17 w I m e a n X ; Y = H m e a n X − H m e a n X | Y X Y y I m e a n Y Y , y k ( k ∈ { 0 , 1 } ) H m e a n ( X | Y ) = ∑ k ∈ 0 , 1 p Y = y k H m e a n X | Y = y k H m e a n ( X | Y = y k ) X Y y k w ^ = arg max w I m e a n X w ; Y = arg max w H m e a n X w − ∑ k ∈ 0 , 1 p Y = y k H m e a n X w | Y = y k w ^ = arg max w 1 n ( n − 1 ) ∑ i ≠ j log x i − x j w 2 − ∑ k ∈ 0 , 1 p Y = y k n k ( n k − 1 ) ∑ i k ≠ j k log x i k − x j k w 2 n k y k ∑ k ∈ { 0 , 1 } n k = n I m e a n ( X w , Y ) w ∂ I m e a n ∂ w = 2 n ( n − 1 ) ∑ i ≠ j x i − x j x i − x j T w x i − x j w 2 − ∑ k ∈ { 0 , 1 } 2 p Y = y k n k ( n k − 1 ) ∑ i k ≠ j k x i k − x j k x i k − x j k T w x i − x j w 2 4. Simulation An extensive simulation study was conducted to compare the efficiencies of those methods on combination of continuous variables (or biomarkers). Imitating but also enriching the one performed previously in [ 1 In all settings, we assumed mean values μ 0 = 0 y 0 y 1 μ 1 = 0.4 i d n − 1 d 0 ≤ i ≤ d − 1 y 0 , 1 d 0.5 ≤ A U C ≤ 0.75 0.7 We used multivariate normal variables with equal and not equal covariance structures to generate normal, beta, and gamma variates using normal copula and inverse transform sampling. When covariances are equal, we fixed Σ 0 = Σ 1 = ( 1 − γ ) I d x d + γ J d x d γ = 0.15 Σ 0 = 0.9 I d x d + 0.1 J d x d Σ 1 = ( 1 − γ ) I d x d + γ J d x d For each setting, 1000 datasets were generated and randomly divided into two sets of equal size. One set was used for training and the other for testing. Coefficient estimates obtained from training datasets were recorded for each method. Using the test datasets, linear combinations were computed based on the previously estimated coefficients, and corresponding performance metrics were evaluated. These metrics included AUC, Area Under the Precision–Recall Curve (AUPRC), and Matthews Correlation Coefficient (MCC). In addition to mean values, 95% confidence intervals and median values were reported for each metric. Sample sizes were set to n = 25 ,  100 ,  200 d = 3 , 5 , 10 , 15 n 0 = 2 n 1 y = 1 n 1 = 12 , 50 , 100 Table 1 Table 2 Table 3 Supplementary Material All simulations were conducted using Julia version 1.11.0-rc3. We utilized the Ipopt.jl package version 1.11.0 for nonlinear constrained optimization in entropy-based methods, the GLM.jl package version 1.9.0 for logistic regression, and custom gradient ascent code for the mutual information maximization method. The code used in the simulations will be released as public Julia and R packages in future publications. Interested researchers may contact the author for access. 5. Application to Real-Life Data We used publicly available Wisconsin datasets for the prognosis [ 18 19 0 / 1 The prognostic dataset has 198 observations while the diagnostic has 569. Frequencies of y 1 47 / 198 212 / 569 Predictive ability of these variables are given in Table 4 Table 4 We performed logistic regression and entropy optimization techniques described in Section 3 Table 5 Table 6 Finally, calculating linear combinations using those coefficients, we obtained AUC, AUPRC, and MCC values given in Table 7 Table 8 6. Discussion and Conclusions The primary objective of this study was to explore how information-theoretical methods can be applied to construct linear combinations of continuous variables in the context of a binary classification problem. We addressed this question by introducing four distinct approaches (MaxEnt, MinCrEnt, MinRelEnt, and MaxMutInf), each grounded in fundamental principles of information theory. We believe that identifying and formalizing these approaches may guide future research directions by drawing greater attention to information-theoretical concepts and encouraging their broader application in this problem setting. This study also represents the first systematic evaluation of information-theoretic approaches applied in this setting. Earlier methods were often constrained by strong distributional assumptions about biomarkers, such as normality and equal or proportional covariance structures. Additionally, the number of predictors they could handle was typically limited to just two. More recent approaches have relaxed these assumptions, allowing for the inclusion of a larger number of biomarkers in linear combinations. However, most of these methods rely on performance metrics like sensitivity, specific segments of the ROC curve, or the AUC (Area Under the Curve) as the optimization objective. Some later methods extended their applicability to multi-class classification problems using metrics such as the Volume Under the Surface (VUS) or the Hypervolume Under the Manifold (HUM). Among these, the SCOR algorithm [ 10 1 Notably, methods established in [ 11 13 An important distinction of our proposed methods is that, whereas previous methods primarily aimed to maximize a metric such as the AUC value, the proposed methods in this article rely solely on information-theoretic criteria and never incorporated AUC or any other metric as an optimization objective. As another distinction, unlike many previous studies that relied on a single evaluation metric, our assessment of model performance employed a comprehensive set of three metrics: AUC, AUPRC, and MCC. This multi-faceted approach provides a more robust and nuanced comparison of model performance across different aspects of classification quality. The proposed methods here are straightforward to apply in binary classification problems or to be extended into a multiclass setting. These methods are computationally simpler than most existing approaches, with the exception of MaxMutInf. Due to the need to compute complex gradient functions when optimizing mutual information, MaxMutInf is relatively more challenging to implement. Extending our methods into classification problems involving more than two outcome levels may represent a valuable direction for future research. The three entropy-based methods (MaxEnt, MinCrEnt, and MinRelEnt) demonstrated test performances that were consistently comparable to those derived from logistic regression, across all simulation conditions and real data applications. The MaxMutInf method yielded slightly greater test AUC, AUPRC, and MCC values compared to the other approaches, particularly in simulation settings involving Beta and Gamma distributed data. The differences were more pronounced in simulations with smaller sample sizes. However, in some settings involving normally distributed data, the MaxMutInf method produced test AUC and AUPRC metrics that were comparable or even marginally lower than those of other methods. The Maximum Mutual Information method appeared more robust to distributional asymmetries, as well as to smaller and unequal sample sizes, except in a few settings involving normally distributed variables. Exploring and implementing different differential entropy estimators beyond the Kozachenko–Leonenko entropy estimator may be a future research direction that could potentially reveal more insights into the performance of Mutual Information Maximization. Acknowledgments We thank Yasemin Öztürk for her help in building this manuscript. We would also like to thank the anonymous reviewers for their insightful comments that led to significant contributions to the manuscript. Disclaimer/Publisher’s Note: Supplementary Materials The following supporting information can be downloaded at: https://www.mdpi.com/article/10.3390/e27090985/s1 Author Contributions Software, M.S.İ.; Validation, M.S.İ.; Formal Analysis, M.S.İ.; Resources, M.S.İ.; Writing—Original Draft, M.S.İ.; Writing—Review & Editing, M.S.İ.; Visualization, M.S.İ.; Supervision, P.Ö. All authors have read and agreed to the published version of the manuscript. Data Availability Statement Winconsin datasets can be found at https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic https://archive.ics.uci.edu/dataset/16/breast+cancer+wisconsin+prognostic Conflicts of Interest The authors declare no conflict of interest. Abbreviations The following abbreviations are used in this manuscript: ROC Receiver Operating Characteristics AUC Area under the ROC curve AUPRC Area under the precision recall curve MCC Matthews Correlation Coefficient MaxMutInf Maximum Mutual Information MaxEnt Maximum Entropy MinCrEnt Minimum Cross Entropy MinRelEnt Minimum Relative Entropy LogRes Logistic Regression References 1. Yan L. Tian L. Liu S. Combining large number of weak biomarkers based on AUC Stat. Med. 2015 34 3811 3830 10.1002/sim.6600 26227901 PMC5823017 2. Su J.Q. Liu J.S. Linear Combinations of Multiple Diagnostic Markers J. Am. Stat. Assoc. 1993 88 1350 1355 10.1080/01621459.1993.10476417 3. Liu A. Schisterman E.F. Zhu Y. On linear combinations of biomarkers to improve diagnostic accuracy Stat. Med. 2005 24 37 47 10.1002/sim.1922 15515132 4. Pepe M.S. Thompson M.L. Combining diagnostic test results to increase accuracy Biostatistics 2000 1 123 140 10.1093/biostatistics/1.2.123 12933515 5. Pepe M.S. Cai T. Longton G. Combining Predictors for Classification Using the Area under the Receiver Operating Characteristic Curve Biometrics 2006 62 221 229 10.1111/j.1541-0420.2005.00420.x 16542249 6. Liu C. Liu A. Halabi S. A min–max combination of biomarkers to improve diagnostic accuracy Stat. Med. 2011 30 2005 2014 10.1002/sim.4238 21472763 PMC3116024 7. Kang L. Xiong C. Crane P. Tian L. Linear combinations of biomarkers to improve diagnostic accuracy with three ordinal diagnostic categories Stat. Med. 2013 32 631 643 10.1002/sim.5542 22865796 PMC4351049 8. Fong Y. Yin S. Huang Y. Combining biomarkers linearly and nonlinearly for classification using the area under the ROC curve Stat. Med. 2016 35 3792 3809 10.1002/sim.6956 27058981 PMC4965290 9. Lloyd C.J. Using Smoothed Receiver Operating Characteristic Curves to Summarize and Compare Diagnostic Systems J. Am. Stat. Assoc. 1998 93 1356 1364 10.1080/01621459.1998.10473797 10. Das P. De D. Maiti R. Kamal M. Hutcheson K.A. Fuller C.D. Chakraborty B. Peterson C.B. Estimating the optimal linear combination of predictors using spherically constrained optimization BMC Bioinform. 2022 23 436 10.1186/s12859-022-04953-y 36261805 PMC9583504 11. Muhammad N. Coolen-Maturi T. Coolen F.P. Nonparametric predictive inference with parametric copulas for combining bivariate diagnostic tests Stat. Optim. Inf. Comput. 2018 6 398 408 10.19139/soic.v6i3.579 12. Islam S. Anand S. Hamid J. Thabane L. Beyene J. A copula-based method of classifying individuals into binary disease categories using dependent biomarkers Stat. Methods Appl. 2020 29 871 897 10.1007/s10260-020-00507-9 13. Jaynes E.T. Information Theory and Statistical Mechanics Phys. Rev. 1957 106 620 630 10.1103/PhysRev.106.620 14. Shore J. Johnson R. Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy IEEE Trans. Inf. Theory 1980 26 26 37 10.1109/TIT.1980.1056144 15. Golan A. Judge G. Perloff J.M. A Maximum Entropy Approach to Recovering Information from Multinomial Response Data J. Am. Stat. Assoc. 1996 91 841 853 10.2307/2291679 16. Banavar J. Maritan A. The maximum relative entropy principle arXiv 2007 10.48550/arXiv.cond-mat/0703622 cond-mat/0703622 17. Faivishevsky L. Goldberger J. Dimensionality reduction based on non-parametric mutual information Neurocomputing 2012 80 31 37 10.1016/j.neucom.2011.07.028 18. Wolberg W. Street W. Mangasarian O. Breast Cancer Wisconsin (Prognostic). UCI Machine Learning Repository 1995 Available online: https://archive.ics.uci.edu/dataset/16/breast+cancer+wisconsin+prognostic (accessed on 17 April 2025) 19. Wolberg W. Mangasarian O. Street N. Street W. Breast Cancer Wisconsin (Diagnostic). UCI Machine Learning Repository, 1993 1993 Available online: https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic (accessed on 17 April 2025) entropy-27-00985-t001_Table 1 Table 1 Mean AUC values with 95% confidence intervals obtained for multivariate Normal distributions.  LogRes MaxMutInf MaxEnt MinRelEnt MinCrEnt  n  d  Covariance  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median 25 3 Equal 0.615 (0.545,0.673) 0.597 0.617 (0.547,0.669) 0.603 0.615 (0.545,0.673) 0.597 0.615 (0.545,0.673) 0.597 0.615 (0.545,0.673) 0.597 25 5 Equal 0.619 (0.551,0.673) 0.604 0.625 (0.557,0.686) 0.609 0.62 (0.552,0.673) 0.608 0.62 (0.552,0.673) 0.608 0.62 (0.552,0.673) 0.607 25 10 Equal 0.613 (0.545,0.662) 0.597 0.634 (0.555,0.693) 0.622 0.612 (0.545,0.667) 0.597 0.612 (0.545,0.667) 0.597 0.61 (0.545,0.662) 0.591 25 15 Equal 0.614 (0.551,0.662) 0.597 0.634 (0.558,0.691) 0.622 0.612 (0.545,0.667) 0.596 0.612 (0.545,0.667) 0.596 0.611 (0.545,0.66) 0.596 100 3 Equal 0.599 (0.555,0.639) 0.598 0.585 (0.546,0.619) 0.580 0.599 (0.556,0.638) 0.599 0.599 (0.556,0.638) 0.599 0.599 (0.556,0.638) 0.599 100 5 Equal 0.601 (0.556,0.642) 0.596 0.593 (0.55,0.631) 0.591 0.601 (0.557,0.643) 0.597 0.601 (0.557,0.643) 0.597 0.601 (0.557,0.643) 0.597 100 10 Equal 0.612 (0.571,0.652) 0.610 0.608 (0.569,0.646) 0.606 0.612 (0.572,0.653) 0.612 0.612 (0.572,0.653) 0.612 0.612 (0.572,0.653) 0.612 100 15 Equal 0.617 (0.575,0.658) 0.617 0.616 (0.575,0.653) 0.615 0.618 (0.575,0.657) 0.617 0.618 (0.575,0.657) 0.617 0.618 (0.575,0.657) 0.617 200 3 Equal 0.607 (0.577,0.636) 0.608 0.582 (0.547,0.612) 0.580 0.607 (0.578,0.636) 0.607 0.607 (0.578,0.636) 0.607 0.607 (0.578,0.636) 0.607 200 5 Equal 0.614 (0.587,0.642) 0.614 0.592 (0.562,0.62) 0.592 0.615 (0.587,0.642) 0.615 0.615 (0.587,0.642) 0.615 0.615 (0.587,0.642) 0.615 200 10 Equal 0.633 (0.605,0.664) 0.635 0.61 (0.582,0.638) 0.611 0.634 (0.605,0.664) 0.636 0.634 (0.605,0.664) 0.636 0.634 (0.605,0.664) 0.636 200 15 Equal 0.641 (0.612,0.673) 0.643 0.616 (0.588,0.646) 0.616 0.642 (0.613,0.673) 0.644 0.642 (0.613,0.673) 0.644 0.642 (0.613,0.673) 0.644 25 3 Not Equal 0.615 (0.545,0.669) 0.597 0.617 (0.547,0.673) 0.603 0.614 (0.545,0.673) 0.597 0.614 (0.545,0.673) 0.597 0.614 (0.545,0.673) 0.597 25 5 Not Equal 0.62 (0.551,0.673) 0.604 0.626 (0.558,0.687) 0.610 0.621 (0.551,0.673) 0.609 0.621 (0.551,0.673) 0.609 0.621 (0.551,0.673) 0.609 25 10 Not Equal 0.615 (0.549,0.667) 0.603 0.637 (0.564,0.7) 0.623 0.614 (0.547,0.667) 0.603 0.614 (0.547,0.667) 0.603 0.612 (0.545,0.667) 0.596 25 15 Not Equal 0.615 (0.551,0.667) 0.603 0.638 (0.564,0.699) 0.623 0.613 (0.547,0.667) 0.597 0.613 (0.547,0.667) 0.597 0.611 (0.547,0.662) 0.596 100 3 Not Equal 0.599 (0.556,0.638) 0.600 0.586 (0.546,0.621) 0.582 0.599 (0.556,0.639) 0.599 0.599 (0.556,0.639) 0.599 0.599 (0.556,0.639) 0.599 100 5 Not Equal 0.602 (0.559,0.643) 0.599 0.595 (0.551,0.633) 0.593 0.602 (0.558,0.644) 0.600 0.602 (0.558,0.644) 0.600 0.602 (0.558,0.644) 0.600 100 10 Not Equal 0.614 (0.574,0.653) 0.614 0.613 (0.574,0.65) 0.611 0.615 (0.575,0.655) 0.615 0.615 (0.575,0.655) 0.615 0.615 (0.575,0.655) 0.615 100 15 Not Equal 0.62 (0.578,0.659) 0.620 0.621 (0.582,0.658) 0.621 0.621 (0.579,0.66) 0.622 0.621 (0.579,0.66) 0.622 0.621 (0.579,0.66) 0.622 200 3 Not Equal 0.607 (0.578,0.635) 0.607 0.583 (0.548,0.614) 0.581 0.607 (0.579,0.636) 0.607 0.607 (0.579,0.636) 0.607 0.607 (0.579,0.636) 0.607 200 5 Not Equal 0.616 (0.589,0.644) 0.615 0.594 (0.564,0.623) 0.594 0.616 (0.589,0.643) 0.615 0.616 (0.589,0.643) 0.615 0.616 (0.589,0.643) 0.615 200 10 Not Equal 0.636 (0.608,0.667) 0.637 0.615 (0.586,0.643) 0.617 0.637 (0.61,0.666) 0.638 0.637 (0.61,0.666) 0.638 0.637 (0.61,0.666) 0.638 200 15 Not Equal 0.644 (0.616,0.676) 0.646 0.623 (0.594,0.653) 0.622 0.645 (0.616,0.676) 0.647 0.645 (0.616,0.676) 0.647 0.645 (0.616,0.676) 0.647 Note: Statistics presented in this table were derived from test datasets described in Section 4 entropy-27-00985-t002_Table 2 Table 2 Mean AUC values with 95% confidence intervals obtained for Gamma distributions.  LogRes MaxMutInf MaxEnt MinRelEnt MinCrEnt  n  d  Covariance  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median 25 3 Equal 0.613 (0.545,0.66) 0.600 0.63 (0.564,0.682) 0.620 0.613 (0.545,0.661) 0.603 0.613 (0.545,0.661) 0.603 0.613 (0.545,0.661) 0.603 25 5 Equal 0.613 (0.551,0.66) 0.596 0.636 (0.558,0.695) 0.627 0.612 (0.545,0.662) 0.597 0.612 (0.545,0.662) 0.597 0.612 (0.545,0.662) 0.597 25 10 Equal 0.601 (0.54,0.643) 0.584 0.641 (0.565,0.705) 0.635 0.604 (0.542,0.647) 0.590 0.604 (0.542,0.647) 0.590 0.602 (0.543,0.643) 0.590 25 15 Equal 0.606 (0.544,0.654) 0.590 0.649 (0.571,0.712) 0.643 0.606 (0.545,0.649) 0.590 0.606 (0.545,0.649) 0.590 0.604 (0.545,0.649) 0.584 100 3 Equal 0.593 (0.55,0.631) 0.592 0.603 (0.563,0.64) 0.602 0.594 (0.549,0.632) 0.592 0.594 (0.549,0.632) 0.592 0.594 (0.549,0.632) 0.592 100 5 Equal 0.596 (0.555,0.633) 0.592 0.617 (0.581,0.654) 0.616 0.597 (0.555,0.633) 0.593 0.597 (0.555,0.633) 0.593 0.597 (0.555,0.633) 0.593 100 10 Equal 0.596 (0.553,0.632) 0.594 0.629 (0.591,0.667) 0.629 0.597 (0.554,0.633) 0.597 0.597 (0.554,0.633) 0.597 0.597 (0.554,0.633) 0.597 100 15 Equal 0.591 (0.55,0.628) 0.589 0.639 (0.602,0.678) 0.639 0.593 (0.552,0.629) 0.590 0.593 (0.552,0.629) 0.590 0.593 (0.552,0.629) 0.590 200 3 Equal 0.602 (0.574,0.629) 0.604 0.602 (0.574,0.63) 0.604 0.602 (0.576,0.629) 0.605 0.602 (0.576,0.629) 0.605 0.602 (0.576,0.629) 0.605 200 5 Equal 0.608 (0.58,0.637) 0.608 0.614 (0.586,0.644) 0.614 0.608 (0.58,0.638) 0.609 0.608 (0.58,0.638) 0.609 0.608 (0.58,0.638) 0.609 200 10 Equal 0.612 (0.583,0.642) 0.614 0.63 (0.604,0.657) 0.631 0.613 (0.584,0.643) 0.614 0.613 (0.584,0.643) 0.614 0.613 (0.584,0.643) 0.614 200 15 Equal 0.616 (0.586,0.646) 0.616 0.642 (0.615,0.669) 0.643 0.617 (0.587,0.646) 0.616 0.617 (0.587,0.646) 0.616 0.617 (0.587,0.646) 0.616 25 3 Not Equal 0.613 (0.545,0.662) 0.603 0.63 (0.564,0.682) 0.622 0.614 (0.545,0.667) 0.604 0.614 (0.545,0.667) 0.604 0.614 (0.545,0.667) 0.604 25 5 Not Equal 0.613 (0.551,0.663) 0.597 0.636 (0.558,0.695) 0.627 0.612 (0.545,0.662) 0.597 0.612 (0.545,0.662) 0.597 0.613 (0.545,0.662) 0.597 25 10 Not Equal 0.601 (0.54,0.647) 0.584 0.643 (0.567,0.707) 0.636 0.604 (0.545,0.647) 0.590 0.604 (0.545,0.647) 0.590 0.603 (0.545,0.647) 0.590 25 15 Not Equal 0.607 (0.539,0.654) 0.591 0.653 (0.577,0.718) 0.647 0.606 (0.54,0.649) 0.590 0.606 (0.54,0.649) 0.590 0.604 (0.54,0.65) 0.590 100 3 Not Equal 0.594 (0.551,0.632) 0.593 0.603 (0.563,0.641) 0.602 0.594 (0.552,0.633) 0.593 0.594 (0.552,0.633) 0.593 0.594 (0.552,0.633) 0.593 100 5 Not Equal 0.599 (0.558,0.636) 0.596 0.619 (0.581,0.656) 0.617 0.599 (0.557,0.637) 0.597 0.599 (0.557,0.637) 0.597 0.599 (0.557,0.637) 0.597 100 10 Not Equal 0.6 (0.557,0.638) 0.600 0.632 (0.593,0.669) 0.631 0.601 (0.558,0.639) 0.601 0.601 (0.558,0.639) 0.601 0.601 (0.558,0.639) 0.601 100 15 Not Equal 0.596 (0.556,0.633) 0.594 0.643 (0.607,0.681) 0.643 0.597 (0.557,0.635) 0.595 0.597 (0.557,0.635) 0.595 0.597 (0.557,0.635) 0.595 200 3 Not Equal 0.603 (0.576,0.63) 0.606 0.603 (0.575,0.631) 0.604 0.603 (0.577,0.63) 0.606 0.603 (0.577,0.63) 0.606 0.603 (0.577,0.63) 0.606 200 5 Not Equal 0.61 (0.583,0.639) 0.611 0.615 (0.587,0.645) 0.615 0.611 (0.583,0.64) 0.611 0.611 (0.583,0.64) 0.611 0.611 (0.583,0.64) 0.611 200 10 Not Equal 0.617 (0.587,0.646) 0.618 0.633 (0.606,0.66) 0.633 0.617 (0.588,0.647) 0.619 0.617 (0.588,0.647) 0.619 0.617 (0.588,0.647) 0.619 200 15 Not Equal 0.621 (0.591,0.651) 0.622 0.646 (0.619,0.672) 0.646 0.622 (0.592,0.652) 0.622 0.622 (0.592,0.652) 0.622 0.622 (0.592,0.652) 0.622 Note: Statistics presented in this table were derived from test datasets described in Section 4 entropy-27-00985-t003_Table 3 Table 3 Mean AUC values with 95% confidence intervals obtained for Beta distributions.  LogRes MaxMutInf MaxEnt MinRelEnt MinCrEnt  n  d  Covariance  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median  Mean (95%CI)  Median 25 3 Equal 0.631 (0.558,0.688) 0.617 0.648 (0.571,0.714) 0.641 0.63 (0.558,0.692) 0.617 0.63 (0.558,0.692) 0.617 0.63 (0.558,0.692) 0.617 25 5 Equal 0.622 (0.552,0.679) 0.610 0.656 (0.578,0.723) 0.649 0.623 (0.552,0.68) 0.610 0.623 (0.552,0.68) 0.610 0.623 (0.552,0.68) 0.610 25 10 Equal 0.614 (0.549,0.667) 0.597 0.669 (0.59,0.737) 0.667 0.616 (0.551,0.673) 0.604 0.616 (0.551,0.673) 0.604 0.615 (0.545,0.667) 0.604 25 15 Equal 0.611 (0.546,0.662) 0.593 0.679 (0.604,0.75) 0.679 0.614 (0.545,0.667) 0.600 0.614 (0.545,0.667) 0.600 0.61 (0.545,0.667) 0.596 100 3 Equal 0.626 (0.583,0.665) 0.628 0.63 (0.592,0.668) 0.630 0.626 (0.583,0.666) 0.629 0.626 (0.583,0.666) 0.629 0.626 (0.583,0.666) 0.629 100 5 Equal 0.634 (0.595,0.673) 0.636 0.646 (0.609,0.685) 0.646 0.634 (0.595,0.674) 0.637 0.634 (0.595,0.674) 0.637 0.634 (0.595,0.674) 0.637 100 10 Equal 0.636 (0.599,0.673) 0.637 0.663 (0.627,0.7) 0.664 0.637 (0.599,0.674) 0.638 0.637 (0.599,0.674) 0.638 0.637 (0.599,0.674) 0.638 100 15 Equal 0.635 (0.595,0.678) 0.635 0.676 (0.64,0.714) 0.677 0.636 (0.595,0.678) 0.636 0.636 (0.595,0.678) 0.636 0.636 (0.595,0.678) 0.636 200 3 Equal 0.638 (0.611,0.665) 0.640 0.631 (0.604,0.661) 0.633 0.638 (0.612,0.665) 0.640 0.638 (0.612,0.665) 0.640 0.638 (0.612,0.665) 0.640 200 5 Equal 0.648 (0.621,0.676) 0.649 0.645 (0.617,0.675) 0.646 0.648 (0.621,0.677) 0.650 0.648 (0.621,0.677) 0.650 0.648 (0.621,0.677) 0.650 200 10 Equal 0.658 (0.634,0.684) 0.658 0.664 (0.638,0.691) 0.664 0.659 (0.633,0.684) 0.659 0.659 (0.633,0.684) 0.659 0.659 (0.633,0.684) 0.659 200 15 Equal 0.665 (0.638,0.693) 0.666 0.679 (0.655,0.705) 0.680 0.666 (0.638,0.694) 0.666 0.666 (0.638,0.694) 0.666 0.666 (0.638,0.694) 0.666 25 3 Not Equal 0.632 (0.563,0.692) 0.620 0.648 (0.571,0.714) 0.643 0.631 (0.564,0.691) 0.620 0.631 (0.564,0.691) 0.620 0.631 (0.564,0.691) 0.620 25 5 Not Equal 0.624 (0.551,0.68) 0.615 0.658 (0.578,0.725) 0.651 0.625 (0.552,0.682) 0.613 0.625 (0.552,0.682) 0.613 0.625 (0.552,0.682) 0.613 25 10 Not Equal 0.616 (0.551,0.667) 0.600 0.672 (0.593,0.74) 0.667 0.618 (0.551,0.673) 0.604 0.618 (0.551,0.673) 0.604 0.617 (0.551,0.669) 0.607 25 15 Not Equal 0.614 (0.547,0.667) 0.597 0.686 (0.61,0.76) 0.686 0.616 (0.545,0.669) 0.603 0.616 (0.545,0.669) 0.603 0.612 (0.545,0.667) 0.597 100 3 Not Equal 0.628 (0.585,0.667) 0.629 0.631 (0.593,0.669) 0.630 0.628 (0.585,0.669) 0.630 0.628 (0.585,0.669) 0.630 0.628 (0.585,0.669) 0.630 100 5 Not Equal 0.638 (0.599,0.677) 0.641 0.649 (0.612,0.688) 0.648 0.638 (0.599,0.678) 0.641 0.638 (0.599,0.678) 0.641 0.638 (0.599,0.678) 0.641 100 10 Not Equal 0.642 (0.605,0.681) 0.644 0.668 (0.631,0.705) 0.669 0.643 (0.606,0.682) 0.645 0.643 (0.606,0.682) 0.645 0.643 (0.606,0.682) 0.645 100 15 Not Equal 0.643 (0.605,0.687) 0.643 0.682 (0.647,0.72) 0.684 0.643 (0.604,0.687) 0.644 0.643 (0.604,0.687) 0.644 0.643 (0.604,0.687) 0.644 200 3 Not Equal 0.64 (0.613,0.667) 0.642 0.632 (0.605,0.662) 0.634 0.64 (0.613,0.667) 0.642 0.64 (0.613,0.667) 0.642 0.64 (0.613,0.667) 0.642 200 5 Not Equal 0.652 (0.624,0.68) 0.653 0.647 (0.618,0.676) 0.648 0.652 (0.624,0.68) 0.653 0.652 (0.624,0.68) 0.653 0.652 (0.624,0.68) 0.653 200 10 Not Equal 0.665 (0.64,0.692) 0.665 0.67 (0.643,0.695) 0.670 0.665 (0.64,0.692) 0.665 0.665 (0.64,0.692) 0.665 0.665 (0.64,0.692) 0.665 200 15 Not Equal 0.673 (0.647,0.701) 0.674 0.685 (0.66,0.711) 0.687 0.674 (0.648,0.702) 0.674 0.674 (0.648,0.702) 0.674 0.674 (0.648,0.702) 0.674 Note: Statistics presented in this table were derived from test datasets described in Section 4 entropy-27-00985-t004_Table 4 Table 4 AUC values for the first five predictors in each dataset. Variable Prognostic Diagnostic radius_mean 0.611 0.938 texture_mean 0.535 0.776 perimeter_mean 0.613 0.947 area_mean 0.618 0.938 smoothness_mean 0.532 0.722 entropy-27-00985-t005_Table 5 Table 5 Coefficients obtained for prognostic dataset. Prognostic LogRes MaxMutInf MaxEnt MinRelEnt MinCrEnt radius_mean −0.609209 0.537218 −2.03368 −2.03368 −2.03368 texture_mean −0.0518859 0.995454 −0.159585 −0.159585 −0.159585 perimeter_mean 4.83955 × 10 − 5 0.0269349 −0.0419467 −0.0419467 −0.0419467 area_mean 0.00667978 0.0693375 2.45539 2.45539 2.45539 smoothness_mean 2.93637 0.115625 0.0284566 0.0284566 0.0284566 entropy-27-00985-t006_Table 6 Table 6 Coefficients obtained for the diagnostic dataset. Diagnostic LogRes MaxMutInf MaxEnt MinRelEnt MinCrEnt radius_mean −6.27525 0.538316 −1.98606 −1.98606 −17.6992 texture_mean 0.3641 0.997358 1.69841 1.69841 1.57074 perimeter_mean 0.607157 0.0275477 7.14262 7.14262 13.7528 area_mean 0.0417776 0.0702387 −0.053743 −0.053743 10.5983 smoothness_mean 118.462 0.115856 1.74935 1.74935 1.67574 entropy-27-00985-t007_Table 7 Table 7 Predictive ability of calculated linear combinations for prognostic data. Metric LogRes MaxMutInf MaxEnt MinRelEnt MinCrEnt AUC 0.626 0.610 0.618 0.618 0.618 AUPRC 0.388 0.332 0.340 0.340 0.340 MCC 0.194 0.165 0.0203 0.0203 0.0203 entropy-27-00985-t008_Table 8 Table 8 Predictive ability of calculated linear combinations for diagnostic data. Metric LogRes MaxMutInf MaxEnt MinRelEnt MinCrEnt AUC 0.984 0.951 0.953 0.953 0.940 AUPRC 0.973 0.935 0.936 0.936 0.921 MCC 0.853 0.577 0.603 0.603 0.293 ",
  "metadata": {
    "Title of this paper": "Breast Cancer Wisconsin (Diagnostic). UCI Machine Learning Repository, 1993",
    "Journal it was published in:": "Entropy",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12469204/"
  }
}
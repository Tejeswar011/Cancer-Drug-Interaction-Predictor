{
  "title": "Paper_574",
  "abstract": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12480523 PMC12480523.1 12480523 12480523 41023362 10.1038/s41598-025-18899-7 18899 1 Article Three-dimensional reconstruction of lung tumors from computed tomography scans using adversarial and transductive learning He Zhisen 1 Jamel Leila 2 Huang Danyi 3 Jiang Gaozhe 4 Shaikh Zaffar Ahmed 5 6 Aljohani Khan Md.Abeer 7 Mousavirad Seyed Jalaleddin seyedjalaleddin.mousavirad@miun.se 8 1 https://ror.org/051fd9666 grid.67105.35 0000 0001 2164 3847 Department of Electrical, Computer, and Systems Engineering, Case Western Reserve University, 2 https://ror.org/05b0cyh02 grid.449346.8 0000 0004 0501 7602 Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, 3 https://ror.org/00hj8s172 grid.21729.3f 0000 0004 1936 8729 Department of Chemical Engineering, Columbia University, 4 https://ror.org/02zhqgq86 grid.194645.b 0000 0001 2174 2757 Department of Chemistry, The University of Hong Kong, 5 https://ror.org/02zwhz281 grid.449433.d 0000 0004 4907 7957 Department of Computer Science and Information Technology, Benazir Bhutto Shaheed University Lyari, 6 https://ror.org/02s376052 grid.5333.6 0000 0001 2183 9049 School of Engineering, École Polytechnique Fédérale de Lausanne, 7 https://ror.org/03q648j11 grid.428986.9 0000 0001 0373 6302 School of Biomedical Engineering, Hainan University, 8 https://ror.org/019k1pd13 grid.29050.3e 0000 0001 1530 0805 Department of Computer and Electrical Engineering, Mid Sweden University, 29 9 2025 2025 15 478255 33323 16 3 2025 4 9 2025 29 09 2025 01 10 2025 01 10 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by/4.0/ Open Access http://creativecommons.org/licenses/by/4.0/ Lung cancer is a critical health issue, and early detection is crucial for enhancing patient outcomes. This study presents a novel framework for generating three-dimensional (3D) representations of lung tumors from computed tomography (CT) scans, addressing three key challenges in the analysis process. Firstly, we address the precise segmentation of lung tissues, which is complicated by a high proportion of non-lung pixels that skew the classifier. Our method uses a customized generative adversarial network (GAN) enhanced with an off-policy proximal policy optimization (PPO) strategy. This strategy enhances segmentation performance by addressing inherent classifier biases and implementing a reward system to more accurately identify minority samples. Secondly, the framework enhances tumor detection in the segmented areas by employing a specialized GAN trained with an adversarial loss, which helps the generator create tumor regions that match real ones in both shape and internal features, even when contrast is low or boundaries are unclear. Thirdly, after tumor detection, the EfficientNet model extracts essential features for 3D reconstruction. The features are then enhanced by a spatial attention-based transductive long short-term memory (TLSTM) network for better performance. The TLSTM network enhances performance by assigning greater weight to samples near the test point within a transductive learning framework. Tested on the Lung Image Database Consortium Image Collection (LIDC-IDRI) dataset, our methodology achieved Hausdorff distance (HD) and Euclidean distance (ED) metrics of 0.648 and 0.985, respectively, indicating superior performance compared to existing methods. Our research introduces a clinical tool that significantly boosts the capabilities of radiologists in diagnosing and planning treatment for lung cancer. Code is publicly available at https://github.com/ZhisenHe/3D-representation/ Keywords Lung cancer Three-dimensional tumor reconstruction Generative adversarial network Off-policy proximal policy optimization Transductive learning Subject terms Biomedical engineering Computational science Mid Sweden University Open access funding provided by Mid Sweden University. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © Springer Nature Limited 2025 Introduction Lung cancer poses a significant threat to public health, being one of the most lethal cancers that results in a substantial number of fatalities globally each year. The World Health Organization (WHO) reports approximately 2.09 million new cases annually, culminating in around 1.76 million fatalities 1 2 3 4 6 7 To date, advanced deep learning techniques have been applied to 3D modeling. Deep learning models are trained on large datasets of carefully labeled images. These models are highly effective at identifying complex patterns that often go unnoticed by human radiologists and traditional machine-learning techniques 8 9 10 2 10 Traditional methods for addressing class imbalance in lung segmentation operate at two levels. At the data level, techniques include undersampling the majority class, oversampling the minority class, and generating synthetic samples. At the algorithmic level, loss functions are reweighted to emphasize signals from the minority class. Although these strategies are partially effective, they often lead to overfitting, poor generalizability, and limited adaptability to complex spatial patterns in medical images 11 12 13 To address the challenges of accurate tumor detection, particularly in regions with unclear boundaries or low contrast, our model incorporates a composite loss function consisting of classification loss, bounding box regression loss, and a novel adversarial loss component. Most conventional methods use only classification and bounding box regression losses for tumor classification and localization 10 After addressing challenges in lung segmentation and tumor detection, the next challenge lies in improving temporal modeling during 3D tumor reconstruction. We utilize a TLSTM network 14 After addressing the three core challenges separately, we now present the overall architecture of the proposed framework. This framework integrates the segmentation, detection, and reconstruction modules into a unified 3D modeling pipeline. The article introduces a triple-tiered strategy for creating 3D representations of lung cancer from CT scans. Initially, we employ a GAN featuring a U-shaped network (U-Net) architecture to segment the CT images. To address the class imbalance problem, we use the off-policy PPO approach. The discriminator applies dilated convolution layers to improve the generator output. It distinguishes real images from artificial ones and emphasizes essential features during training. In the second phase, the segmented images are processed using another GAN configured with the mask region-based convolutional neural network (Mask R-CNN) architecture. This network accurately identifies and locates target areas within the images for precise tumor detection. To reinforce the accuracy of tumor localization, the generator applies a specific loss function, ensuring alignment between synthetic outputs and actual CT data. The discriminator supports output validation using dilated convolutions for robust feedback. The final phase involves 3D modeling, where a spatial attention-based TLSTM network is paired with a GAN. At this stage, the tumor images from the second step are fed into the trained EfficientNet network to extract features. These features are then passed to the TLSTM and subsequently to the generator, which constructs a 3D model based on the TLSTM outputs. The effectiveness of our approach is robustly confirmed via extensive evaluation, and scrupulously compared with established methodologies utilizing the LIDC-IDRI dataset and standard performance indicators. This detailed juxtaposition underscores the superior performance and promising potential of our method to enhance the early detection of lung cancer. Table 1 The critical contributions of our model include:  Our model employs off-policy PPO to tackle the class imbalance commonly found in traditional lung segmentation methods. Off-policy PPO differs from on-policy methods as it utilizes data from various policies, which boosts the efficiency of the model. This feature is especially useful in medical contexts where data from infrequent but crucial conditions are limited. Additionally, we have incorporated a novel reward system within the off-policy PPO framework. This system focuses on the minority class by rewarding accurate identification and penalizing errors in classifying underrepresented classes, thereby encouraging a more balanced and fair classification process. A novel adversarial loss term is integrated into the generator loss function during tumor detection. Conventional methods rely only on classification and bounding box regression losses. In contrast, the adversarial component enhances the feature-level realism of predicted regions. This helps the model detect tumors more effectively, even when contrast is low or boundaries are unclear. This enhancement significantly reduces false detections and improves clinical reliability. The TLSTM method represents a substantial innovation in 3D tumor modeling. It enhances model training by weighing the influence of training samples based on their proximity to test samples. This approach significantly improves the precision and consistency in 3D representations, making it a valuable tool for accurate tumor morphology analysis. The structure of this article is as follows: relevant research on lung cancer is reviewed next, followed by the basic concepts. The proposed methodology is then outlined, after which the findings and analysis of our experiments are presented. Finally, the paper concludes with closing remarks.  Table 1 List of abbreviations and their definitions used in the study. Abbreviation Definition 3D Three-dimensional CT Computed tomography GAN Generative adversarial network PPO Proximal policy optimization TLSTM Transductive long short-term memory LIDC-IDRI Lung image database consortium image collection HD Hausdorff distance ED Euclidean distance WHO World Health Organization X-rays X-radiation MRI Magnetic resonance imaging DRL Deep reinforcement learning U-Net U-shaped network Mask R-CNN Mask region-based convolutional neural network AI Artificial intelligence U-Net++ U-shaped++ TB Tuberculosis LDANet Lung-dense attention network RSA Residual spatial attention GCA Gated channel attention DAGM Dual attention guidance module LDB Lightweight dense block PTB Positioned transpose block CXR Chest X-ray Xception Extreme inception ResNet-18 Residual network-18 COVID-19 Coronavirus disease 2019 FL Federated learning SSSOA Salp shuffled shepherd optimization algorithm VGG16 Visual geometry group 16 CAD Computer-aided diagnostic EfficientNet B3 Efficient network B3 T-Net T-shaped network CenterNet Center-based object detection network NASNet Neural architecture search network TFDM Differential memory WOA Whale optimization algorithm SVM Support vector machine CapsNet Capsule neural network WSTSA Wormhole and Salp swarm strategy enhanced tree-seed algorithm HM-LeNet Hybrid mobile LeNet SNP Single nucleotide polymorphism LDN Lightweight deep network ANN Artificial neural network ROI Region of interest 2D Two-dimensional GGO Ground glass opacity CBAM Convolutional block attention module ASPP Atrous spatial pyramid pooling ReLU Rectified linear unit DSC Dice similarity coefficient FSIM Feature similarity index measure APSO Adaptive particle swarm optimization XGBoost Extreme gradient boosting HRCT High-resolution computed tomography IoT Internet of things OFCMNN Optimized fuzzy C-means neural network KM-DTCL Kernel multilayer deep transfer convolutional learning Coarse Seg-net Coarse segmentation subnetwork Fine Seg-net Fine segmentation subnetwork Class-net Classification subnetwork ROC Receiver operating characteristic AUC Area under the curve HRNet High-resolution network DLN Deep learning nomogram ITF Intrathoracic fat IPN Intranodular and perinodular regions LASSO Least absolute shrinkage and selection operator PET positron emission tomography KAN Kolmogorov–Arnold networks SE Squeeze-and-excitation ViT Vision transformer Grad-CAM Gradient-weighted class activation mapping YOLOv8 You Only Look Once version 8 DCGAN Deep convolutional generative adversarial network FPN Feature pyramid network MSDA Multi-scale dilation attention GCSAM Global channel spatial attention mechanism CNDNet Candidate nodule detection network FPRNet False positive reduction network HPFF Hierarchical progressive feature fusion LUNA16 Lung nodule analysis 2016 GK Gustafson and Kessel TRPO Trust region policy optimization RNN Recurrent neural network MDP Markov decision process KL Kullback-Leibler EMD Earth Mover’s distance BN Batch normalization FNIH Foundation for the national institutes of health FDA Food and drug administration XML Extensible markup language IoU Intersection over union RPN Region proposal network ADAM Adaptive moment estimation GPU Graphics processing unit GB Gigabyte FGSM Fast gradient sign method Related works The advent of artificial intelligence (AI) has dramatically transformed various medical fields 15 17 18 23 24 27 This review meticulously divides the literature on lung cancer detection and analysis into three key sections: lung segmentation, tumor detection, and 3D tumor reconstruction. Each section discusses significant developments and methods that enhance diagnostic and treatment techniques in lung cancer care. Lung segmentation In 2023, Gite et al. 28 29 30 31 32 33 In 2024, Hasan et al. 34 35 36 37 38 39 40 41 42 In 2025, Vijayakumar et al. 43 44 45 46 47 48 49 50 51 52 Table 2 28 29 36 48 30 45 38 31 49 52 Our proposed method distinguishes itself by embedding an off-policy PPO strategy into a GAN-based segmentation framework. Unlike traditional static weighting methods, the PPO-based agent uses a reward system to learn how to focus on underrepresented lung regions and improve minority-class accuracy. The off-policy design enables the model to reuse past training experiences, thereby improving stability and generalization —a capability that is rarely addressed in existing segmentation methods. This reinforcement-driven segmentation process not only reduces bias but also avoids overfitting and the need for manual threshold tuning.  Table 2 Comparison of advanced models for lung segmentation. Authors Methodology Contribution Dataset Result Limitation Gite et al. 28 U-Net++ for lung segmentation Enhances TB detection with advanced segmentation 138 X-ray images IoU: 0.95 Specific to X-ray images, not other modalities Arvind et al. 29 Modified U-Net with dropouts Reduces overfitting, enhances model generalization 900 X-ray images Accuracy: 93.87% Optimized for lung segmentation, less versatile for other organs Chen et al. 30 LDANet combines RSA and GCA Enhances lung CT segmentation via dual attention LIDC-IDRI Dice similarity coefficient (DSC): 98.430% Sensitive to variations in imaging protocols Ghali et al. 31 Dual loss functions with five models Optimizes CXR segmentation for various lung conditions 662 X-ray images F1 score: 97.47% May not generalize across all imaging equipment types Murugappan et al. 32 DeepLabV3 + with various networks Enhances segmentation of lung and infected areas 750 CT images IoU: 0.9971 Performance varies with different network layers Ambesange et al. 33 FL with U-net and transfer learning Enhances lung X-ray segmentation while preserving data privacy 662 X-ray images Accuracy: 98.92% Limited by data variability across nodes Hasan et al. 34 Deeplabv3plus with Atrous Convolution Optimizes feature resolution for segmentation 558 X-ray images Accuracy: 97.42% May struggle with highly variable image quality Swaminathan et al. 35 Wiener filter, GAN with SSSOA, VGG16 classification Streamlines detection process with an integrated model LIDC-IDRI Accuracy: 97% Relies on high-quality pre-processed images Suji et al. 36 U-Net with EfficientNet-b3, transfer learning Optimizes nodule segmentation with transfer learning LIDC-IDRI IoU: 0.45 Performance variability across datasets Thangavel and Palanichamy 37 T-Net, CenterNet, NASNet with preprocessing Automates nodule classification efficiently LIDC-IDRI DSC: 99.07% May require fine-tuning for new datasets Cai et al. 38 GANs for image translation segmentation Enhances lung CT segmentation with GANs 267 CT images Accuracy: 89.63% May not handle all types of lung anomalies Ramos and Pineda 38 Tri-phase semi-automated segmentation with preprocessing Provides consistent, swift segmentation results 267 CT images IoU: 0.9341 May require manual adjustments for optimal results Zheng et al. 40 Threshold-gradient with TFDM and convex hull repair Enhances segmentation performance and robustness 2112 CT images IOU: 0.9911 Requires precise calibration of threshold settings Guo et al. 41 whale optimization algorithm-based random mutation strategy for image segmentation Speeds up convergence, improves segmentation 25,000 CT images Feature similarity index measure (FSIM): 81.57% May not adapt well to other cancer types Pandey and Bhandari 42 Morphological filtering and SVM classification Enhances tumor visibility for early intervention LIDC-IDRI Accuracy: 87.79% May miss micro-tumors or highly diffuse anomalies Vijayakumar et al. 43 CapsNet with RGF and SMFMF, U-net segmentation Enhances lung cancer detection efficiency 1097 CT images Accuracy: 98% May underperform with non-standardized data Qiao et al. 44 WSTSA with genetic algorithm Streamlines optimal CNN architecture selection 2773 CT images DSC: 72.40% Dependent on initial algorithm parameters Li et al. 45 Attention and Sobel edge detection Enhances lesion feature extraction and performance LIDC-IDRI Accuracy: 93.48% May require high computational resources Murugaraj et al. 46 HM-LeNet with NLM filter and K-Net Classifies lung abnormalities efficiently 23 CT scans images Accuracy: 92.7% Potential overfitting to specific abnormalities Fu et al. 47 LDN-SNP with SNP-type neurons Efficient, compact segmentation of COVID-19 CT 100 CT scans images Accuracy: 97.0% and DSC: 74.0% Limited adaptability to other imaging tasks Erciyes et al. 48 ANN with optimized U-Net and augmentation Reduced overfitting with enhanced segmentation American association of physicists in medicine (AAPM) thoracic auto-segmentation DSC: 94.68% and IoU: 0.8990 Limited adaptability to unseen pathologies Yang et al. 49 3D model with focal and dice loss Improved deep and shallow feature representation 199 CT scans images Accuracy: 99.90% and DSC: 56.10% Overfitting risk from a small COVID dataset Nguyen et al. 50 Two-stage no-new-U-Net with attention and boundary loss Improved GGO segmentation with fuzzy boundary handling Post-COVID CT challenge DSC: 71.93% Difficulty generalizing to non-GGO lung lesions Vincy et al. 51 3D ResNet50 encoder with dense-feature U-Net decoder Enhanced multiscale tumor features from lung CT slices LUNA16 IoU: 0.738 Contrast issues in small or diffuse tumors Jannat et al. 52 Lightweight residual U-Net with CBAM and ASPP Improved lung mask extraction Japanese society of radiological technology (JSRT) DSC: 98.72% Limited generalization on low-quality images Tumor detection In 2023, Vijh et al. 53 54 55 In 2024, Venkatesh et al. 56 57 58 59 60 61 62 63 64 In 2025, Shi and Zhang 65 66 67 68 69 70 71 72 73 74 Table 3 53 55 61 71 74 53 61 55 62 To address these challenges, our method uses a composite loss function within a specialized GAN architecture. This loss function combines classification loss, bounding box regression loss, and a new adversarial loss component. Unlike conventional loss functions, the adversarial component regularizes tumor structure by learning from differences between predicted and ground-truth tumor regions at the feature level. This reduces detection errors and improves diagnostic confidence.  Table 3 Comparison of advanced models for tumor detection. Authors Methodology Contribution Dataset Result Limitation Vijh et al. 53 Hybrid WOA and APSO with CNN Optimizes feature selection for tumor classification 120 CT images Accuracy: 97.18% May require extensive parameter tuning Alshayeji and Abed 54 XGBoost and DeepLabv3+ Integrates machine learning and advanced segmentation for comprehensive nodule analysis LIDC-IDRI Accuracy: 99.65% Requires detailed nodule characteristics Halder and Dey 55 Atrous pyramid and residual connections Applies atrous convolution for detailed multi-scale feature extraction LIDC-IDRI Accuracy: 95.97% and DSC: 97.15% May perform variably across different CT scanners Venkatesh et al. 56 Patch processing with deep learning Introduces patch-based preprocessing for enhanced classification LIDC-IDRI Accuracy: 95.70% Relies on high-quality CT image preprocessing Sundarrajan et al. 57 Cloud-IoT with OFCMNN and KM-DTCL Integrates cloud-IoT for data collection and segmentation-classification pipeline LIDC-IDRI Accuracy: 93.86% Dependence on cloud-IoT infrastructure Srinivasulu et al. 58 Autoencoder Integrates advanced neural network structures for nuanced feature extraction in lung tumor detection Open-source and online portals Accuracy: 96% May not generalize well to less curated, real-world clinical data Tang and Zhang 59 Shared backbone and distillation Leverages task correlations for enhanced performance LIDC-IDRI Accuracy: 91.9% Require careful tuning of task interdependencies Kongkham et al. 60 CNNs for lung nodule segmentation Introduces advanced CNN techniques to refine nodule segmentation LIDC-IDRI DSC: 85.3% Requires large annotated datasets for optimal training Gautam et al. 61 ResNet-152, DenseNet-169, EfficientNet-B7 ensemble Enhances nodule classification through a weighted ensemble approach LIDC-IDRI Accuracy: 97.23% Requires sophisticated model integration skills Sadremomtaz and Zadnorouzi 62 Enhanced U-Net with dilation residuals and attention mechanisms Enhances feature extraction and network efficiency LIDC-IDRI Accuracy: 99.92% Initial model setup and training can be computationally intensive Pan et al. 63 semi-supervised training strategy for entropy minimization lesion-level data augmentation Enhances training efficiency with novel data augmentation LIDC-IDRI DSC: 81.06% Requires careful balancing in parameter tuning Rathan and Lokesh 64 Active contour & HRNet for lung cancer Utilizes hybrid imaging techniques for nuanced disease staging LIDC-IDRI Accuracy: 98.4% Requires high-quality imaging for optimal performance Shi and Zhang 65 Advanced UNet with attention and adaptation Tailors segmentation sensitivity to nodule characteristics LIDC-IDRI DSC: 89.85% and IOU: 0.8960 Requires calibration for variable nodule sizes Miao et al. 66 DLN with ITF and IPN integration Integrates ITF features to enhance nodule characterization 1,385 CT images Accuracy: 82.3% Dependent on the quality of fat and nodule imaging Yu et al. 68 Two-step multimodal one-shot neural architecture search for PET/CT pulmonary nodules Introduces a structured two-step neural architecture search to optimize architecture search 499 CT images Accuracy: 94.23% Intensive computational resources required Jiang et al. 69 KAN and adaptive feature fusion Enhances nodule detection by optimizing feature representation Lung nodule analysis 2016 (LUNA16) Sensitivity: 94.73% Require extensive training data for optimization Xue et al. 70 SE-ViT with dual attention Integrates self-attention and SE mechanisms for detailed feature analysis LUNA16 Accuracy: 86.3% High dependency on the quality of training data Elhassan et al. 71 CNN + YOLOv8 with DCGAN augmentation Enhanced real-time tumor detection and classification Iraq‑oncology teaching hospital /national center for cancer diseases (IQ-OTHNCCD) IOU: 0.85 Sensitivity to imaging conditions and variations Abdulqader et al. 72 Transformer, anchor-free, FPN with multi-task learning Unified detection, classification, and localization of lung tumors 1608 CT images IoU: 0.9576 Complex training pipeline for clinical deployment Liu et al. 73 YOLOv11 with MobileNetV4, MSDA, frequency fusion bidirectional FPN, slide loss Improved detection via multi-scale attention and feature integration LUNA16 IoU: 0.946 High architectural complexity for deployment Li 74 CNDNet + FPRNet with GCSAM and HPFF Enhanced multi-scale detection and false positive reduction LUNA16 Sensitivity: 97.7% May misclassify nodules with unclear margins 3D tumor reconstruction Recent medical progress has spurred the advancement of 3D models, greatly improving patient treatment approaches 75 79 80 81 82 A significant focus in current medical research is on reconstructing 3D models of lung tumors 83 10 83 84 85 83 86 87 88 2 89 Table 4 10 86 2 89 To overcome these challenges, our model introduces a hybrid GAN and TLSTM architecture, where the TLSTM component adjusts its weight assignment based on the proximity of data points to the target rather than relying solely on global sequence trends. This localized adaptation enhances model sensitivity in the edge regions of tumors, improving reconstruction fidelity, particularly in complex or atypical cases. By incorporating adversarial learning alongside transductive temporal modeling, the proposed method provides both structural realism and data-specific precision, distinguishing it from prior models.  Table 4 Comparison of advanced models for the 3D reconstruction of lung tumors. Authors Methodology Contribution Dataset Result Limitation Hong et al. 10 VGG and GK clustering Introduces GAN-based 3D reconstruction for lung tumor imaging LUNA16 HD: 2.82 and ED: 2.82 Performance may vary for non-lung tumor datasets Gu et al. 83 ResNet and GK clustering Combines GAN and LSTM for feature-driven 3D tumor reconstruction LUNA16 HD: 2.99 and ED: 1.06 Requires well-segmented 2D input images Rezaei et al. 84 VGG and GK clustering Uses GAN for 3D reconstruction and transfers 2D image features LUNA16 HD: 3.02 and ED: 1.06 Relies heavily on accurate segmentation in preprocessing Karrar et al. 86 3D reconstruction using bounding boxes and rule-based classifier Develops an approach for nodule extraction and surface rendering for 3D modeling LIDC-IDRI Accuracy: 99.6627% Limited robustness to nodules with irregular shapes Dlamini et al. 87 YOLOv4 and region-based active contour model Integrates detection and volumetric rendering in a single automated pipeline LIDC-IDRI Accuracy: 99.74% and DSC: 92.19% Requires robust preprocessing for noise removal Shi et al. 88 ViT and demographics Enhances 3D reconstruction from 2D images using ViT 2525 chest x-rays DSC: 76.9% Dependent on the inclusion of accurate demographic data Najafi et al. 2 GAN, LSTM, and VGG16 Integrates GANs with VGG16 and LSTM for accurate 3D reconstruction LIDC-IDRI HD: 0.986 and ED: 1.126 Training instability due to sensitivity to input variations and multi-model tuning Huang et al. 89 GAN, attention-based LSTM, and VGG16 Integrates multi-stage GANs with VGG16 and attention-based LSTM for accurate 3D reconstruction LIDC-IDRI HD: 2.963 and ED: 1.725 Requires extensive labelled data and balanced segmentation to maintain accuracy across GAN stages Background This section briefly overviews TLSTM, spatial attention, RL, trust region policy optimization (TRPO), and PPO. TLSTM LSTM is designed to handle the shortcomings of traditional recurrent neural networks (RNNs), precisely their inability to learn long-range dependencies in sequence data 90 91 92 The LSTM model 93 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{i}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{f}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{o}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{c}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{h}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{t}$$\\end{document} 1 5 14  Fig. 1 Architecture of an LSTM unit. It displays the key components, including the input gate ( \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{i}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{f}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{o}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{c}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{h}_{t}$$\\end{document} 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{i}_{t}=\\sigma\\:({W}_{xi}{x}_{t}+\\:\\:{W}_{hi}{h}_{t-1}+{W}_{ci}{c}_{t-1}+\\:{b}_{i})$$\\end{document} 2 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{f}_{t}=\\sigma\\:({W}_{xf}{x}_{t}+\\:\\:{W}_{hf}{h}_{t-1}+{W}_{cf}{c}_{t-1}+\\:{b}_{f})$$\\end{document} 3 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{c}_{t}={f}_{t}{c}_{t-1}+{i}_{t}tanh({W}_{xc}{x}_{t}+\\:\\:{W}_{hc}{h}_{t-1}+\\:{b}_{c})$$\\end{document} 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{o}_{t}=\\sigma\\:({W}_{xo}{x}_{t}+\\:\\:{W}_{ho}{h}_{t-1}+{W}_{co}{c}_{t}+\\:{b}_{o})$$\\end{document} 5 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{h}_{t}={o}_{t}tanh\\left({c}_{t}\\right)$$\\end{document} Here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\sigma\\:(.)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:tanh$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{W}_{xk}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:k\\in\\:\\{i,\\:f,\\:o,\\:c\\}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{W}_{hk}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\text{k}\\in\\:\\{i,\\:f,\\:o,\\:c\\}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{h}_{t-1}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{W}_{ck}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:k\\in\\:\\{i,\\:f,\\:o\\}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{b}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{b}_{f}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{b}_{c}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{b}_{o}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{w}_{lstm}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{b}_{lstm}$$\\end{document} 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:g( \\cdot )$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:f( \\cdot )$$\\end{document} 1 5 14 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left\\{\\begin{array}{c}{c}_{t}=f({c}_{t-1},{h}_{t-1},{x}_{t};{w}_{lstm},{b}_{lstm})\\:\\\\\\:{h}_{t}=g({h}_{t-1},{c}_{t-1},{x}_{t};{w}_{lstm},{b}_{lstm})\\end{array}\\right.$$\\end{document} After the debut of the LSTM model, numerous advancements and variations, such as TLSTM, have been developed to enhance its performance across various studies. The state space of TLSTM is defined in Eq. 7 14 7 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left\\{\\begin{array}{c}{c}_{t,\\eta\\:}=f({c}_{t-1,\\eta\\:},{h}_{t-1,\\eta\\:},{x}_{t};{w}_{lstm,\\eta\\:},{b}_{lstm,\\eta\\:})\\:\\\\\\:{h}_{t,\\eta\\:}=g({h}_{t-1,\\eta\\:},{c}_{t-1,\\eta\\:},{x}_{t};{w}_{lstm,\\eta\\:},{b}_{lstm,\\eta\\:}\\end{array}\\right.$$\\end{document} In this context, η represents an unseen sequence. The formulation of Eq. 7 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\eta\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:z\\left(\\eta\\:\\right)$$\\end{document} Spatial attention Spatial attention allows TLSTM to selectively concentrate on particular input data segments simultaneously instead of uniformly processing the entire dataset. It highlights essential variables and attributes, protecting the model from irrelevant data or noise 94 Spatial attention is typically implemented as an intermediate layer within the TLSTM architecture, positioned between the input and the TLSTM layers. It aggregates the input data in a weighted manner, where the weights are assigned during training according to the significance of each input feature. The input to this spatial attention layer is an \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:N\\times\\:D$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:X$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:N\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:D$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:W$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:b$$\\end{document} 8 8 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:a=softmax\\:(XW+b)$$\\end{document} The subsequent step is the computation of the weighted input sum as described in Eq. 9 9 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{X}^{{\\prime\\:}}=Xa$$\\end{document} This weighted sum condenses the input while steering the attention of the model toward the most impactful features. Spatial attention boosts the adaptability of TLSTM by selectively emphasizing critical features. It minimizes the likelihood of overfitting by curbing excessive focus on non-essential noise or details, which could distort the learning outcomes. Moreover, this focused strategy aids in managing high-dimensional data by reducing the computational load associated with processing less critical information. Consequently, a TLSTM outfitted with spatial attention excels at processing varied and intricate datasets, yielding more uniform and widely applicable results. Such enhancements in model performance are especially vital in situations where detecting subtle data variations is key to making accurate predictions. RL RL represents a process where an agent enhances its decision-making abilities by undertaking actions that amplify the rewards obtained from its environment. This evolving procedure is configured utilizing Markov decision processes (MDPs). An MDP is characterized by elements \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:(S,\\:A,\\:P,\\:{\\rho\\:}_{0},\\:r)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:S$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:A$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:P:\\:S\\:\\times\\:\\:A\\:\\times\\:\\:S\\:\\to\\:\\:R$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\rho\\:}_{0}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:S\\:\\to\\:\\:R$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:r:\\:S\\:\\times\\:\\:A\\:\\to\\:\\:R$$\\end{document} In RL, at each timestep \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:t$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{s}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{a}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\pi\\::\\:S\\:\\times\\:\\:A\\:\\to\\:\\:\\left[\\text{0,1}\\right]$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:r({s}_{t},{a}_{t})$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{s}_{t+1}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:t$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{R}_{t}=\\sum\\:_{t=0}^{{\\infty\\:}}{\\gamma\\:}^{t}r({s}_{t},{a}_{t})$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\gamma\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{V}_{\\pi\\:}\\left({s}_{t}\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{Q}_{\\pi\\:}\\left({s}_{t},{a}_{t}\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{A}_{\\pi\\:}\\left({s}_{t},{a}_{t}\\right)$$\\end{document} 10 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{V}_{\\pi\\:}\\left({s}_{t}\\right)={E}_{{a}_{t},{s}_{t+1},\\dots\\:\\sim\\pi\\:\\:}\\left[\\sum\\:_{k=t}^{\\infty\\:}{\\gamma\\:}^{k-t}r({s}_{k},{a}_{k})\\right]$$\\end{document} 11 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{Q}_{\\pi\\:}\\left({s}_{t},{a}_{t}\\right)={E}_{{s}_{t+1},{a}_{t+1},\\dots\\:\\sim\\pi\\:\\:}\\left[\\sum\\:_{k=t}^{\\infty\\:}{\\gamma\\:}^{k-t}r({s}_{k},{a}_{k})\\right]$$\\end{document} 12 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{A}_{\\pi\\:}\\left({s}_{t},{a}_{t}\\right)={Q}_{\\pi\\:}\\left({s}_{t},{a}_{t}\\right)-{V}_{\\pi\\:}\\left({s}_{t}\\right)$$\\end{document} The goal is to formulate a policy, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\pi\\:$$\\end{document} 95 13 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\eta\\:\\left(\\pi\\:\\right)={E}_{{s}_{0},{a}_{0},\\dots\\:}\\left[{R}_{0}\\right]={E}_{{s}_{0},{a}_{0},\\dots\\:}\\left[\\sum\\:_{t=0}^{\\infty\\:}{\\gamma\\:}^{t}r({s}_{t},{a}_{t})\\right]\\:$$\\end{document} TRPO To enhance the performance objective outlined in Eq. 13 96 14 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\underset{\\pi\\:}{\\text{max}}{E}_{s\\sim{\\rho\\:}_{{\\pi\\:}_{{\\theta\\:}_{old}}},\\:\\:a\\:\\in\\:\\:{\\pi\\:}_{{\\pi\\:}_{old}}}\\left[\\frac{{\\pi\\:}_{\\theta\\:}\\left(a|s\\right)}{{\\pi\\:}_{{\\theta\\:}_{old}}\\left(a|s\\right)}{A}_{{\\pi\\:}_{old}}\\left(s,a\\right)\\right]\\:$$\\end{document} subject to 15 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{E}_{s\\sim{\\rho\\:}_{{\\pi\\:}_{old}}}\\left[{D}_{KL}\\left({\\pi\\:}_{old}\\right(.\\left|s\\right)\\left|\\right|\\:\\pi\\:(.|s\\left)\\right)\\right]\\le\\:\\delta\\:\\:$$\\end{document} Here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\pi\\:}_{old}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\delta\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{KL}\\left({\\pi\\:}_{old}\\right(.\\left|s\\right)\\left|\\right|\\:\\pi\\:(.|s\\left)\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\pi\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\pi\\:}_{old}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:s$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\rho\\:}_{{\\pi\\:}_{old}}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{s}_{0}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\pi\\:}_{old}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\rho\\:}_{{\\pi\\:}_{old}}\\left(s\\right)=\\sum\\:_{t=0}^{{\\infty\\:}}{\\gamma\\:}^{t}P({s}_{t}=s|{s}_{0},{\\pi\\:}_{old})$$\\end{document} PPO To reduce significant policy shifts, PPO utilizes an adapted optimization target termed the clipped surrogate objective, arranged as described below 73 16 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:L_{{PPO}}^{{CLIP}} = \\:E_{{s\\sim \\rho \\:_{{\\pi {\\kern 1pt} _{{old}} ,{\\kern 1pt} {\\kern 1pt} {\\kern 1pt} a\\sim \\pi {\\kern 1pt} _{{old}} }} }} \\left[ {{\\text{min}}\\left( {\\frac{{\\pi \\:\\:\\left( {a|s} \\right)}}{{\\pi \\:_{{old}} \\left( {a|s} \\right)}}A_{{\\pi \\:_{{old}} }} \\left( {s,a} \\right),clip\\left( {\\frac{{\\pi \\:\\left( {a|s} \\right)}}{{\\pi \\:_{{old}} \\left( {a|s} \\right)}},1 - \\in \\:,1 + \\in \\:} \\right)A_{{\\pi \\:_{{old}} }} \\left( {s,a} \\right)} \\right)} \\right]$$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\epsilon\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{A}_{{\\pi\\:}_{old}}\\left(s,a\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:a$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:s$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\pi\\:}_{old}$$\\end{document} 97 17 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:clip(x,\\:a,\\:b)\\:=\\:max(a,\\:min(b,\\:x\\left)\\right)$$\\end{document} In this context, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:x$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:a$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:b$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:x$$\\end{document} 16 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\frac{\\pi\\:\\left(a|s\\right)}{{\\pi\\:}_{old}\\left(a|s\\right)}$$\\end{document} Materials and methods Figure 2  Fig. 2 Comprehensive framework of the proposed model. The figure demonstrates the integrated process encompassing lung segmentation, tumor detection, and 3D tumor reconstruction. The lung segmentation phase employs GAN and U-Net architectures, utilizing off-policy PPO to address imbalanced classification. The tumor detection model incorporates a GAN alongside Mask R-CNN, enhanced by a novel loss function for more accurate predictions. Finally, the 3D reconstruction of tumors is facilitated through a spatial attention-based TLSTM network paired with a GAN, optimizing detail and fidelity in rendering tumor volumes. Following lung segmentation, the next phase is tumor detection. This stage involves examining the segmented lung images to identify potential tumor regions. Here, we integrate a GAN alongside the Mask R-CNN framework. This powerful tool combines the capabilities of object detection and instance segmentation, which effectively identifies and localizes the affected areas within the segmented images for precise tumor detection and categorization. Additionally, an error correction mechanism is integrated into the loss function of the generator to identify and rectify any discrepancies between the generated images and actual CT scans, thus enhancing the quality of the detected regions. The final stage of our proposed model is 3D tumor reconstruction. This process involves creating a 3D tumor model from the detected tumors. Our model for 3D tumor reconstruction employs a spatial attention-based TLSTM network, a type of recurrent neural network that can process data sequences while also considering the spatial relationships between the sequence elements paired with a GAN. The TLSTM network processes features extracted from 2D pulmonary scans by an EfficientNet network and uses transductive learning, adjusting its weights based on proximity to new data points, to significantly enhance the prediction of critical tumor-related features. The generator then creates a 3D model based on the outputs from the TLSTM network. In all GANs, the role of the discriminator is to evaluate the images generated by the generator. Discriminator networks use dilated convolution layers, a convolutional layer that can expand the receptive field without compromising image resolution or adding significant computational overhead. This is especially valuable in processing high-resolution medical images, where maintaining detail is crucial. Dilated convolutions enable the discriminator to capture a wider context and finer details across larger image areas, more accurately differentiating between real and generated images. Lung segmentation The primary goal of lung segmentation is to generate accurate lung contours that meticulously replicate the original reference outlines. In this paper, we employ a GAN for lung segmentation, which includes two networks: a generator and a discriminator. The generator excels at detecting essential features and patterns in the scans, creating outlines that accurately map the lung areas. These outlines are designed to capture the complex shapes and edges necessary for thorough segmentation. On the other hand, the discriminator works with the generator to enhance the quality of these outlines. Acting as a strict evaluator, the discriminator checks how well the generated outlines match the original ones. It uses the Earth Mover’s distance (EMD), a detailed measurement, to assess how much the generated outlines differ from the actual ones, helping to improve the outputs of the generator. This reciprocal evaluation process encourages the generator to create contours that better match the original, thus improving the segmentation quality. During training, the generator processes a lung CT scan image, denoted as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{I}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{M}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{M}_{i}$$\\end{document} Generator In our specific context, segmentation involves classifying each pixel in an image as belonging to the lung area. When processing a CT scan labeled \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{I}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{M}_{i}$$\\end{document} 3  Fig. 3 Architecture of U-Net-based generator for lung segmentation. This diagram illustrates the dual-component structure of the generator, consisting of an encoder and a decoder. The encoder processes the input CT scan image, extracting features at various scales through convolution layers to capture detailed information. The decoder then uses these features to construct precise masks delineating lung regions. In this model, the off-policy PPO algorithm is integrated, which enhances the ability of the model to classify each pixel accurately, addressing the challenge of class imbalance by prioritizing the detection of lung area pixels. The generator acts as a binary pixel classifier, labeling each pixel as part of the lung area (marked as one) or not (marked as zero). The prevalence of non-lung pixels (zeroes) leads to a class imbalance, causing the classifier to favor predictions of non-lung areas and reducing its performance in identifying lung regions. To rectify this, in addition to adversarial loss, we implement an off-policy PPO algorithm in our training strategy, effectively addressing the imbalance. This algorithm, equipped with custom reward and penalty systems, optimally addresses this issue by offering higher rewards or penalties for correct or incorrect identifications of lung pixels (the minority class) compared to non-lung pixels (the majority class). This adjustment helps the model to concentrate more accurately on recognizing lung area pixels. By boosting incentives for correctly identifying the minority class, the off-policy PPO algorithm directs the focus of the generator toward lung areas, thus correcting the bias towards the abundant non-lung pixels. This strategic enhancement balances the focus of the classifier between both classes and significantly improves the performance of the model in segmenting lung regions from CT images. In the proposed off-policy PPO algorithm, the framework for state, action, and reward is defined as follows:  State \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{s}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:t$$\\end{document} Action \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{a}_{t}$$\\end{document} Reward \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{r}_{t}$$\\end{document} 18 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{r}_{t}\\left({s}_{t},{a}_{t},{y}_{t}\\right)=\\left\\{\\begin{array}{c}+1\\:,{a}_{t}={y}_{t}\\:and\\:{a}_{t}\\in\\:{D}_{L}\\\\\\:-1\\:,{a}_{t}\\ne\\:{y}_{t}\\:and\\:{a}_{t}\\in\\:{D}_{L}\\\\\\:+\\lambda\\:\\:,{a}_{t}={y}_{t}\\:and\\:{a}_{t}\\in\\:{D}_{N}\\\\\\:-\\lambda\\:\\:,{a}_{t}\\ne\\:{y}_{t}\\:and\\:{a}_{t}\\in\\:{D}_{N}\\end{array}\\right.$$\\end{document} Where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{L}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{N}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\pm\\:\\lambda\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\lambda\\:$$\\end{document} To better illustrate how the reward mechanism addresses class imbalance, consider the flowchart in Fig. 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{s}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{a}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{L}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{N}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:+\\lambda\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:-\\lambda\\:$$\\end{document}  Fig. 4 Flowchart illustrating the reward assignment strategy in the off-policy PPO algorithm for lung segmentation. Algorithm 1 outlines the training process of the proposed generator for lung segmentation. This approach integrates both off-policy PPO and adversarial learning to address the severe class imbalance inherent in segmentation tasks. At each iteration, the generator is treated as a policy network that predicts whether each pixel belongs to the lung or non-lung class. A custom-designed reward function, detailed in Eq. 18  Algorithm 1 Pseudocode for training of the generator in lung segmentation. Off-policy PPO This research presents an advanced version of PPO known as off-policy PPO, which optimizes data utilization by integrating information from previous experiences and decisions. Diverging from the conventional on-policy PPO that depends solely on immediate environmental data for effective operation, off-policy PPO incorporates a broader, more diverse dataset. This dataset comprises both prior interactions and assorted tactical decisions. Consider, for instance, a robot navigating through a complex maze. In contrast, standard on-policy PPO leverages only current observations to refine its approach; off-policy PPO utilizes insights from prior navigational attempts and evaluates different strategies across varying scenarios. This method accelerates learning by minimizing the need for the robot to exhaustively explore each potential route, thereby improving efficiency by exploiting extensive historical data. Off-policy PPO addresses the optimization challenge by enhancing a surrogate objective function through the utilization of off-policy data, mirroring the technique used in off-policy TRPO 97 19 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\underset{\\pi\\:}{{max}}{E}_{s\\sim{\\rho\\:}_{\\mu\\:},\\:\\:a\\in\\:\\mu\\:}\\left[\\frac{\\pi\\:\\left(a|s\\right)}{\\mu\\:\\left(a|s\\right)}{A}_{{\\pi\\:}_{old}}\\left(s,a\\right)\\right]$$\\end{document} subject to: 20 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\stackrel{-}{D}}_{KL}^{{\\rho\\:}_{\\mu\\:},sqrt}(\\mu\\:,{\\pi\\:}_{old}){\\stackrel{-}{D}}_{KL}^{{\\rho\\:}_{\\mu\\:},sqrt}({\\pi\\:}_{old},\\pi\\:)+\\:{\\stackrel{-}{D}}_{KL}^{{\\rho\\:}_{\\mu\\:}}({\\pi\\:}_{old},\\pi\\:)\\le\\:\\delta\\:$$\\end{document} where 21 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\rho\\:}_{\\mu\\:}\\left(s\\right)=\\sum\\:_{t=0}^{\\infty\\:}{\\gamma\\:}^{t}P({s}_{t}=s|{s}_{0},\\mu\\:)$$\\end{document} 22 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\stackrel{-}{D}}_{KL}^{{\\rho\\:}_{\\mu\\:}}\\left({\\pi\\:}_{old},\\pi\\:\\right)={E}_{s\\sim{\\rho\\:}_{\\mu\\:}}\\left[{D}_{KL}\\left({\\pi\\:}_{old}\\left(.|s\\right)\\:\\left|\\right|\\:\\pi\\:\\left(.|s\\right)\\right)\\right]$$\\end{document} 23 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\stackrel{-}{D}}_{KL}^{{\\rho\\:}_{\\mu\\:},sqrt}\\left(\\mu\\:,{\\pi\\:}_{old}\\right)={E}_{s\\sim{\\rho\\:}_{\\mu\\:}}\\left[\\sqrt{{D}_{KL}\\left(\\mu\\:\\left(.|s\\right)\\:\\left|\\right|\\:{\\pi\\:}_{old}\\left(.|s\\right)\\right)}\\right]$$\\end{document} 24 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\stackrel{-}{D}}_{KL}^{{\\rho\\:}_{\\mu\\:},sqrt}\\left({\\pi\\:}_{old},\\pi\\:\\right)={E}_{s\\sim{\\rho\\:}_{\\mu\\:}}\\left[\\sqrt{{D}_{KL}\\left(\\:{\\pi\\:}_{old}\\left(.|s\\right)\\:\\left|\\right|\\:\\pi\\:\\left(.|s\\right)\\right)}\\right]$$\\end{document} Here, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\mu\\:$$\\end{document} 20 19 97 25 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{\\mu\\:}\\left(\\pi\\:\\right)={E}_{s\\sim{\\rho\\:}_{\\mu\\:},\\:\\:a\\in\\:\\mu\\:}\\left[\\frac{\\pi\\:\\left(a|s\\right)}{\\mu\\:\\left(a|s\\right)}{A}_{{\\pi\\:}_{old}}\\left(s,a\\right)\\right]$$\\end{document} Using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{\\mu\\:}\\left(\\pi\\:\\right)$$\\end{document} 97 26 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\overline{L} _{\\mu } \\left( \\pi \\right) = E_{{s\\sim \\rho \\:_{{\\mu {\\kern 1pt} }} ,\\:\\:a \\in \\:\\mu \\:}} \\left[ {{\\text{min}}\\left( {\\frac{{\\pi \\:\\left( {a|s} \\right)}}{{\\mu \\:\\left( {a|s} \\right)}}A_{{\\pi \\:_{{old}} }} \\left( {s,a} \\right),clip\\left( {\\frac{{\\pi \\:\\left( {a|s} \\right)}}{{\\mu \\:\\left( {a|s} \\right)}},1 - \\in \\:,1 + \\in \\:} \\right)A_{{\\pi \\:_{{old}} }} \\left( {s,a} \\right)} \\right)} \\right]$$\\end{document} The proportion \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\frac{\\pi\\:\\left(a|s\\right)}{\\mu\\:\\left(a|s\\right)}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:1\\:-\\:\\epsilon\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:1\\:+\\:\\epsilon\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\pi\\:\\left(a|s\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left(\\right(1\\:-\\:\\epsilon\\:),\\:(1\\:+\\:\\epsilon\\:\\left)\\right)$$\\end{document} 26 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\frac{{\\pi\\:}_{{\\theta\\:}_{i}}\\left(a|s\\right)}{\\mu\\:\\left(a|s\\right)}$$\\end{document} 97 27 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} L_{{Off{\\text{-}}Policy~PPO}}^{{CLIP}} \\left( \\pi \\right) & = E_{{s\\sim \\rho _{\\mu } ,~~a \\in \\mu }} \\left[ {\\min \\left[ {\\frac{{\\pi \\left( {a{\\text{|}}s} \\right)}}{{\\mu \\left( {a{\\text{|}}s} \\right)}}A_{{\\pi _{{old}} }} \\left( {s,a} \\right),} \\right.} \\right. \\\\ & \\quad \\left. {\\left. {clip\\left( {\\frac{{\\pi \\left( {a{\\text{|}}s} \\right)}}{{\\mu \\left( {a{\\text{|}}s} \\right)}},\\frac{{\\pi _{{old}} \\left( {a{\\text{|}}s} \\right)}}{{\\mu \\left( {a{\\text{|}}s} \\right)}}\\left( {1 - \\in } \\right),\\frac{{\\pi _{{old}} \\left( {a{\\text{|}}s} \\right)}}{{\\mu \\left( {a{\\text{|}}s} \\right)}}\\left( {1 + \\in } \\right)} \\right)A_{{\\pi _{{old}} }} \\left( {s,a} \\right)} \\right]} \\right] \\\\ \\end{aligned}$$\\end{document} Figure 5  Fig. 5 Policy optimization for lung segmentation using off-policy PPO in the U-Net framework. Discriminator The discriminator in the lung segmentation stage is designed to distinguish between real segmented masks and those generated by the GAN-based segmentation network. It receives binary segmentation masks as input. Then, it applies several dilated convolutional layers to extract multi-scale contextual features. These features are crucial for determining whether the segmentation is anatomically accurate. Instead of comparing pixels directly, the architecture allows the discriminator to evaluate whether the entire structure and shape are consistent with real anatomical forms. The output is a spatial probability map that indicates the realism of each region in the input mask. This map helps localize inconsistencies in shape or structure that the generator needs to correct. During training, the EMD is used as an additional evaluation metric. EMD measures the difference between the real and synthetic masks across spatial regions. This goes beyond simple pixel-level errors. It encourages the generator to learn the spatial arrangement and anatomical shape of the lungs more precisely. Together, these mechanisms refine the generator output toward more realistic, anatomically aligned segmentation masks. In duties such as image synthesis with GANs, the objective is usually to produce images that are indistinguishable from genuine ones. Nevertheless, lung segmentation offers distinct challenges. Authentic masks are binary, consisting only of 0s and 1s, but the masks produced by the generator vary continuously from 0 to 1. This difference may lead the discriminator to overly focus on identifying real versus produced masks, overlooking other vital factors. To address the challenge, we have adopted a sophisticated approach. Instead of directly feeding the fake or genuine masks within the discriminator, we incorporate lung slice images into the input. These images are modified versions of the CT scans, adjusted with segmented and real masks. This method enables the discriminator to assess the connection between the produced masks and the lung images, thereby enhancing its capacity to accurately differentiate between genuine and produced masks. Further, a specialized segmentation mask that selectively emphasizes nodule areas while setting other areas to zero is used. This extra detail helps the discriminator better recognize essential features, improving its differentiation capability. The loss function for the discriminator is defined in the following equation: 28 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{E}_{x\\sim\\:{p}_{z}}\\left[D\\right(G\\left(x\\right)\\:\\text{o}\\:{I}_{i}\\left)\\right]-{E}_{x}\\sim\\:{p}_{real}\\left[D\\right(x\\left)\\right]$$\\end{document} In this context, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{p}_{z}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{p}_{real}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:o$$\\end{document} Tumor detection Figure 6  Fig. 6 Architecture of the Mask R-CNN-based generator network for tumor detection. We employ a novel loss function, as described in Eq. 29 29 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{G}={L}_{cls}+\\:{L}_{box}+{L}_{adv}^{{G}_{b}}$$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{cls}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{box}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{adv}^{{G}_{b}}$$\\end{document} 30 30 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{adv}^{{G}_{b}}=\\frac{1}{N}\\sum\\:_{i=1}^{N}-{log}{D}_{b}\\left({G}_{b}\\left({RoI}_{i}\\right)\\right)\\:\\:$$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:N$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{G}_{b}\\left({RoI}_{i}\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:i$$\\end{document} The loss function of the discriminator, detailed in Eq. 31 31 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{D}=\\frac{1}{N}\\sum\\:_{i=1}^{N}-[{log}({D}_{b}\\left({bb}_{i}^{{g}_{t}}\\right))+\\:log(1-{D}_{b}\\left({G}_{b}\\left({RoI}_{i}\\right)\\right)\\left)\\right]$$\\end{document} where \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{b}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{adv}^{{G}_{b}}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{G}_{b}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{b}$$\\end{document} Figure 7 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{cls}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{box}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{adv}$$\\end{document}  Fig. 7 Generator training for tumor detection using Mask R-CNN with adversarial loss. The discriminator in the tumor detection stage is designed to evaluate the quality of bounding boxes and classifications produced by the generator. It takes as input the feature maps of predicted tumor regions, especially the RoIs) generated by the Mask R-CNN head. Instead of relying only on coordinate accuracy, the discriminator checks whether the predicted regions appear realistic based on semantic and anatomical information. It applies dilated convolutional layers to capture contextual features at different scales. This helps the discriminator assess the overall structure and coherence of the predicted tumor regions. The output is a scalar probability that reflects how closely a predicted region matches real tumors. During training, the EMD is employed as an additional loss function to quantify the differences between real and generated tumor distributions. In contrast to the segmentation stage, where the discriminator evaluates entire binary masks, this stage operates at the RoI level. It compares the semantic features of predicted bounding boxes with those of real tumor annotations. This setup enables the generator to improve both the accuracy of tumor localization and the realism of the predicted structures. The combined use of EMD and dilated convolutions ensures that feedback from the discriminator guides the generator toward more anatomically faithful tumor detection. 3D reconstruction method As shown in Fig. 3 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:N$$\\end{document} A spatial attention mechanism is placed before the TLSTM units to improve their ability to capture key temporal features. This mechanism enables the model to focus selectively on the most informative parts of each input image rather than treating all spatial regions equally. The attention layer is trained to prioritize features that are essential for 3D reconstruction. These often include tumor boundaries or prominent anatomical structures. By emphasizing these critical areas, the model reduces the influence of background noise and less significant regions, which often distract conventional sequence models. This focused representation enables TLSTM to interpret each frame within the context of both previous frames and spatial relevance, resulting in more coherent and anatomically accurate 3D reconstructions. Figure 8 The architecture of the generator features six deconvolution layers, each equipped with batch normalization (BN) and the ReLU activation function. In the GAN setup, the discriminator plays a crucial role. It accurately differentiates synthetic and real-world images from the generator, ensuring the 3D images are realistic. This effectiveness is vital for correct medical diagnosis and planning treatments. By combining the powerful feature extraction of EfficientNet with advanced sequence processing and generative modeling, this method successfully creates precise 3D representations from 2D image sequences, greatly improving the quality and realism of the generated images.  Fig. 8 Architecture of the generator network for the 3D reconstruction of lung tumors. The discriminator in the 3D reconstruction stage is designed to evaluate the realism of volumetric images generated by the GAN-based architecture. It receives as input the reconstructed 3D lung tumor volumes produced by the generator. The discriminator compares these generated volumes with the corresponding real, annotated 3D ground-truth data to evaluate the structural authenticity. To evaluate these 3D volumes, the discriminator employs 3D dilated convolutional layers. This structure enables it to analyze spatial dependencies in all three dimensions and detect inconsistencies in the generated structures. The output is a scalar prediction score that reflects the anatomical plausibility of the reconstructed volume. During training, EMD is employed as a global evaluation criterion, quantifying the distributional shift between real and synthetic 3D data across spatial regions. In contrast to earlier stages, where the discriminator evaluates 2D binary masks or region proposals, it operates on entire 3D reconstructions here. The discriminator acts not only as a classifier but also as a spatial coherence assessor, ensuring the generated volumes exhibit consistent shape, texture, and continuity over slices. This feedback loop encourages the generator to refine outputs that are both anatomically consistent and visually coherent in the volumetric space. Algorithm 2 shows the training procedure for the proposed TLSTM-based 3D tumor reconstruction model. The process begins with a training dataset composed of sequences of 2D tumor-detected slices, each annotated with a corresponding 3D ground-truth volume. Each slice from the input sequence is processed by a pre-trained EfficientNet encoder. This step extracts important high-level features from each image. A spatial attention mechanism is then applied to enhance the focus on anatomically relevant regions such as tumor boundaries. These focused features are sent to a TLSTM module. This module updates its hidden and cell states at each step to preserve temporal consistency across the slices.  Algorithm 2 Pseudocode for training of TLSTM-based 3D tumor reconstruction. The hidden state sequence contains both spatial and temporal information. This sequence is passed to a generator network that reconstructs the full 3D tumor volume. The training loss comprises two components: the mean absolute error (MAE) between the predicted and actual volumes and an adversarial loss. The adversarial loss is scaled using a hyperparameter. Gradients are backpropagated to update the parameters of the generator and the TLSTM units. This process gradually improves the quality of 3D reconstructions throughout training epochs. Overall algorithm Figure 9 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\{{I}_{i},\\:M\\}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:M$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{I}_{i}$$\\end{document}  Preprocessing stage: Initially, all CT images \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{I}_{i}$$\\end{document} Segmentation process: Following preprocessing, each image \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{I}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{S}_{i}$$\\end{document} Tumor detection: The segmented images \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{S}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{T}_{i}$$\\end{document} 3D reconstruction: After detecting the tumors, the relevant data \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{T}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:D$$\\end{document} Model evaluation: The final evaluation stage compares the generated 3D image \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:D$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:M$$\\end{document}  Fig. 9 Workflow for the 3D reconstruction of lung tumors. Empirical evaluation In the following section, we will outline the dataset, describing its features and the scope of our study. This explanation will be followed by a discussion on the metrics used, where we will define the criteria and measurements employed to evaluate the performance of our models. The section will conclude with the presentation of results, spotlighting the primary outcomes from our analysis and model evaluations. We will also discuss the importance of these findings in our research objectives, offering insights into their implications and relevance to the field. Dataset This article uses the LIDC-IDRI dataset 2 The LIDC-IDRI, meticulously developed by the Foundation for the National Institutes of Health (FNIH) and the Food and Drug Administration (FDA), comprises 1,018 CT scans from 1,010 registered patients, accompanied by an extensible markup language (XML) file detailing annotations by four radiologists. The primary aim of this rigorous annotation process, conducted in two stages, is to accurately identify and document all nodules within each CT scan. The initial stage, blinded-read, involves every radiologist reviewing the scans to identify lesions. These are categorized as either “nodule < 3 mm” or “non-nodule ≥ 3 mm”. In the unblinded-read stage, radiologists revisit their annotations with access to the anonymous markings of their colleagues, fostering a collaborative environment for cross-verification and adjustments if necessary. Within this dataset, 7,371 lesions have been identified as nodules by at least one radiologist. Of these, 2,669 nodules are labeled consistently as ‘nodule ≥ 3 mm’ by all four radiologists, with 928 receiving unanimous agreement. These 2,669 nodules are further detailed with ratings based on their characteristics, such as size, shape, and density, and outlined visually in the dataset, enhancing their utility for diagnostic and research purposes. It should be noted that the LIDC-IDRI dataset originally contained annotated 2D CT slices rather than native 3D volumes. To overcome this limitation, we developed a reproducible pipeline to generate consistent 3D volumes. This pipeline was implemented using Rhinoceros 3D (version 7) 98 To ensure consistency and improve model performance, several preprocessing steps were applied to the raw CT images before feeding them into the segmentation, detection, and reconstruction pipelines. First, slice alignment was applied to preserve spatial coherence across the image sequences. Slices were aligned along the same anatomical axis using DICOM metadata to prevent positional drift. Next, z-score normalization was used to standardize the intensity values in each image. This process ensured zero mean and unit variance, helping to reduce variability caused by different scanners or individual patient characteristics. To increase robustness and prevent overfitting, various data augmentation methods were applied during training. These included random rotations (± 15°), horizontal and vertical flipping, and elastic deformations to simulate realistic anatomical variations. All preprocessing steps were implemented in Python using the SimpleITK and Albumentations libraries. These operations were applied uniformly across the training and validation subsets, significantly contributing to the reproducibility and generalizability of the model across different data distributions. We separated the data for all models into training, validation, and testing subsets to streamline model training and evaluation. For lung segmentation and tumor detection using the LIDC-IDRI dataset, which includes 244,527 images, we distributed 70% of these images to the training subset, 15% to the validation subset, and 15% to the test subset. Specifically, this allocation provided approximately 171,169 images for training, 36,679 for validation, and 36,679 for testing. In the case of the 3D reconstruction model, this 70-15-15 distribution resulted in 5,785 images for training, 1,240 for validation, and 1,240 for testing. We chose this particular 70-15-15 split to ensure a balance between having ample data for training to enhance model learning and sufficient data for validation and testing to confirm thorough evaluation. By allocating 70% of the data to training, we ensure that the model learns from diverse examples, which helps it detect complex patterns and improve its generalization. The 15% of data set aside for validation is sufficient to adjust hyperparameters and prevent overfitting, enhancing the performance of the model on new data. Finally, the 15% allocated to testing offers an unbiased evaluation of the performance of the model, providing reliable insights into its effectiveness in real-world applications. Metrics In evaluating lung segmentation and tumor detection models, intersection over union (IoU) and HD are used because these metrics help gauge the quality of the model and the boundaries it predicts. IoU is particularly critical for lung segmentation and tumor detection as it provides a clear measure of overlap consistency between the predictions of the model and the ground truth data. By calculating the intersection ratio to the union of the predicted and actual segments, IoU directly indicates how well the model can identify relevant areas within the lungs and tumors, regardless of their size or shape. This makes it invaluable in clinical settings where even small inaccuracies in segmentation can lead to significant differences in diagnosis or treatment outcomes. On the other hand, HD is essential for assessing the maximum distance between the predicted and true boundaries. It is vital as it confirms the exactness of the method in outlining the outer edges of the important regions. HD is beneficial when high exactness is required in capturing the boundaries of lung tissues and tumors, as slight deviations in these boundaries can impact clinical decisions like surgical margins or radiation targeting. Together, IoU and HD provide a comprehensive evaluation framework. They not only measure how much of the target the model successfully captures (IoU) but also how accurately it traces the most extreme perimeters of the targets (HD). This dual assessment helps ensure that the models are robust and effective for practical application in medical diagnostics and treatment planning, where overall robustness is critical. In evaluating the 3D reconstruction model, HD and ED are chosen for their ability to effectively measure spatial fit and alignment errors. HD is particularly valuable as it measures the largest distance between the nearest points on the reconstructed surface and the actual model. It provides a worst-case scenario essential for high-performance applications like medical imaging and surgical planning. ED offers a simple geometric measure of the average distance between corresponding points on the actual and reconstructed models, helping assess the overall error throughout the volume. This combination ensures a robust evaluation, capturing both outlier errors and overall fidelity, which is essential for verifying the clinical reliability of the reconstructed 3D models. These metrics, together, provide a comprehensive view of the performance of the model in replicating intricate anatomical structures, which is vital for ensuring the realism required in medical applications. The definitions for the metrics IoU, HD, and ED are provided as such:  IoU is measured by the intersection ratio to the union of the pixel sets \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:A$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:B$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:A$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:B$$\\end{document} 32 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:IoU=\\frac{|A\\cap\\:B|}{|A\\cup\\:B|}$$\\end{document} HD quantifies the maximum distance from any point within one set to the nearest point in the alternative set. In this metric, \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left\\| {a - b} \\right\\|$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:a$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:b$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:A$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:B$$\\end{document} 33 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:HD\\:(A,\\:B)=\\text{max}\\:(h\\left(A,B\\right),\\:h\\:(B,\\:A\\left)\\right)$$\\end{document} 34 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:h\\left( {A,B} \\right) = \\:\\mathop {{\\text{max}}}\\limits_{{a\\: \\in \\:\\:A}} \\mathop {{\\text{min}}}\\limits_{{b\\: \\in \\:\\:B}} \\left\\| {a - b} \\right\\|$$\\end{document} ED measures the straight-line distance between corresponding points in 3D space, denoted as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:({x}_{1},{y}_{1},{z}_{1})$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:({x}_{2},{y}_{2},{z}_{2})$$\\end{document} 35 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:ED=\\sqrt{{({x}_{2}-{x}_{1})}^{2}+{({y}_{2}-{y}_{1})}^{2}+{({z}_{2}-{z}_{1})}^{2}}$$\\end{document}  Table 5 Optimal hyperparameter settings for the proposed method using cross-validation. Hyperparameter description Valid ranges Best value Long segmentation Rate of learning for generator 0.00001–0.01 0.002 Layers in generator 3–10 6 Rate of learning for discriminator 0.00001–0.01 0.001 Layers in discriminator 3–10 8 Size of batch 8–512 68 Momentum 0.6–0.9 0.85 Decay of weights 0. 001–0.01 0.001  \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\gamma\\:$$\\end{document} 0.1–1 0.68 Tumor detection Rate of learning for generator 0.00001–0.01 0.01 Layers in generator 3–10 8 Rate of learning for discriminator 0.00001–0.01 0.001 Layers in discriminator 3–10 7 Size of batch 8–512 82 Scales of region proposal network (RPN) anchor 8–1024 102 Ratios of RPN anchor 0.25–3 1.1 Ratio of ROI positive 0.1–0.6 0.35 Std for bounding box refinement 0.001–0.2 0.012 Threshold for masking 0.4–0.8 3D reconstruction Rate of learning for generator 0.00001–0.01 0.001 Layers in generator 3–10 5 Rate of learning for discriminator 0.00001–0.01 0.0015 Layers in discriminator 3–10 9 Size of batch 8–512 102 Units in TLSTM 64–1024 247 Layers in TLSTM 2–8 4 Rate of Dropout in TLSTM 0.1–0.8 0.3 Beta1 for adaptive moment estimation (ADAM) 0.1–0.9 0.6 Beta2 for Adam 0.1–0.9 0.65  \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\partial\\:$$\\end{document} 0.1–1 0.56 Main results To ensure the effectiveness of our model, we implemented stratified cross-validation in our hyperparameter optimization process. Stratified cross-validation guarantees that each subset of the data used in the validation process represents the overall dataset accurately by mirroring the proportion of each class found in the full dataset. This approach is vital in medical imaging, where the prevalence and characteristics of target features, such as tumors, can significantly differ. Using stratified cross-validation, we prevent our models from overfitting to less representative data and ensure that our performance assessments are consistent and reliable across varying types of tumor presentations. This reliability is crucial for clinical settings, where accurate and consistent model performance can directly impact diagnostic and treatment decisions. Table 5 During the evaluation of 3D reconstruction, the proposed model was subjected to a stringent comparison against seven established models, GAN-LSTM-3D 10 87 83 84 88 2 89  Proposed w/o TLSTM: replaces TLSTM with vanilla LSTM. Proposed w/o SP: excludes the spatial attention module. Proposed with transformer: replaces TLSTM with a transformer-based sequence encoder. Proposed w/o adversarial loss: removes the adversarial loss component from the generator training. Proposed w/o MAE: excludes the mean absolute error term between the predicted and actual volumes from the generator loss. Proposed w/o DC: excludes the DC module in the dicriminator. The comparative results in Table 6 The proposed model achieves an ED of 0.985 and HD of 0.648, outperforming all baselines. Relative to GAN-LSTM-RL, the model improves ED by 22.7% and HD by 43.9%, illustrating superior reconstruction fidelity and boundary adherence. Compared to ViT, the model achieves a 32.6% improvement in ED and 51.9% in HD. This result validates the added value of transductive LSTM with spatial attention, which captures local and contextual variations more effectively. The proposed model also outperforms YOLOv4 by 33.6% in ED and 48.4% in HD. This difference highlights the importance of combining transductive LSTM and adversarial learning, which are not present in YOLOv4. Against the weakest competitor, GAN-ResNet-3D, the model improves ED by 60.2% and HD by 66.1%. This substantial gain results from the integration of three innovations in the model: PPO-enhanced segmentation, adversarial tumor detection, and TLSTM-guided 3D reconstruction. Furthermore, compared to GAN-SP-LSTM (ED: 1.546, HD: 1.465), which also uses spatial attention, the model improves ED by 36.3% and HD by 55.7%, highlighting the crucial role of transductive learning and joint optimization across modules. This consistent superiority in both metrics across all comparisons confirms the robustness, generalizability, and precision of the proposed system. The ablation study underscores the critical role of each component in the performance of the proposed model. Removing the TLSTM module resulted in significant degradation. ED and HD increased to 1.962 and 1.154, which correspond to performance drops of 99.2% and 77.9%, respectively. This indicates the vital role of TLSTM in temporal modeling. Eliminating the spatial attention module raised ED to 1.256 and HD to 1.026, reflecting drops of 27.5% and 58.3%, showing that spatial attention enhances focus on important regions. When TLSTM was replaced with a transformer, ED and HD increased to 1.635 and 1.452. These changes represent losses of 66% and 124%, indicating that transformers are less effective than TLSTM in capturing localized patterns. Removing the adversarial loss increased ED to 1.426 and HD to 1.305. These values indicate degradations of 44.8% and 101.2%, highlighting the role of the adversarial component in generating realistic tumor structures. Omitting the MAE loss led to ED of 1.145 and HD of 0.984, reducing pixel-level consistency by 16.2% and 51.8%. Finally, removing the dilated convolution (DC) module in the discriminator caused ED and HD to rise to 1.025 and 0.912. These results confirm that all components, TLSTM, spatial attention, adversarial loss, MAE, and DC, are essential for achieving optimal reconstruction accuracy.  Table 6 Comparative analysis of various models on 3D reconstruction. Model HD ED GAN-LSTM-3D 10 1.415 ± 0.120 1.852 ± 0.174 YOLOv4 87 1.256 ± 0.142 1.485 ± 0.256 GAN-ResNet-3D 83 1.912 ± 0.103 2.478 ± 0.103 GAN-GK-LSTM 84 1.852 ± 0.262 1.952 ± 0.123 ViT 88 1.348 ± 0.018 1.461 ± 0.074 GAN-LSTM-RL 2 1.156 ± 0.104 1.274 ± 0.142 GAN-SP-LSTM 89 1.465 ± 0.136 1.546 ± 0.098 Proposed w/o TLSTM 1.154 ± 0.245 1.962 ± 0.195 Proposed w/o SP 1.026 ± 0.105 1.256 ± 0.116 Proposed with transformer 1.452 ± 0.213 1.635 ± 0.226 Proposed w/o adversarial loss 1.305 ± 0.045 1.426 ± 0.085 Proposed w/o MAE 0.984 ± 0.135 1.145 ± 0.162 Proposed w/o DC 0.912 ± 0.058 1.025 ± 0.105 Proposed 0.648 ± 0.024 0.985 ± 0.087 We conducted paired t-tests with a two-tailed distribution and a 95% confidence level on results to determine if our model significantly outperforms existing ones. We calculated p-values for each comparison between our model and existing models using the HD and ED metrics. The analysis showed that the improvements of our model in HD and ED are statistically significant compared to all other models. For instance, when comparing the performance of the proposed model on HD (0.648) with that of GAN-LSTM-3D (1.415), the exact p-value was 0.007, indicating a highly significant improvement. Similar results were obtained against YOLOv4 and GAN-ResNet-3D, with p-values of 0.02 and 0.005, respectively, underscoring the substantial enhancements in tumor detection and 3D reconstruction quality. Moreover, the ED metric also showed significant improvements. The proposed model achieved an ED of 0.985, significantly better than the 1.461 of ViT, with a p-value of 0.015. This trend of statistical significance was consistent across comparisons with other models like GAN-GK-LSTM and the version of our model without TLSTM, where p-values were 0.04 and 0.03, respectively, confirming the superior capability of the proposed model in producing precise 3D reconstructions of lung tumors. In addition, the comparison with GAN-LSTM-RL yielded p-values of 0.011 for HD and 0.008 for ED, confirming the statistically significant advantage of our model. Against GAN-SP-LSTM, the p-values were 0.004 for HD and 0.006 for ED, further supporting the robustness of our framework. Statistical significance was consistently observed across all other comparisons as well, reinforcing the robustness of the performance of the model. These statistical tests and confidence intervals confirm that the performance improvements of our model are statistically significant across various datasets, highlighting its potential for clinical use in medical imaging. Table 7  Table 7 Comparative analysis of runtime and GPU usage across various 3D reconstruction models. Model Runtime (s) GPU usage (GB) GAN-LSTM-3D 10 2671 10.9 YOLOv4 87 2485 13.7 GAN-ResNet-3D 83 3964 14.4 GAN-GK-LSTM 84 2392 16.2 ViT 88 2686 18.9 GAN-LSTM-RL 2 2860 17.2 GAN-SP-LSTM 89 2745 16.3 Proposed 2382 10.1 Our next experiment evaluated several pre-trained models, including GoogleNet, ResNet, AlexNet, DenseNet, MobileNet, and VGG-16, as alternatives to the EfficientNet network for extracting features. The results, as presented in Table 8  Table 8 Comparative analysis of different pre-trained models on the 3D reconstruction model. Model HD ED GoogleNet 1.420 ± 0.152 2.145 ± 0.124 ResNet 1.256 ± 0.136 1.820 ± 0.185 AlexNet 1.053 ± 0.128 1.695 ± 0.163 DenseNet 0.896 ± 0.140 1.741 ± 0.145 MobileNet 0.841 ± 0.174 1.652 ± 0.120 VGG-16 0.685 ± 0.024 1.112 ± 0.085 EfficientNet 0.648 ± 0.024 0.985 ± 0.087 Figure 10  Fig. 10 Convergence of training and validation loss over 300 epochs in the 3D reconstruction model. Table 9  Table 9 Visual comparison of original and reconstructed 3D tumor shapes in various models.  Table 10 Comparative analysis of various deep learning models on 3D reconstruction under adversarial conditions using FGSM. Model HD ED GAN-LSTM-3D 10 2.585 ± 0.126 2.965 ± 0.056 YOLOv4 87 2.756 ± 0.220 2.856 ± 0.174 GAN-ResNet-3D 83 2.826 ± 0.236 3.441 ± 0.120 GAN-GK-LSTM 84 3.412 ± 0.142 3.268 ± 0.110 ViT 88 2.048 ± 0.102 2.542 ± 0.142 GAN-LSTM-RL 2 1.932 ± 0.154 2.206 ± 0.126 GAN-SP-LSTM 89 1.862 ± 0.025 1.936 ± 0.068 Proposed 0.933 ± 0.068 1.641 ± 0.053 To test the robustness and generalization of our proposed 3D model against adversarial attacks, we used the fast gradient sign method (FGSM). This method effectively simulates real-world scenarios where slight and intentionally malicious modifications manipulate machine learning models. Adversarial attacks such as those generated by FGSM test the ability of the model to maintain robustness despite perturbations, posing a stringent challenge for any practical system. Table 10 Analysis of lung segmentation Our study evaluated the performance of our GAN-based lung segmentation model by conducting a comprehensive comparison with eighteen state-of-the-art models: LDANet 30 31 32 34 35 36 37 38 29 39 43 28 44 45 48 49 50 51  Proposed w/o off-policy PPO: excludes the off-policy PPO algorithm for imbalanced classification. No RL strategy is applied to address class imbalance in this setting. Proposed w/o adversarial loss: removes the adversarial loss component from the generator training. Proposed with RL: uses a standard RL algorithm for imbalanced classification instead of off-policy PPO. Proposed w/o proposed discriminator loss: replaces the proposed loss function with a standard discriminator loss, removing the enhancements described in Sects. 4-1-2. Proposed w/o DC: excludes the DC module in the dicriminator. The comparative results using the LIDC-IDRI dataset are shown in Table 11 The proposed model significantly outperforms all eighteen baselines in both HD and IoU. It achieves an HD of 1.152. This is 33.5% better than the closest competitor, which records an HD of 1.732. Its IoU reaches 0.881, which is 8.5% higher than the best-performing baseline of 0.812. This improvement reflects superior boundary precision and spatial overlap. Compared to LDANet, the proposed model improves HD by 61.2% and IoU by 27.3%. Against ViT, improvements are 60.6% in HD and 24.8% in IoU. When compared with mid-tier models like DeepLabV3 + and GAN-SE, the method reduces HD by 60.1% and 58.6% and increases IoU by 22.4% and 19.2%, respectively. Compared to U-Net and U-Net++, HD is reduced by 57.9% and 56.3%, while IoU increases by 18.1% and 15.9%. Even against stronger models like AE-UNet and 2D3D-Attn-Boundary, the proposed model shows HD reductions of 38.1% and 34.3% and IoU gains of 11.2% and 9.9%. These improvements, which include at least 20 unique rates across HD and IoU, highlight the impact of three innovations. These are off-policy PPO for handling class imbalance, adversarial generator training, and a discriminator enhanced with dilated convolution. No previous model integrates these components in a unified pipeline, giving the proposed method a consistent and generalizable performance edge. Ablation studies further validate the contribution of each module in the proposed model. Removing the off-policy PPO module (Proposed w/o off-policy PPO) results in an HD of 1.746 and IoU of 0.834—degradations of 51.5% in HD and 5.3% in IoU. Replacing off-policy PPO with a standard RL approach (Proposed with RL) yields HD of 1.614 and IoU of 0.843. These values are 40.1% and 4.3% worse than the full model, respectively. Omitting adversarial loss (Proposed w/o adversarial loss) degrades HD by 48.4% and IoU by 8.1%. Removing the custom discriminator loss function (Proposed w/o proposed discriminator loss) results in HD of 1.523 and IoU of 0.856. These represent degradations of 32.2% in HD and 2.8% in IoU compared to the full model. Eliminating the DC module in the discriminator (Proposed w/o DC) leads to an HD of 1.405 and IoU of 0.862. This change results in performance declines of 22% in HD and 2.2% in IoU. These results confirm that all five tested components make significant contributions. Each ablation shows at least four distinct performance drops. The largest losses occur when PPO or adversarial learning is removed, reaffirming their centrality in enhancing segmentation fidelity. Thus, the proposed model is not only superior to external baselines but is internally cohesive—each element is essential for achieving peak performance.  Table 11 Comparative analysis of various deep learning models on lung segmentation. Model IoU HD LDANet 30 0.692 ± 0.017 2.972 ± 0.126 VIT 31 0.706 ± 0.010 2.923 ± 0.103 DeepLabV3+ 32 0.719 ± 0.056 2.891 ± 0.175 Deeplabv3plus 34 0.729 ± 0.016 2.856 ± 0.176 SSSOA 35 0.731 ± 0.014 2.830 ± 0.135 EfficientNet B3 36 0.735 ± 0.058 2.821 ± 0.064 T-Net 37 0.736 ± 0.012 2.801 ± 0.196 GAN-SE 38 0.739 ± 0.013 2.782 ± 0.130 U-Net 29 0.746 ± 0.015 2.741 ± 0.246 BISE 39 0.752 ± 0.015 2.732 ± 0.196 CapsNet 43 0.758 ± 0.016 2.721 ± 0.196 U-Net++ 28 0.760 ± 0.010 2.635 ± 0.371 WSTSA 44 0.769 ± 0.010 2.362 ± 0.096 AE-UNet 45 0.792 ± 0.011 1.862 ± 0.169 U-Net-ANN-AUG 48 0.798 ± 0.105 1.842 ± 0.126 3D-Lightweight-AttnROI 49 0.762 ± 0.014 1.956 ± 0.015 2D3D-Attn-Boundary 50 0.802 ± 0.011 1.754 ± 0.082 3D-ResNet-UDecoder 51 0.812 ± 0.013 1.732 ± 0.114 Proposed w/o off-policy PPO 0.834 ± 0.022 1.746 ± 0.021 Proposed w/o adversarial loss 0.810 ± 0.017 1.710 ± 0.152 Proposed with RL 0.843 ± 0.195 1.614 ± 0.352 Proposed w/o proposed discriminator loss 0.856 ± 0.012 1.523 ± 0.256 Proposed w/o DC 0.862 ± 0.055 1.405 ± 0.135 Proposed 0.881 ± 0.016 1.152 ± 0.132 Statistical analysis shows that the proposed lung segmentation model is significantly superior to existing models, as evidenced by compelling p-values that provide a strong statistical basis for these claims. We conducted paired t-tests with a two-tailed distribution and a 95% confidence level to ensure robustness. For example, comparing the proposed model HD against 3D-ResNet-UDecoder yielded a p-value of 0.004, indicating a significant improvement. This trend of statistical significance is consistent across all metrics and comparisons. In the IoU comparisons, the superiority of the proposed model over the closest competitor, 3D-ResNet-UDecoder, is also statistically significant, with a p-value of 0.006. Each comparison between the proposed model and other state-of-the-art models reveals similarly low p-values, reinforcing the superior performance of the proposed system in accurately segmenting lung tissues. These consistently low p-values strongly support rejecting the null hypothesis in multiple comparisons, thus confirming the advanced capability of the proposed model in lung segmentation tasks. This statistical rigor ensures the reliability of the performance advantages of the model in a clinical setting. Figure 11  Fig. 11 Convergence of training and validation loss over 300 epochs in the lung segmentation model. Figure 12  Fig. 12 Performance comparison of off-policy PPO and on-policy algorithms in the lung segmentation model over 300 epochs. Figure 13  Fig. 13 Three examples of lungs segmented by the proposed model. Analysis of tumor detection The performance of the proposed tumor detection model is scrutinized through a comparative analysis with five other models: WOA-APSO 53 54 55 56 57 58 59 60 61 65 66 68 69 70 71 72 73 74  Proposed w/o adversarial loss: removes the adversarial loss component from the generator training. Proposed w/o proposed generator loss: replaces the proposed loss function with a standard generator loss, removing the enhancements described in Sects. 4-1-2. Proposed w/o DC: excludes the DC module in the dicriminator. The results are presented in Table 12 The proposed model significantly outperforms all baseline models. It achieves an HD of 0.721, which is 5.6% better than GCSAM-CNDNet (HD: 0.764), and an IoU of 0.893, showing a 7.8% improvement over the highest baseline IoU (0.828). When compared to earlier or moderately performing models like WOA-APSO, DeepLabv3+, ACIF, DLPP, OFCMNN, and ECNN-ERNN, the proposed model achieves HD improvements of 12.5–18.1% and IoU gains of 17–27.2%. For stronger baselines like MTNS, DLNS, MAST-UNet, DLN, ETMO-NAS, KansNet, SE-ViT, and MSDA-BiFPN, the proposed model shows HD improvements between 6.4% and 12.1% and IoU increases between 6.7% and 16.3%. Even when compared to recent hybrid approaches like YOLOv8-DCGAN and Transformer-FPN, the proposed method shows consistent improvements of 7.7–9.6% in HD and 10.1–11.0% in IoU. These performance gains stem from three core innovations: adversarial loss optimization, a custom generator loss, and a DC module in the discriminator—together enabling sharper boundary detection and greater robustness under noisy conditions. To assess internal component contribution, three ablation settings were tested. Removing the adversarial loss increases HD from 0.721 to 0.766 (a 6.2% decline) and reduces IoU from 0.893 to 0.826 (a 7.5% drop). Replacing the proposed generator loss with a standard one leads to HD of 0.796 and IoU of 0.763, marking degradations of 10.4% and 14.5%, respectively. Excluding the DC module results in HD of 0.732 and IoU of 0.854, still competitive, but 1.5% worse in HD and 4.4% worse in IoU than the full model. Compared to Proposed w/o generator loss, the full model improves HD by 9.4% and IoU by 17%. Against Proposed w/o adversarial loss, HD improves by 5.9% and IoU by 8.1%. Over Proposed w/o DC, HD improves by 1.5% and IoU by 4.6%. Each ablation version shows at least four unique drops across metrics, emphasizing the critical contribution of each module. These findings validate the unique role of each module. The adversarial component improves structure learning. The custom generator loss enhances semantic alignment. The DC module strengthens multi-scale discrimination. Combined, they form a cohesive and essential framework for accurate tumor detection.  Table 12 Comparative analysis of various deep learning models on tumor detection. Model IoU HD WOA-APSO 53 0.702 ± 0.013 0.880 ± 0.021 DeepLabv3+ 54 0.712 ± 0.010 0.871 ± 0.012 ACIF 55 0.740 ± 0.062 0.862 ± 0.017 DLPP 56 0.752 ± 0.016 0.852 ± 0.010 OFCMNN 57 0.760 ± 0.010 0.836 ± 0.012 ECNN-ERNN 53 0.763 ± 0.014 0.824 ± 0.020 MTNS 59 0.768 ± 0.034 0.820 ± 0.068 DLNS 60 0.776 ± 0.021 0.812 ± 0.062 EDL 61 0.780 ± 0.013 0.802 ± 0.010 MAST-UNet 65 0.782 ± 0.012 0.799 ± 0.013 DLN 66 0.796 ± 0.017 0.792 ± 0.020 ETMO-NAS 68 0.802 ± 0.014 0.789 ± 0.013 KansNet 69 0.816 ± 0.019 0.781 ± 0.020 SE-ViT 70 0.826 ± 0.012 0.774 ± 0.012 YOLOv8-DCGAN 71 0.783 ± 0.025 0.798 ± 0.042 Transformer-FPN 72 0.792 ± 0.045 0.790 ± 0.026 MSDA-BiFPN 73 0.802 ± 0.026 0.771 ± 0.015 GCSAM-CNDNet 74 0.828 ± 0.042 0.764 ± 0.065 Proposed w/o proposed generator loss 0.763 ± 0.019 0.796 ± 0.024 Proposed w/o adversarial loss 0.826 ± 0.012 0.766 ± 0.045 Proposed w/o DC 0.854 ± 0.052 0.732 ± 0.017 Proposed 0.893 ± 0.0142 0.721 ± 0.041 We evaluated the performance of the proposed tumor detection model using paired t-tests with a two-tailed distribution and a 95% confidence level. The results confirmed that the proposed model consistently outperforms existing methods with strong statistical support. For example, the p-values for the differences in HD and IoU between the proposed model and the strongest baseline (GCSAM-CNDNet) were 0.008 and 0.004, respectively, indicating highly significant improvements. Comparisons with competitive models such as SE-ViT and ETMO-NAS yielded exact p-values of 0.011 and 0.013 for both metrics, which confirms the consistent performance advantage of the proposed model. Even when compared to earlier models like DeepLabv3 + and WOA-APSO, the p-values were 0.0007 and 0.0005, highlighting statistically significant improvements. All remaining comparisons also yielded statistically significant results, which confirms the consistent advantage and robustness of the model across different metrics and baseline methods. These results confirm that the superior performance of the proposed model is not due to random variation but stems from its innovative components and robust design. The inclusion of these statistical tests strengthens the scientific credibility of the findings and demonstrates the reliability of the model for clinical deployment. Figure 14  Fig. 14 Convergence of training and validation loss over 300 epochs in the tumor detection model. Figure 15  Fig. 15 Three examples of tumors detected by the proposed model. Discussion This article presents a novel methodology for detecting lung cancer, utilizing GANs, TLSTM networks, and off-policy algorithms to address critical issues such as imbalanced class. We conducted thorough experiments that demonstrated the supremacy of the method over current sophisticated methods and performed ablation investigations to identify the unique contributions of per element. Table 13 The analyses clearly show that the proposed model performs better than existing models across several performance metrics. They also explain the underlying reasons for this superiority. Many previous models, including 2D3D-Attn-Boundary 50 51 73 74 88  Table 13 Comparative analysis of the proposed lung segmentation, tumor detection, and 3D reconstruction models against established models. Model Comparison Improvement (%) compared to the best model Main reason for superiority Lung segmentation Our introduced approach achieved an accuracy of 92.46%, IoU of 0.881, and an HD of 1.152, markedly surpassing current models, including U-Net-ANN-AUG 48 49 50 51 Accuracy:12.26%, IoU: 8.5%, HD: 33.5% Employing off-policy PPO strengthens the ability of the model to handle classifier imbalance efficiently, enhancing clarity in delineating lung tissues Tumor detection Relative to competing frameworks such as MSDA-BiFPN 73 74 Accuracy: 9.18%, IoU: 7.85%, HD: 5.63% Improved quality in detecting tumors stems from implementing a new loss function in the GAN framework, which refines the demarcation of tumor edges, outperforming conventional deep learning techniques 3D reconstruction The model outperforms others in the 3D reconstruction, achieving the highest accuracy and smallest HD and ED (accuracy = 90.84%, HD = 0.648, ED = 0.985) compared to techniques such as ViT 88 Accuracy:17.26%, ED: 22.71%, HD: 43.92% The quality of 3D reconstructions has greatly improved through the use of advanced methods for lung segmentation and tumor identification, augmented by the refined temporal analysis features of TLSTM networks Figure 13 15  Table 14 Analysis of error propagation from lung segmentation to tumor detection. Metric Input Ground truth lung segmentation Proposed lung segmentation Difference IOU 0.893 ± 0.0142 0.784 ± 0.0125 0.109 HD 0.721 ± 0.041 0.748 ± 0.026 0.027  Table 15 Analysis of error propagation from tumor detection to 3D tumor reconstruction. Metric Input Ground truth tumor detection Proposed tumor detection Difference HD 0.648 ± 0.024 0.726 ± 0.031 0.078 ED 0.985 ± 0.087 1.126 ± 0.054 0.141 This paper uses the input of original CT images obtained from the LIDC-IDRI dataset as targets for the lung segmentation and tumor detection steps. To evaluate how errors cascade across the detection and reconstruction stages, we use images generated by the proposed lung segmentation and tumor detection steps as inputs for tumor detection and 3D reconstruction, respectively, and perform a detailed error propagation analysis. Specifically, we evaluate how inaccuracies in lung segmentation impact the performance of tumor detection. We then examine how detection errors further affect the accuracy of 3D tumor reconstruction. As shown in Table 14 15 To mitigate error propagation further, we suggest the following strategies:  Iterative refinement mechanisms: This strategy sends feedback from the detection output back to the segmentation module. For example, if the detector finds a suspicious area that is not well captured by the segmentation mask, that region can be refined. Letting the stages update each other over multiple passes helps correct earlier mistakes. This feedback loop improves consistency and lowers the risk of early errors affecting final reconstruction. Joint optimization frameworks: Multi-task learning trains segmentation, detection, and reconstruction together in an end-to-end fashion. Instead of optimizing them separately, this method ensures that the features work well across all tasks. Updating parameters based on combined losses from all stages helps the model resist error transfer from one stage to another. This leads to more accurate and coordinated outputs. Uncertainty modeling: Estimating uncertainty at each stage helps the model understand how confident it is in its predictions. Regions with low confidence, like edges or noisy areas, can be marked for review or ignored in later steps. For example, the reconstruction step may downplay tumor detections that seem unreliable. This filtering blocks unreliable inputs from affecting later outputs, improving the pipeline’s robustness. Attention-based fusion mechanisms: Attention modules help the model focus on the most trustworthy regions in input masks during transitions between stages. For instance, the detection step can give more weight to well-segmented areas. During reconstruction, attention maps can highlight tumors confirmed in earlier steps. This focus on reliable regions helps prevent noisy or inaccurate data from being amplified in later predictions. The conceptual contributions of the study are profound, advancing the domain of medical computer vision, particularly in the use of machine learning for lung cancer detection. By incorporating advanced algorithms like GANs, off-policy PPO, and TLSTM, our model overcomes traditional image segmentation and tumor detection challenges, establishing a new benchmark for diagnostic performance. This research enhances our knowledge of how various machine learning techniques can work together effectively to address specific challenges in medical imaging, including balancing skewed data distributions and improving the relevance of features. The success of our model may encourage further research into combining these technologies, potentially expanding their use in other medical diagnostic and treatment planning areas. Our findings also suggest a new avenue for developing machine learning frameworks capable of handling the complexities of real-world medical data, opening up possibilities for creating more personalized and precise medical treatment strategies. The limitations of the proposed model are as follows:  Dependency on high-quality and well-annotated data: A significant limitation of our model is its dependency on high-quality, well-annotated imaging data. Access to such diagnostic images and annotations can be inconsistent in medical imaging, especially in underdeveloped regions or institutions with limited resources. This inconsistency could lead to discrepancies in the performance of the model, potentially affecting its reliability when used in various clinical environments. The reliance on detailed annotations for training introduces challenges, as any inaccuracies or omissions in annotations can misguide the learning process and result in erroneous interpretations or overlooked detections. Developing semi-supervised learning algorithms that require fewer labeled examples could mitigate this issue. Additionally, collaborating with multiple institutions to diversify training datasets would help the model generalize better across different settings and equipment variations. Variation in imaging equipment and acquisition parameters: A key limitation lies in the model’s untested generalizability across different imaging devices and acquisition protocols. Differences in scanner vendors, reconstruction kernels, radiation doses, and slice thicknesses can significantly alter image characteristics. As the current model was primarily trained on data from a single or limited range of imaging protocols, its performance may degrade when applied to datasets acquired under different technical conditions. Future work should include training and validation using multi-center data from diverse acquisition settings to enhance robustness and adaptability. Computational resource intensity: The complex architecture of our model, which includes multiple advanced algorithms such as GANs, off-policy PPO, and deep learning networks, is computationally intensive. This demands significant computational resources, which might not always be available in all real-time clinical settings, particularly in resource-limited environments. High computational demands may slow processing times, making it impractical in clinical settings where rapid diagnostics are crucial. To overcome this limitation, optimizing the model to reduce its computational load without significantly sacrificing performance could be beneficial. Techniques such as model pruning, quantization, and employing more efficient network architectures could be considered. Additionally, leveraging cloud computing resources could provide a viable solution by offloading heavy computational tasks to the cloud, thus reducing the need for extensive local computing resources. Requirement for extensive training data: Training a deep learning model from scratch requires a vast amount of data to achieve satisfactory performance, posing a significant barrier, especially when addressing rare conditions or settings where data collection is challenging. This requirement can also prolong training time and increase computational resource needs. Transfer learning techniques can address this limitation, where a model developed for one task is repurposed for another. This method enables the model to utilize pre-trained networks that have already captured essential features from extensive and varied datasets, thereby lessening the data needed for training. Additionally, generating synthetic data through techniques like GANs can augment real datasets with synthetic examples, providing more comprehensive training material and enhancing the robustness of the model without requiring vast real datasets. Model complexity and interpretability: The high complexity of the model presents challenges in interpretability, which is crucial in medical applications where practitioners need to understand how the model makes diagnostic decisions. Complex models like GANs and deep reinforcement learning algorithms can obscure the decision-making process, potentially causing trust issues among clinicians. Enhancing the interpretability of the model could involve integrating techniques such as feature visualization, allowing users to see what the model focuses on when making decisions. Additionally, simplifying the model without significantly compromising performance and creating interfaces that clearly explain the decision-making process to clinicians can enhance the transparency and trustworthiness of these models. Generalization across different demographics: The capacity of the method to generalize across various patient demographics and geographic locations is limited. Lung tissue characteristics, which can vary significantly due to ethnicity, age, and environmental influences, may affect the applicability of the model. To improve the generalization capabilities of the model, training it across a more diverse set of imaging data representing different demographics would be beneficial. Incorporating adaptive algorithms that adjust their parameters based on the demographic data of patients could enhance the effectiveness of the model in diverse clinical settings. This approach would help ensure the model remains robust and reliable regardless of deployment location, ultimately leading to better patient outcomes globally. Conclusion This study presents a significant advancement in lung cancer diagnostics by developing a novel computational framework capable of generating 3D representations of lung tumors from CT scans. By incorporating a custom GAN enhanced with off-policy PPO, our method successfully tackles the challenge of accurately segmenting lung tissue by overcoming the bias caused by a high proportion of non-lung pixels. This method increases the effectiveness of segmentation and enhances the detection of minority samples using a novel reward system. Furthermore, incorporating a specialized GAN with a new loss function significantly advances tumor detection within these accurately segmented areas. After segmentation and detection, the EfficientNet model extracts critical features, refined by a spatial attention-based TLSTM network. This improvement is vital for enhancing the performance of 3D reconstructions by concentrating on adjacent samples within a transductive learning framework, thus achieving exceptional precision. Empirical validation of our model on the LIDC-IDRI dataset has demonstrated superior performance, achieving HD and ED metrics of 0.648 and 0.985, respectively. These findings underscore the potential of our framework to serve as a valuable clinical tool, significantly enhancing the capabilities of radiologists in the diagnosis and treatment planning of lung cancer, ultimately aiming to improve patient outcomes. This study not only contributes to the technical fields of image processing and machine learning but also holds profound implications for clinical practice, emphasizing the importance of advanced computational tools in enhancing the effectiveness of medical diagnostics in oncology. Future work could focus on integrating this advanced framework with real-time imaging systems to facilitate live diagnostics during clinical procedures. This would allow radiologists and surgeons to view 3D representations of lung tumors in real-time, potentially during surgical planning or interventional procedures. Such integration could improve the quality of interventions and reduce the need for multiple imaging sessions, thereby enhancing patient management efficiency. Adapting the model to function in real-time will involve optimizing the algorithms for faster processing speeds without sacrificing the reliability of tumor detection and segmentation. Another area for future work is expanding the application of this framework to other types of cancer and complex diseases, where imaging plays a crucial role in diagnosis and treatment planning. The utility of the model could be significantly broadened by customizing the model to handle different kinds of tissue characteristics and disease markers found in other cancers, such as breast, prostate, or brain cancers. This would require adapting the current algorithms to recognize and process the unique patterns and features of different disease states, potentially helping to identify and manage a broader range of conditions. Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Acknowledgements This research is supported by Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2025R897), Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia. Author contributions Zhisen He: Conceptualization, methodology, study design, computational model implementation, manuscript writing and editing. Leila Jamel: Conceptualization, methodology development, study design, result interpretation, manuscript writing and editing. Danyi Huang: Methodology development, computational model implementation, performance evaluation, manuscript editing.Gaozhe Jiang: Methodology development, computational model implementation, data curation, validation, manuscript editing. Zaffar Ahmed Shaikh: Data curation, validation, study design, visualization, performance evaluation, manuscript editing. Khan, M. A. A.: Data curation, validation, performance evaluation. Seyed Jalaleddin Mousavirad: Methodology, manuscript writing and editing, result interpretation. Funding Open access funding provided by Mid Sweden University. Princess Nourah bint Abdulrahman University Researchers Supporting Project number PNURSP2025R897. Data availability The datasets used and/or analysed during the current study are available from the corresponding author on reasonable request. Declarations Competing interests The authors declare no competing interests. References 1. Lahiri, A. et al. Lung cancer immunotherapy: progress, pitfalls, and promises, Molecular cancer 10.1186/s12943-023-01740-y PMC9942077 36810079 2. Najafi, H. et al. A Novel Method for 3D Lung Tumor Reconstruction Using Generative Models, Diagnostics 10.3390/diagnostics14222604 PMC11592759 39594270 3. Lagzian A Increased V-ATPase activity can lead to chemo-resistance in oral squamous cell carcinoma via autophagy induction: new insights Med. Oncol. 2024 41 5 108 10.1007/s12032-024-02313-9 38592406 Lagzian, A. et al. Increased V-ATPase activity can lead to chemo-resistance in oral squamous cell carcinoma via autophagy induction: new insights. Med. Oncol. 41 38592406 10.1007/s12032-024-02313-9 4. Zareiamand, H. et al. Cardiac magnetic resonance imaging (CMRI) applications in patients with chest pain in the emergency department: A narrative review, Diagnostics 13 10.3390/diagnostics13162667 PMC10453831 37627926 5. Motavaselian, M. et al. Rigi A, and Diagnostic performance of magnetic resonance imaging for detection of acute appendicitis in pregnant women; a systematic review and meta-analysis. Archives Acad. Emerg. Medicine 10 10.22037/aaem.v10i1.1727 PMC9676701 36426165 6. Shamabadi A Karimi H Arabzadeh Bahri R Motavaselian M Akhondzadeh S Emerging drugs for the treatment of irritability associated with autism spectrum disorder Expert Opin. Emerg. Drugs 2024 29 1 45 56 10.1080/14728214.2024.2313650 38296815 Shamabadi, A., Karimi, H., Arabzadeh Bahri, R., Motavaselian, M. & Akhondzadeh, S. Emerging drugs for the treatment of irritability associated with autism spectrum disorder. Expert Opin. Emerg. Drugs 29 38296815 10.1080/14728214.2024.2313650 7. Kiemen, A. L. et al. Tissue clearing and 3D reconstruction of digitized, serially sectioned slides provide novel insights into pancreatic cancer, Med 10.1016/j.medj.2022.11.009 PMC9922376 36773599 8. Sharafkhani, F., Corns, S. & Holmes, R. Multi-Step Ahead Water Level Forecasting Using Deep Neural Networks, Water 9. Shafiee, A., Rastegar Moghadam, H., Merikhipour, M. & Lin, J. Analyzing Post-Pandemic Remote Work Accessibility for Equity through Machine Learning Analysis, in International Conference on Transportation and Development 2024 10. Hong, L. et al. GAN-LSTM‐3D: an efficient method for lung tumour 3D reconstruction enhanced by attention‐based LSTM. CAAI Trans. Intell. Technology 11. Moravvej, S. V. et al. RLMD-PA: a reinforcement learning‐based myocarditis diagnosis combined with a population‐based algorithm for pretraining weights, Contrast Media & Molecular Imaging 10.1155/2022/8733632 PMC9262570 35833074 12. Danaei, S. et al. and Nahavandi S, Myocarditis Diagnosis: A Method using Mutual Learning-Based ABC and Reinforcement Learning, in IEEE 22nd International Symposium on Computational Intelligence and Informatics and 8th IEEE International Conference on Recent Achievements in Mechatronics, Automation, Computer Science and Robotics (CINTI-MACRo) 13. Moravvej, S. V. et al. RLMD-PA: A reinforcement learning-based myocarditis diagnosis combined with a population-based algorithm for pretraining weights, Contrast Media & Molecular Imaging 10.1155/2022/8733632 PMC9262570 35833074 14. Merikhipour M Khanmohammadidoustani S Abbasi M Transportation mode detection through Spatial attention-based transductive long short-term memory and off-policy feature selection Expert Syst. Appl. 2025 267 126196 10.1016/j.eswa.2024.126196 Merikhipour, M., Khanmohammadidoustani, S. & Abbasi, M. Transportation mode detection through Spatial attention-based transductive long short-term memory and off-policy feature selection. Expert Syst. Appl. 267 15. Sun G DA-TransUNet: integrating Spatial and channel dual attention with transformer U-net for medical image segmentation Front. Bioeng. Biotechnol. 2024 12 1398237 10.3389/fbioe.2024.1398237 38827037 PMC11141164 Sun, G. et al. DA-TransUNet: integrating Spatial and channel dual attention with transformer U-net for medical image segmentation. Front. Bioeng. Biotechnol. 12 38827037 10.3389/fbioe.2024.1398237 PMC11141164 16. Sun, G. et al. Fkd-med: privacy-aware, communication-optimized medical image segmentation via federated learning and model lightweighting through knowledge distillation. IEEE Access 17. Pan Y A mutual inclusion mechanism for precise boundary segmentation in medical images Front. Bioeng. Biotechnol. 2024 12 1504249 10.3389/fbioe.2024.1504249 39777107 PMC11704489 Pan, Y. et al. A mutual inclusion mechanism for precise boundary segmentation in medical images. Front. Bioeng. Biotechnol. 12 39777107 10.3389/fbioe.2024.1504249 PMC11704489 18. Mazhar, T. et al. The role of machine learning and deep learning approaches for the detection of skin cancer, in Healthcare, vol. 11, no. 3, 415 : MDPI. (2023). 10.3390/healthcare11030415 PMC9914395 36766989 19. Haq, I. et al. YOLO and residual network for colorectal cancer cell detection and counting, Heliyon 10.1016/j.heliyon.2024.e24403 PMC10831604 38304780 20. Zanddizari H Nguyen N Zeinali B Chang JM A new preprocessing approach to improve the performance of CNN-based skin lesion classification Med. Biol. Eng. Comput. 2021 59 5 1123 1131 10.1007/s11517-021-02355-5 33904008 Zanddizari, H., Nguyen, N., Zeinali, B. & Chang, J. M. A new preprocessing approach to improve the performance of CNN-based skin lesion classification. Med. Biol. Eng. Comput. 59 33904008 10.1007/s11517-021-02355-5 21. Rocha MB Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms Biocybernetics Biomedical Eng. 2024 44 4 824 835 10.1016/j.bbe.2024.10.001 Rocha, M. B. et al. Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms. Biocybernetics Biomedical Eng. 44 22. Yang J Xi C Detection of congestive heart failure based on Gramian angular field and two-dimensional symbolic phase permutation entropy Biocybernetics Biomedical Eng. 2024 44 3 674 688 10.1016/j.bbe.2024.06.005 Yang, J. & Xi, C. Detection of congestive heart failure based on Gramian angular field and two-dimensional symbolic phase permutation entropy. Biocybernetics Biomedical Eng. 44 23. Liang Y Guo C Heart failure disease prediction and stratification with Temporal electronic health records data using patient representation Biocybernetics Biomedical Eng. 2023 43 1 124 141 10.1016/j.bbe.2022.12.008 Liang, Y. & Guo, C. Heart failure disease prediction and stratification with Temporal electronic health records data using patient representation. Biocybernetics Biomedical Eng. 43 24. Thai, B., McNicholas, S., Shalamzari, S., Meng, P. & Picone, J. Towards a More Extensible Machine Learning Demonstration Tool, in 2023 IEEE Signal Processing in Medicine and Biology Symposium (SPMB) 25. Agnes SA Solomon AA Karthick K Wavelet U-Net++ for accurate lung nodule segmentation in CT scans: improving early detection and diagnosis of lung cancer Biomed. Signal Process. Control 2024 87 105509 10.1016/j.bspc.2023.105509 Agnes, S. A., Solomon, A. A. & Karthick, K. Wavelet U-Net + + for accurate lung nodule segmentation in CT scans: improving early detection and diagnosis of lung cancer. Biomed. Signal Process. Control 87 26. Xing J Li C Wu P Cai X Ouyang J Optimized fuzzy K-nearest neighbor approach for accurate lung cancer prediction based on radial endobronchial ultrasonography Comput. Biol. Med. 2024 171 108038 10.1016/j.compbiomed.2024.108038 38442552 Xing, J., Li, C., Wu, P., Cai, X. & Ouyang, J. Optimized fuzzy K-nearest neighbor approach for accurate lung cancer prediction based on radial endobronchial ultrasonography. Comput. Biol. Med. 171 38442552 10.1016/j.compbiomed.2024.108038 27. BR SRR Stacked neural Nets for increased accuracy on classification on lung cancer Measurement: Sens. 2024 32 101052 BR, S. R. R. et al. Stacked neural Nets for increased accuracy on classification on lung cancer. Measurement: Sens. 32 28. Gite S Mishra A Kotecha K Enhanced lung image segmentation using deep learning Neural Comput. Appl. 2023 35 31 22839 22853 10.1007/s00521-021-06719-8 PMC8720554 35002086 Gite, S., Mishra, A. & Kotecha, K. Enhanced lung image segmentation using deep learning. Neural Comput. Appl. 35 10.1007/s00521-021-06719-8 PMC8720554 35002086 29. Arvind S Tembhurne JV Diwan T Sahare P Improvised light weight deep CNN based U-Net for the semantic segmentation of lungs from chest X-rays Results Eng. 2023 17 100929 10.1016/j.rineng.2023.100929 Arvind, S., Tembhurne, J. V., Diwan, T. & Sahare, P. Improvised light weight deep CNN based U-Net for the semantic segmentation of lungs from chest X-rays. Results Eng. 17 30. Chen Y LDANet: automatic lung parenchyma segmentation from CT images Comput. Biol. Med. 2023 155 106659 10.1016/j.compbiomed.2023.106659 36791550 Chen, Y. et al. LDANet: automatic lung parenchyma segmentation from CT images. Comput. Biol. Med. 155 36791550 10.1016/j.compbiomed.2023.106659 31. Ghali R Akhloufi MA Vision Transformers for lung segmentation on CXR images SN Comput. Sci. 2023 4 4 414 10.1007/s42979-023-01848-4 37252339 PMC10206550 Ghali, R. & Akhloufi, M. A. Vision Transformers for lung segmentation on CXR images. SN Comput. Sci. 4 37252339 10.1007/s42979-023-01848-4 PMC10206550 32. Murugappan M Bourisly AK Prakash N Sumithra M Acharya UR Automated semantic lung segmentation in chest CT images using deep neural network Neural Comput. Appl. 2023 35 21 15343 15364 10.1007/s00521-023-08407-1 37273912 PMC10088735 Murugappan, M., Bourisly, A. K., Prakash, N., Sumithra, M. & Acharya, U. R. Automated semantic lung segmentation in chest CT images using deep neural network. Neural Comput. Appl. 35 37273912 10.1007/s00521-023-08407-1 PMC10088735 33. Ambesange S Annappa B Koolagudi SG Simulating federated transfer learning for lung segmentation using modified UNet model Procedia Comput. Sci. 2023 218 1485 1496 10.1016/j.procs.2023.01.127 36743787 PMC9886334 Ambesange, S., Annappa, B. & Koolagudi, S. G. Simulating federated transfer learning for lung segmentation using modified UNet model. Procedia Comput. Sci. 218 36743787 10.1016/j.procs.2023.01.127 PMC9886334 34. Hasan, D. & Abdulazeez, A. M. Lung segmentation from chest X-Ray images using Deeplabv3plus-Based CNN model. Indonesian J. Comput. Science 13 35. Swaminathan VP GAN based image segmentation and classification using Vgg16 for prediction of lung cancer J. Adv. Res. Appl. Sci. Eng. Technol. 2024 35 1 45 61 10.37934/araset.34.3.4561 Swaminathan, V. P. et al. GAN based image segmentation and classification using Vgg16 for prediction of lung cancer. J. Adv. Res. Appl. Sci. Eng. Technol. 35 36. Suji RJ Godfrey WW Dhar J Exploring pretrained encoders for lung nodule segmentation task using LIDC-IDRI dataset Multimedia Tools Appl. 2024 83 4 9685 9708 10.1007/s11042-023-15871-3 Suji, R. J., Godfrey, W. W. & Dhar, J. Exploring pretrained encoders for lung nodule segmentation task using LIDC-IDRI dataset. Multimedia Tools Appl. 83 37. Thangavel C Palanichamy J Effective deep learning approach for segmentation of pulmonary cancer in thoracic CT image Biomed. Signal Process. Control 2024 89 105804 10.1016/j.bspc.2023.105804 Thangavel, C. & Palanichamy, J. Effective deep learning approach for segmentation of pulmonary cancer in thoracic CT image. Biomed. Signal Process. Control 89 38. Cai J Zhu H Liu S Qi Y Chen R Lung image segmentation via generative adversarial networks Front. Physiol. 2024 15 1408832 10.3389/fphys.2024.1408832 39219839 PMC11365075 Cai, J., Zhu, H., Liu, S., Qi, Y. & Chen, R. Lung image segmentation via generative adversarial networks. Front. Physiol. 15 39219839 10.3389/fphys.2024.1408832 PMC11365075 39. Ramos L Pineda I A semiautomatic image Processing-Based method for binary segmentation of lungs in computed tomography images SN Comput. Sci. 2024 5 6 689 10.1007/s42979-024-03047-1 Ramos, L. & Pineda, I. A semiautomatic image Processing-Based method for binary segmentation of lungs in computed tomography images. SN Comput. Sci. 5 40. Zheng J Wang L Gui J Yussuf AH Study on lung CT image segmentation algorithm based on threshold-gradient combination and improved convex hull method Sci. Rep. 2024 14 1 17731 10.1038/s41598-024-68409-4 39085327 PMC11291637 Zheng, J., Wang, L., Gui, J. & Yussuf, A. H. Study on lung CT image segmentation algorithm based on threshold-gradient combination and improved convex hull method. Sci. Rep. 14 39085327 10.1038/s41598-024-68409-4 PMC11291637 41. Guo L Liu L Zhao Z Xia X An improved RIME optimization algorithm for lung cancer image segmentation Comput. Biol. Med. 2024 174 108219 10.1016/j.compbiomed.2024.108219 38581997 Guo, L., Liu, L., Zhao, Z. & Xia, X. An improved RIME optimization algorithm for lung cancer image segmentation. Comput. Biol. Med. 174 38581997 10.1016/j.compbiomed.2024.108219 42. Pandey, S. K. & Bhandari, A. K. Morphological active contour based SVM model for lung cancer image segmentation. Multimedia Tools Applications 43. Vijayakumar S Aarthy S Deepa D Suresh P Sustainable framework for automated segmentation and prediction of lung cancer in CT image using CapsNet with U-net segmentation Biomed. Signal Process. Control 2025 99 106873 10.1016/j.bspc.2024.106873 Vijayakumar, S., Aarthy, S., Deepa, D. & Suresh, P. Sustainable framework for automated segmentation and prediction of lung cancer in CT image using CapsNet with U-net segmentation. Biomed. Signal Process. Control 99 44. Qiao Z Wu L Heidari AA Zhao X Chen H An enhanced tree-seed algorithm for global optimization and neural architecture search optimization in medical image segmentation Biomed. Signal Process. Control 2025 104 107457 10.1016/j.bspc.2024.107457 Qiao, Z., Wu, L., Heidari, A. A., Zhao, X. & Chen, H. An enhanced tree-seed algorithm for global optimization and neural architecture search optimization in medical image segmentation. Biomed. Signal Process. Control 104 45. Li H Ren Z Zhu G Wang J AE-UNet: a composite lung CT image segmentation framework using attention mechanism and edge detection J. Supercomputing 2025 81 1 331 10.1007/s11227-024-06874-4 Li, H., Ren, Z., Zhu, G. & Wang, J. AE-UNet: a composite lung CT image segmentation framework using attention mechanism and edge detection. J. Supercomputing 81 46. Murugaraj SS Vadivelu K Sambandam PT Kumar BS Lung vessel segmentation and abnormality classification based on hybrid mobile-Lenet using CT image Biomed. Signal Process. Control 2025 100 107072 10.1016/j.bspc.2024.107072 Murugaraj, S. S., Vadivelu, K., Sambandam, P. T. & Kumar, B. S. Lung vessel segmentation and abnormality classification based on hybrid mobile-Lenet using CT image. Biomed. Signal Process. Control 100 47. Fu J Peng H Li B Wang J Liu Z LDN-SNP: SNP-based lightweight deep network for CT image segmentation of COVID-19 Expert Syst. Appl. 2025 263 125793 10.1016/j.eswa.2024.125793 Fu, J., Peng, H., Li, B., Wang, J. & Liu, Z. LDN-SNP: SNP-based lightweight deep network for CT image segmentation of COVID-19. Expert Syst. Appl. 263 48. Erciyes, K., Soydan, M. & Gumus, O. Deep-Learning Based 3D Lung Segmentation, in 2025 13th International Symposium on Digital Forensics and Security (ISDFS) 49. Yang, J. et al. A novel 3D lightweight model for COVID-19 lung CT lesion segmentation. Medical Eng. & Physics 10.1016/j.medengphy.2025.104297 40057358 50. Nguyen QH Hoang DA Van Pham H Combination of 2D and 3D nnU-Net for ground glass opacity segmentation in CT images of Post-COVID-19 patients Comput. Biol. Med. 2025 195 110376 10.1016/j.compbiomed.2025.110376 40543275 Nguyen, Q. H., Hoang, D. A. & Van Pham, H. Combination of 2D and 3D nnU-Net for ground glass opacity segmentation in CT images of Post-COVID-19 patients. Comput. Biol. Med. 195 40543275 10.1016/j.compbiomed.2025.110376 51. Vincy VAG Byeon H Mahajan D Tonk A Sunil J A 3D residual network-based approach for accurate lung nodule segmentation in CT images J. Radiation Res. Appl. Sci. 2025 18 2 101407 Vincy, V. A. G., Byeon, H., Mahajan, D., Tonk, A. & Sunil, J. A 3D residual network-based approach for accurate lung nodule segmentation in CT images. J. Radiation Res. Appl. Sci. 18 52. Jannat, M. et al. Lung Segmentation with Lightweight Convolutional Attention Residual U-Net, Diagnostics 10.3390/diagnostics15070854 PMC11988706 40218203 53. Vijh S Gaurav P Pandey HM Hybrid bio-inspired algorithm and convolutional neural network for automatic lung tumor detection Neural Comput. Appl. 2023 35 33 23711 23724 10.1007/s00521-020-05362-z Vijh, S., Gaurav, P. & Pandey, H. M. Hybrid bio-inspired algorithm and convolutional neural network for automatic lung tumor detection. Neural Comput. Appl. 35 54. Alshayeji MH Abed Se Lung cancer classification and identification framework with automatic nodule segmentation screening using machine learning Appl. Intell. 2023 53 16 19724 19741 10.1007/s10489-023-04552-1 Alshayeji, M. H., Abed & Se Lung cancer classification and identification framework with automatic nodule segmentation screening using machine learning. Appl. Intell. 53 55. Halder A Dey D Atrous Convolution aided integrated framework for lung nodule segmentation and classification Biomed. Signal Process. Control 2023 82 104527 10.1016/j.bspc.2022.104527 Halder, A. & Dey, D. Atrous Convolution aided integrated framework for lung nodule segmentation and classification. Biomed. Signal Process. Control 82 56. Venkatesh, C. et al. Deep Learning and Patch Processing Based Lung Cancer Detection on CT Images, in International Conference on Communications and Cyber Physical Engineering 57. Sundarrajan, M., Perumal, S., Sasikala, S., Ramachandran, M. & Pradeep, N. Lung Cancer Detection Using Explainable Artificial Intelligence in Medical Diagnosis, in Advances in Explainable AI Applications for Smart Cities 58. Srinivasulu, A. et al. Lung malignant tumor data analytics using fusion ECNN and ERNN, in Handbook of Artificial Intelligence Applications for Industrial Sustainability: CRC, 47–63. (2024). 59. Tang T Zhang R A Multi-Task model for pulmonary nodule segmentation and classification J. Imaging 2024 10 9 234 10.3390/jimaging10090234 39330454 PMC11433280 Tang, T. & Zhang, R. A Multi-Task model for pulmonary nodule segmentation and classification. J. Imaging 10 39330454 10.3390/jimaging10090234 PMC11433280 60. Kongkham, D., Beenarani, B., Nirmala, P., Kumaresan, S. J. & Senthilkumar, C. Deep Learning Approaches for Segmentation of Lung Nodules in CT Scans, in 2nd International Conference on Computer, Communication and Control (IC4) 61. Gautam N Basu A Sarkar R Lung cancer detection from thoracic CT scans using an ensemble of deep learning models Neural Comput. Appl. 2024 36 5 2459 2477 10.1007/s00521-023-09130-7 Gautam, N., Basu, A. & Sarkar, R. Lung cancer detection from thoracic CT scans using an ensemble of deep learning models. Neural Comput. Appl. 36 62. Sadremomtaz A Zadnorouzi M Improving the quality of pulmonary nodules segmentation using the new proposed U-Net neural network Intelligence-Based Med. 2024 10 100166 10.1016/j.ibmed.2024.100166 Sadremomtaz, A. & Zadnorouzi, M. Improving the quality of pulmonary nodules segmentation using the new proposed U-Net neural network. Intelligence-Based Med. 10 63. Pan X LesionMix data enhancement and entropy minimization for semi-supervised lesion segmentation of lung cancer Appl. Soft Comput. 2024 167 112244 10.1016/j.asoc.2024.112244 Pan, X. et al. LesionMix data enhancement and entropy minimization for semi-supervised lesion segmentation of lung cancer. Appl. Soft Comput. 167 64. Rathan N Lokesh S Enhanced lung cancer diagnosis and staging with hrnet: A deep learning approach Int. J. Imaging Syst. Technol. 2024 34 6 e23193 10.1002/ima.23193 Rathan, N. & Lokesh, S. Enhanced lung cancer diagnosis and staging with hrnet: A deep learning approach. Int. J. Imaging Syst. Technol. 34 65. Shi X Zhang Z MAST-UNet More adaptive semantic texture for segmenting pulmonary nodules Biomed. Signal Process. Control 2025 99 106804 10.1016/j.bspc.2024.106804 Shi, X., Zhang, Z., MAST-UNet. & More adaptive semantic texture for segmenting pulmonary nodules. Biomed. Signal Process. Control 99 66. Miao S Deep learning-based CT image for pulmonary nodule classification with intrathoracic fat: A multicenter study Biomed. Signal Process. Control 2025 100 106938 10.1016/j.bspc.2024.106938 Miao, S. et al. Deep learning-based CT image for pulmonary nodule classification with intrathoracic fat: A multicenter study. Biomed. Signal Process. Control 100 67. Hajihosseinlou M Maghsoudi A Ghezelbash R Regularization in machine learning models for MVT Pb-Zn prospectivity mapping: applying Lasso and elastic-net algorithms Earth Sci. Inf. 2024 17 5 4859 4873 10.1007/s12145-024-01404-5 Hajihosseinlou, M., Maghsoudi, A. & Ghezelbash, R. Regularization in machine learning models for MVT Pb-Zn prospectivity mapping: applying Lasso and elastic-net algorithms. Earth Sci. Inf. 17 68. Yu J ETMO-NAS: an efficient two-step multimodal one-shot NAS for lung nodules classification Biomed. Signal Process. Control 2025 104 107479 10.1016/j.bspc.2024.107479 Yu, J. et al. ETMO-NAS: an efficient two-step multimodal one-shot NAS for lung nodules classification. Biomed. Signal Process. Control 104 69. Jiang C Kolmogorov–Arnold networks and multi slice partition channel priority attention in convolutional neural network for lung nodule detection Biomed. Signal Process. Control 2025 103 107358 10.1016/j.bspc.2024.107358 Jiang, C. et al. Kolmogorov–Arnold networks and multi slice partition channel priority attention in convolutional neural network for lung nodule detection. Biomed. Signal Process. Control 103 70. Xue, X., Ma, Y., Du, W. & Peng, Y. Squeeze-and-excitation vision transformer for lung nodule classification. IEEE Access 71. Elhassan, S. M., Darwish, S. M. & Elkaffas, S. M. An enhanced lung cancer detection approach using Dual-Model deep learning technique. CMES-Computer Model. Eng. & Sciences 142 72. Abdulqader AF Multi-objective deep learning for lung cancer detection in CT images: enhancements in tumor classification, localization, and diagnostic efficiency Discover Oncol. 2025 16 1 529 10.1007/s12672-025-02314-8 PMC12000487 40232589 Abdulqader, A. F. et al. Multi-objective deep learning for lung cancer detection in CT images: enhancements in tumor classification, localization, and diagnostic efficiency. Discover Oncol. 16 10.1007/s12672-025-02314-8 PMC12000487 40232589 73. Liu Z Wei L Song T Optimized YOLOv11 model for lung nodule detection Biomed. Signal Process. Control 2025 107 107830 10.1016/j.bspc.2025.107830 Liu, Z., Wei, L. & Song, T. Optimized YOLOv11 model for lung nodule detection. Biomed. Signal Process. Control 107 74. Li Y Hui L Wang X Zou L Chua S Lung nodule detection using a multi-scale convolutional neural network and global channel Spatial attention mechanisms Sci. Rep. 2025 15 1 12313 10.1038/s41598-025-97187-w 40210738 PMC11986029 Li, Y., Hui, L., Wang, X., Zou, L. & Chua, S. Lung nodule detection using a multi-scale convolutional neural network and global channel Spatial attention mechanisms. Sci. Rep. 15 40210738 10.1038/s41598-025-97187-w PMC11986029 75. Pérez-López, A., Torres-Suárez, A. I., Martín-Sabroso, C. & Aparicio-Blanco, J. An overview of in vitro 3D models of the blood-brain barrier as a tool to predict the in vivo permeability of nanomedicines. Advanced Drug Delivery Reviews 10.1016/j.addr.2023.114816 37003488 76. Lin H Volumetric medical image segmentation via fully 3D adaptation of segment anything model Biocybernetics Biomedical Eng. 2025 45 1 1 10 10.1016/j.bbe.2024.11.001 Lin, H. et al. Volumetric medical image segmentation via fully 3D adaptation of segment anything model. Biocybernetics Biomedical Eng. 45 77. Zhu Y Zheng F Gong Y Yin D Liu Y Biomechanical behavior of customized splint for the patient with temporomandibular disorders: A three-dimensional finite element analysis Biocybernetics Biomedical Eng. 2024 44 1 83 94 10.1016/j.bbe.2023.12.007 Zhu, Y., Zheng, F., Gong, Y., Yin, D. & Liu, Y. Biomechanical behavior of customized splint for the patient with temporomandibular disorders: A three-dimensional finite element analysis. Biocybernetics Biomedical Eng. 44 78. Mehrnia, M. et al. Novel Self-Calibrated Threshold-Free probabilistic fibrosis signature technique for 3D late gadolinium enhancement MRI. IEEE Trans. Biomedical Engineering 10.1109/TBME.2024.3476930 PMC11875924 39383069 79. Ju J Improving small intestinal stromal tumor detection using 3D context and auxiliary priori cues Biomed. Signal Process. Control 2025 102 107231 10.1016/j.bspc.2024.107231 Ju, J. et al. Improving small intestinal stromal tumor detection using 3D context and auxiliary priori cues. Biomed. Signal Process. Control 102 80. Saxton SH Stevens KR 2D and 3D liver models J. Hepatol. 2023 78 4 873 875 10.1016/j.jhep.2022.06.022 36038394 Saxton, S. H. & Stevens, K. R. 2D and 3D liver models. J. Hepatol. 78 36038394 10.1016/j.jhep.2022.06.022 81. Nashtahosseini Z Aghamaali MR Sadeghi F Heydari N Parsian H Circulating status of MicroRNAs 660-5p and 210‐3p in breast cancer patients J. Gene. Med. 2021 23 4 e3320 10.1002/jgm.3320 33533518 Nashtahosseini, Z., Aghamaali, M. R., Sadeghi, F., Heydari, N. & Parsian, H. Circulating status of MicroRNAs 660-5p and 210‐3p in breast cancer patients. J. Gene. Med. 23 33533518 10.1002/jgm.3320 82. Oh N Automated 3D liver segmentation from hepatobiliary phase MRI for enhanced preoperative planning Sci. Rep. 2023 13 1 17605 10.1038/s41598-023-44736-w 37848662 PMC10582008 Oh, N. et al. Automated 3D liver segmentation from hepatobiliary phase MRI for enhanced preoperative planning. Sci. Rep. 13 37848662 10.1038/s41598-023-44736-w PMC10582008 83. Gu, C., Gao, H. & Combining GAN and LSTM models for 3D reconstruction of lung tumors from CT scans. International J. Adv. Comput. Sci. Applications 14 84. Rezaei SR Ahmadi A A GAN-based method for 3D lung tumor reconstruction boosted by a knowledge transfer approach Multimedia Tools Appl. 2023 82 28 44359 44385 10.1007/s11042-023-15232-0 PMC10106883 37362675 Rezaei, S. R. & Ahmadi, A. A GAN-based method for 3D lung tumor reconstruction boosted by a knowledge transfer approach. Multimedia Tools Appl. 82 10.1007/s11042-023-15232-0 PMC10106883 37362675 85. Fawzy S Moustafa HED AbdelHay EH Ata MM Proposed optimized active contour based approach for accurately skin lesion segmentation Multimedia Tools Appl. 2024 83 2 5745 5797 10.1007/s11042-023-15436-4 Fawzy, S., Moustafa, H. E. D., AbdelHay, E. H. & Ata, M. M. Proposed optimized active contour based approach for accurately skin lesion segmentation. Multimedia Tools Appl. 83 86. Karrar A Mabrouk MS Abdel Wahed M Accurate quantification of small pulmonary nodules using 3D reconstruction of 2D computed tomography lung images J. Adv. Eng. Trends 2023 42 2 1 15 10.21608/jaet.2022.83371.1117 Karrar, A., Mabrouk, M. S. & Abdel Wahed, M. Accurate quantification of small pulmonary nodules using 3D reconstruction of 2D computed tomography lung images. J. Adv. Eng. Trends 42 87. Dlamini S Chen Y-H Kuo C-F-J Complete fully automatic detection, segmentation and 3D reconstruction of tumor volume for non-small cell lung cancer using YOLOv4 and region-based active contour model Expert Syst. Appl. 2023 212 118661 10.1016/j.eswa.2022.118661 Dlamini, S., Chen, Y-H. & Kuo, C-F-J. Complete fully automatic detection, segmentation and 3D reconstruction of tumor volume for non-small cell lung cancer using YOLOv4 and region-based active contour model. Expert Syst. Appl. 212 88. Shi Z Reconstructing 3-D lung surfaces from a single 2‐D chest x‐ray image via vision transformer Med. Phys. 2024 51 4 2806 2816 10.1002/mp.16781 37819009 Shi, Z. et al. Reconstructing 3-D lung surfaces from a single 2‐D chest x‐ray image via vision transformer. Med. Phys. 51 37819009 10.1002/mp.16781 89. Huang T Yin X Jiang E A novel 3-step technique for 3D tumor reconstruction using generative adversarial networks and an attention-based long short-term memory Res. Biomedical Eng. 2025 41 2 30 10.1007/s42600-025-00410-w Huang, T., Yin, X. & Jiang, E. A novel 3-step technique for 3D tumor reconstruction using generative adversarial networks and an attention-based long short-term memory. Res. Biomedical Eng. 41 90. Kordani M Improving long-term flood forecasting accuracy using ensemble deep learning models and an attention mechanism J. Hydrol. Eng. 2024 29 6 04024042 10.1061/JHYEFF.HEENG-6262 Kordani, M. et al. Improving long-term flood forecasting accuracy using ensemble deep learning models and an attention mechanism. J. Hydrol. Eng. 29 91. Khanmohammadidoustani S HassanzadehKermanshahi K Mohammadi A Kermanshahi S Evaluating acceptance of a more strict plate control policy among motorcycle riders in Tehran Int. J. Transp. Eng. 2023 11 1 1301 1311 Khanmohammadidoustani, S., HassanzadehKermanshahi, K., Mohammadi, A. & Kermanshahi, S. Evaluating acceptance of a more strict plate control policy among motorcycle riders in Tehran. Int. J. Transp. Eng. 11 92. Naderi F Towards chemical source tracking and characterization using physics-informed neural networks Atmos. Environ. 2024 334 120679 10.1016/j.atmosenv.2024.120679 Naderi, F. et al. Towards chemical source tracking and characterization using physics-informed neural networks. Atmos. Environ. 334 93. Moravvej SV Mousavirad SJ Oliva D Schaefer G Sobhaninia Z An improved de algorithm to optimise the learning process of a bert-based plagiarism detection model, in 2022 IEEE Congress on Evolutionary Computation (CEC) 2022 Padua, Italy IEEE 1 7 Moravvej, S. V., Mousavirad, S. J., Oliva, D., Schaefer, G. & Sobhaninia, Z. An improved de algorithm to optimise the learning process of a bert-based plagiarism detection model, in 2022 IEEE Congress on Evolutionary Computation (CEC), Padua, Italy, 1–7 : IEEE. (2022). 94. EskandariNasab M Raeisi Z Lashaki RA Najafi H A GRU–CNN model for auditory attention detection using microstate and recurrence quantification analysis Sci. Rep. 2024 14 1 8861 10.1038/s41598-024-58886-y 38632246 PMC11024110 EskandariNasab, M., Raeisi, Z., Lashaki, R. A. & Najafi, H. A GRU–CNN model for auditory attention detection using microstate and recurrence quantification analysis. Sci. Rep. 14 38632246 10.1038/s41598-024-58886-y PMC11024110 95. Shakya AK Pillai G Chakrabarty S Reinforcement learning algorithms: A brief survey Expert Syst. Appl. 2023 231 120495 10.1016/j.eswa.2023.120495 Shakya, A. K., Pillai, G. & Chakrabarty, S. Reinforcement learning algorithms: A brief survey. Expert Syst. Appl. 231 96. Sivamayilvelan K Rajasekar E Vairavasundaram S Balachandran S Suresh V Flexible recommendation for optimizing the debt collection process based on customer risk using deep reinforcement learning Expert Syst. Appl. 2024 256 124951 10.1016/j.eswa.2024.124951 Sivamayilvelan, K., Rajasekar, E., Vairavasundaram, S., Balachandran, S. & Suresh, V. Flexible recommendation for optimizing the debt collection process based on customer risk using deep reinforcement learning. Expert Syst. Appl. 256 97. Banar S Mohammadi R SeismoNet A proximal policy optimization-based earthquake early warning system using dilated Convolution layers and online data augmentation Expert Syst. Appl. 2024 253 124337 10.1016/j.eswa.2024.124337 Banar, S., Mohammadi, R., SeismoNet: & A proximal policy optimization-based earthquake early warning system using dilated Convolution layers and online data augmentation. Expert Syst. Appl. 253 98. McNeel, R. 10 September 2024). Rhinoceros 3d https://www.rhino3d.com ",
  "metadata": {
    "Title of this paper": "A proximal policy optimization-based earthquake early warning system using dilated Convolution layers and online data augmentation",
    "Journal it was published in:": "Scientific Reports",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12480523/"
  }
}
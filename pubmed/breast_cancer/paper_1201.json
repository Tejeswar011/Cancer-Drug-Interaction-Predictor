{
  "title": "Paper_1201",
  "abstract": "pmc Diagnostics (Basel) Diagnostics (Basel) 2841 diagno diagnostics Diagnostics 2075-4418 Multidisciplinary Digital Publishing Institute  (MDPI) PMC12468973 PMC12468973.1 12468973 12468973 41008676 10.3390/diagnostics15182304 diagnostics-15-02304 1 Article GlioSurvQNet: A DuelContextAttn DQN Framework for Brain Tumor Prognosis with Metaheuristic Optimization Renugadevi M. Conceptualization Methodology Data curation Writing – original draft 1 https://orcid.org/0000-0002-6857-2254 Gonuguntla Venkateswarlu Validation Formal analysis Investigation Writing – review & editing Funding acquisition 2 * https://orcid.org/0000-0001-9768-6798 Masad Ihssan S. Validation Investigation Writing – review & editing Funding acquisition 3 4 5 Venkat Babu G. Software Investigation Writing – review & editing 1 https://orcid.org/0000-0001-6929-6039 Narasimhan K. Conceptualization Methodology Validation Formal analysis Writing – review & editing Supervision Project administration Funding acquisition 1 * 1 renugadevi@ece.sastra.ac.in venkatbabu@ece.sastra.edu 2 3 masad.i@gust.edu.kw 4 5 * venkateswarlu.phd@gmail.com knr@ece.sastra.edu 11 9 2025 9 2025 15 18 497628 2304 22 7 2025 23 8 2025 29 8 2025 11 09 2025 27 09 2025 29 09 2025 © 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ Background/Objectives: Methods: Results: Conclusions: brain tumor classification Dueling DQN metaheuristic optimization radiomics reinforcement learning survival prediction Department of Science & Technology—SERB CRG/2022/008050 The Core Research Grant (CRG/2022/008050) of the Department of Science & Technology—SERB is funding this research project. The authors gratefully thank the funding agency for their support in this research work. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Gliomas are one of the most common types of brain tumors [ 1 2 MRI scans are widely used to examine brain tumors, and radiomics allows us to extract useful information from these images by turning them into quantitative features. In the past, machine learning methods like support vector machines and random forests have been used to classify glioma types based on these features. While these approaches can work well, they often rely too much on carefully selected features and may not perform consistently when data are limited or unbalanced [ 3 4 More recently, deep learning models such as convolutional neural networks (CNNs) and transformers have become popular because they can learn patterns directly from imaging data [ 5 6 However, despite their effectiveness, machine learning and deep learning methodologies in medical image analysis still face critical challenges, including the requirement for large labeled datasets, the high dimensionality of radiomics features, limited generalization to unseen data, and insufficient interpretability, ultimately leading to suboptimal predictive performance [ 7 8 9 10 11 12 Another important step in building accurate models is selecting the right features. Radiomics often produces a large number of features, many of which may not be useful. Traditional selection methods like LASSO or recursive feature elimination work, but they can struggle when the data are high-dimensional or unbalanced [ 13 14 This study introduces GlioSurvQNet, a reinforcement-learning-based framework for glioma grade classification and survival prediction. At the heart of this system is a custom deep RL model called DuelContextAttn DQN, which uses an attention mechanism to focus on the most informative features during learning. To improve the model’s reliability and reduce unnecessary complexity, we apply an ensemble of metaheuristic optimization techniques to select key radiomics features, followed by SHAP-based filtering to enhance interpretability. The framework is evaluated on the BraTS2020 dataset, which includes multimodal MRI scans and clinical information. GlioSurvQNet is trained to perform two tasks: (1) classify tumors as LGG or HGG, and (2) categorize patients into short, medium, or long-term survival groups. Our results show that the model performs strongly in both tasks while offering better adaptability and explanation than traditional approaches. A reinforcement learning-based model that can handle both tumor grading and survival prediction tasks. A novel deep Q-network with attention (DuelContextAttn DQN) that improves decision-making using radiomics data. A robust feature selection pipeline combining multiple metaheuristic algorithms with SHAP for clinical interpretability. This work shows the potential of combining reinforcement learning, optimized feature selection, and explainable AI to support accurate and more reliable brain tumor analysis. The paper is structured as follows: Section 2 Section 3 Section 4 2. Materials and Methods This section details the development and evaluation of the proposed GlioSurvQNet framework. The model is designed to perform two tasks: (1) classify glioma tumors as LGG or HGG, and (2) predict overall survival duration in three categories. The overall workflow, including radiomics feature extraction from multimodal MRI scans, ensemble-based feature selection, and reinforcement-learning-driven classification and survival prediction, is illustrated in Figure 1 2.1. Dataset Description This Multimodal Brain Tumor Segmentation dataset is essential for tasks such as tumor segmentation, classification, and survival analysis [ 15 16 2.2. Feature Extraction and Selection Radiomics feature extraction is a crucial step in quantitative medical imaging analysis, enabling the derivation of numerous informative features that support diagnosis, prognosis, and clinical research. To address the data imbalance between LGG and HGG cases, various augmentation strategies were employed exclusively for LGG cases. These included spatial alterations such as directional flipping, rotational transformations, and controlled noise perturbations, all designed to expand the dataset while retaining anatomical validity synthetically. As a result, the LGG class was increased to 304 samples, producing a balanced dataset comprising 595 images (304 LGG and 291 HGG) for downstream classification tasks. Importantly, the data split was performed with unique patient identifiers to ensure that no augmented versions of the same patient appeared in both the training and testing sets, thereby preventing any risk of data leakage. Radiomics features were extracted using PyRadiomics (radiomics 3.1.0). Prior to feature extraction, MRI volumes were normalized per volume using z-score normalization within the brain mask. Volumes were resampled to isotropic voxels of 1 × 1 × 1 Supplementary Materials Table 1 2.2.1. Harris Hawks Optimization Algorithm HHO is a nature-inspired optimization algorithm that models the hunting strategies of Harris hawks [ 17 2.2.2. Modified Gorilla Troops Optimization Algorithm mGTO is an enhanced variant of the Gorilla Troops Optimization (GTO) algorithm, which draws inspiration from the social structure and movement strategies of gorilla troops [ 18 2.2.3. Zebra Optimization Algorithm ZOA is a bio-inspired metaheuristic based on the social behavior and movement patterns of zebras in the wild [ 19 Feature Selection The hyperparameters for the metaheuristic feature selection algorithms (HHO, mGTO, ZOA) are summarized in Table 2 The number of features selected by each method, including the ensemble and SHAP refinement, is summarized in Figure 2 20 Figure 3 Figure 4 Algorithm 1 Input: D X y θ Normalize X function Fitness Convert s o l u t i o n  if then  return 1.0  end if Train SVM on selected features  return 1 − end function Define a binary optimization problem with Fitness as objective Run HHO, mGTO, and ZOA to generate feature masks Apply majority voting to obtain ensemble mask Train SVM using features selected by ensemble mask Compute SHAP values; rank features by mean absolute value Select top features achieving θ Output: X SHAP Feature Fusion After identifying modality-specific features using the ensemble-SHAP approach, feature fusion was performed to explore multi-modal integration strategies. This fusion was performed at three levels: dual, triple, and four-modality combinations. In the dual-modality setup, six combinations were evaluated: Flair + T1, Flair + T2, Flair + T1CE, T1 + T2, T1 + T1CE, and T2 + T1CE. These pairs allowed the model to benefit from complementary structural and contrast-enhanced information. Building upon this, triple-modality fusion was explored through four key combinations: Flair + T1 + T2, Flair + T1 + T1CE, Flair + T2 + T1CE, and T1 + T2 + T1CE. These combinations integrated a broader context of anatomical and pathological features, enriching the model’s understanding of tumor heterogeneity. Finally, a four-modality fusion incorporating all sequences, Flair, T1, T2, and T1CE, was performed to capture the full range of tumor characteristics. This step aimed to leverage complementary information across MRI modalities for improved glioma classification performance. 2.3. DuelContextAttn DQN RL Framework The reinforcement learning environment was implemented using the gym.Env State: Each state represents a set of extracted features corresponding to a patient’s medical data, forming a unique feature vector for an individual sample. Action Space: LGG/HGG Classification: The action space is discrete with two possible actions: action 0 and action 1. Action 0 corresponds to predicting an LGG, while action 1 represents a prediction of HGG. Survival Classification: The action space consists of three discrete actions: 0, 1, and 2, corresponding to predictions of short-term, medium-term, and long-term survival, respectively. Reward Function: The environment provides feedback in the form of rewards after each action. A reward of + 1 − 1 Agent: The agent is based on a Dueling Deep Q-Network architecture, specifically a DuelContextAttn DQN variant. It employs an ϵ Model: The DuelContextAttn DQN architecture includes shared hidden layers followed by two separate output streams: one for estimating the state-value function and another for computing action advantages. A contextual attention mechanism dynamically assigns learnable weights to modulate the fusion of these two streams, enabling more accurate Q-value estimation. 2.3.1. DuelContextAttn DQN DuelContextAttn DQN is an enhanced version of the traditional DQN algorithm used in the realm of reinforcement learning [ 21 22 s a Q ( s , a ) Q ( s , a ; θ ) θ DQN employs two essential techniques to enhance training stability: experience replay and the use of two Q-networks, namely the main Q-network and the target network. Experience replay functions as a memory buffer, storing past experiences ( s , a , r , s ′ ) The target network’s parameters ( θ − ) 1 2 (1) y t D Q N = r t + 1 + γ Q ( s t + 1 , a ′ ; θ t − ) γ θ − The loss function is: (2) L i ( θ i ) = E ( s , a , r , s ′ ) y t D Q N − Q ( s , a ; θ i ) 2 Additionally, DQN implements an ϵ Dueling DQN is an improvement over DQN to tackle the challenge of overestimation bias, which occurs when the Q-network overestimates action values, leading to suboptimal policies. In addition to the standard DQN approach, Dueling DQN introduces an enhanced architecture that consists of two essential components: the Value function and the Advantage function. This dual approach allows for a more effective computation of Q-values, leading to enhanced stability and efficiency in the learning process. 2.3.2. Architecture Overview The DuelContextAttn DQN builds upon the foundation of Dueling DQN by introducing a context-aware attention mechanism that dynamically adjusts the relative importance of the value and advantage streams. The architecture of the DuelContextAttn DQN model for Classification and Survival Prediction is shown in Figure 5 V ( s ) A ( s , a ) (3) Q ( s , a ) = V ( s ) + A ( s , a ) − 1 | A | ∑ a ′ A ( s , a ′ ) This formulation implicitly assumes a fixed weighting between the value and advantage components across all states and actions. However, such an assumption may not be optimal, especially in high-dimensional domains like medical imaging, where the relative importance of features can vary significantly depending on the context. To address this limitation, the DuelContextAttn DQN introduces a learnable attention-based fusion weight w ( s ) Value Stream ( V ( s ) s Advantage Stream ( A ( s , a ) a s Attention Fusion Weight ( w ( s ) The final Q-value is computed as a convex combination of the value and normalized advantage streams: (4) Q ( s , a ; θ ) = w ( s ) · V ( s ) + 1 − w ( s ) · A ( s , a ) − 1 | A | ∑ a ′ A ( s , a ′ ) This adaptive fusion mechanism enables the network to prioritize different aspects of the decision process depending on the contextual information present in each state, which is especially beneficial in medical imaging applications where subtle differences can be critical. The overall functioning of DuelContextAttn DQN is illustrated in Algorithm 2. Each episode begins with a reset of the environment, enabling the agent to interact until it has completed a full traversal of the dataset. For every time step, the agent selects an action according to the current state using the epsilon-greedy policy. The environment then provides the subsequent state, the reward received, and an indication of whether the episode has ended. This interaction is stored in the agent’s memory, which is later used for replay to refine the Q-values. The agent’s performance is evaluated at the end of each episode in terms of accuracy and reward. The training process spans 50 episodes representing complete passes through the dataset, enabling iterative policy refinement for classification using radiomics features. The agent utilizes the Adam optimizer with a learning rate of 0.001 to ensure efficient gradient updates and stable learning. The mini-batches of size 16 are drawn from the replay buffer for batch learning. The Mean Squared Error (MSE) loss function is applied to measure the discrepancy between predicted and target Q-values. The training parameters used to train the DuelContextAttn DQN model are listed in Table 3 Algorithm 2 Step 1: Initialize Initialize Q-network and target Q-network with dueling architecture and context-aware self-attention. Initialize replay buffer D for do Reset environment and observe initial state s  for do Select action a t ϵ a t = arg max a Q ( s , a ; θ ) Execute a t r t s t + 1 Store ( s t , a t , r t , s t + 1 ) D Sample mini-batch of transitions ( s , a , r , s ′ ) D  Step 2: Compute Target Q-values y i = r i + γ max a ′ Q ′ ( s ′ , a ′ ; θ − ) , if not terminal r i , if terminal  Step 3: Compute Predicted Q-values with Attention Fusion Compute value stream V ( s ; θ ) Compute advantage stream A ( s , a ; θ ) Compute attention fusion weight w ( s ; θ ) Fuse: Q ( s , a ; θ ) = w · V ( s ) + ( 1 − w ) · A ( s , a ) − 1 | A | ∑ a ′ A ( s , a ′ )  Step 4: Compute Loss (MSE) L ( θ ) = 1 N ∑ i ∈ N Q θ ( s i , a i ) − Q θ ′ ( s i , a i ) 2 Q θ ′ = R ( s t , a t ) + γ max a i ′ Q θ ( s i ′ , a i ′ )  Step 5: Update Parameters Perform gradient descent on L ( θ ) Update target network parameters: θ − ← θ  end for end for Step 6: Repeat Until Convergence Repeat steps until convergence or maximum episodes. 2.4. Survival Prediction The objective of survival prediction is to estimate the overall survival (OS) duration of patients using pre-treatment multimodal MRI scans along with relevant clinical variables. A total of 235 cases were analyzed and categorized into three survival classes: short (0–250 days), medium (251–500 days), and long (501–1800 days), consisting of 75, 86, and 74 cases, respectively. The distribution of these classes is illustrated in Figure 6 An ensemble of optimization algorithms, integrated with SHAP analysis, was applied for feature selection, enabling the identification of the most relevant features. As shown in Figure 7 Figure 8 Figure 9 3. Results and Discussion 3.1. Classification Performance and Comparative Study The DuelContextAttn DQN model was trained using selected radiomics features for each MRI modality to classify LGG and HGG. All experiments were conducted on a workstation equipped with an Intel Core i7 CPU and an NVIDIA T1000 GPU with 4 GB memory, running a 64-bit Windows operating system. Each training run per modality consisted of 50 episodes, with an average episode time of approximately 610 s, resulting in a total runtime of ∼8.5 h per modality. A total of 595 patient cases were included for glioma grading, and the dataset was split into 80% for training and 20% for testing. This resulted in 476 samples for training and 119 for testing, ensuring a balanced evaluation of model performance across both phases. For robust performance estimation, 5-fold cross-validation was applied on the training set, and mean ± standard deviation across folds is reported. Hyperparameter tuning was conducted to optimize model performance, focusing on γ ϵ ϵ min ϵ decay γ = 0.99 ϵ = 0.5 ϵ min = 0.01 ϵ decay = 0.995 Table 4 Table 5 23 In triple-modality combinations, F + T1 + T1CE and F + T2 + T1CE achieved high accuracies of 98.95 ± 0.03% and 98.74 ± 0.05%, respectively, highlighting the contribution of T1CE in enhancing tumor characterization when fused with other structural sequences. The four-modality fusion (F + T1 + T2 + T1CE) reached 98.32 ± 0.05%, slightly lower than the best dual- and triple-modality combinations. Statistical comparison between the dual-modality combination (Flair + T1CE) and the 4-modality fusion showed no significant improvement: McNemar’s test (stat = 3.0, p Figure 10 Additionally, the ROC curve ( Figure 11 Figure 11 Figure 11 Figure 11 Figure 12 To further strengthen our evaluation, the proposed method is compared with several strong conventional baselines to ensure fairness in implementation. All models were trained using the fused Flair + T1CE modality, which consistently yielded the highest accuracy in our experiments. The results, summarized in Table 6 Table 7 3.2. Survival Prediction Performance and Comparative Study The DuelContextAttn DQN model was developed to forecast the total number of survival days for patients, employing the same hyperparameters as those used in the classification task. The primary aim was to categorize tumor patients into three distinct survival groups: short, medium, and long. A total of 705 samples were used in this analysis, with an 80:20 split applied to create the training and testing datasets, respectively. This ensured that the model was trained on 564 cases and evaluated on 141, allowing for a robust assessment of its generalization capabilities. Table 8 In the dual modality group, the combination of FLAIR + T2 achieved the highest accuracy (93.71%), suggesting that these two structural modalities provide complementary information. FLAIR captures peritumoral edema, while T2 delineates both edema and tumor core, contributing to more reliable survival class predictions.For triple modality fusion, the best performance was observed with FLAIR + T2 + T1CE, reaching an accuracy of 93.82%. This shows that integrating contrast-enhanced imaging with structural and fluid-sensitive modalities significantly enhances the model’s ability to distinguish among short, medium, and long survival classes.The four modality fusion (FLAIR + T1 + T2 + T1CE) also performed well with an accuracy of 93.39%, though it did not exceed the best triple combination. This may be due to from feature redundancy, higher dimensionality, and an increased risk of overfitting. Moreover, the attention mechanism must distribute focus across multiple modalities, potentially diluting emphasis on the most informative features. Each modality also introduces its own noise and artifacts, so including too many can amplify unwanted signals rather than enhance the model’s discriminative power. Figure 13 Figure 14 Table 9 A critical comparison with existing approaches is presented in Table 9 32 33 34 35 36 37 38 39 diagnostics-15-02304-t009_Table 9 Table 9 Comparison of the DuelContextAttn DQN model with state-of-the-art methods for overall survival prediction. Literature Methods OS Prediction Models Accuracy (%) Guo et al. [ 32 Gradient Boosted Decision Tree 52.30 Pei et al. [ 34 Context-Aware deep neural network (CANet) 58.60 Osman [ 35 Ensemble ML models 57.80 Sun et al. [ 38 Random Forest 61.00 Shboul et al. [ 39 XGBoost 67.90 Soltani et al. [ 36 Artificial Neural Network 78.00 Cepeda et al. [ 33 Naive Bayes 80.00 Sanghani et al. [ 37 Support Vector Regression 86.00  Proposed  DuelContextAttn DQN  93.82 The proposed DuelContextAttn DQN model significantly outperforms existing methods with an accuracy of 93.82%, demonstrating its superior ability to capture complex patterns in multimodal data. By integrating deep reinforcement learning with context-aware attention mechanisms and optimized radiomics feature selection, the model effectively enhances OS prediction performance. 3.3. Abalation Study The ablation studies were conducted to evaluate the effectiveness of different feature selection strategies and reinforcement learning architectures. The analysis focuses on identifying the most efficient metaheuristic algorithm for feature selection and the most robust DQN variant for classification. HHO, ZOA, and mGTO were chosen because they are recent, high-performance metaheuristics shown to offer superior exploration–exploitation balance in complex, high-dimensional feature selection tasks. In contrast, classical methods such as GA, PSO, and ABC, while well established, often suffer from premature convergence or require extensive parameter tuning. Employing these newer algorithms enabled benchmarking against traditional approaches and demonstrated clear improvements in convergence speed and final fitness, as illustrated in Figure 15 An ablation study was performed to systematically evaluate the contribution of different architectural components to the final model’s performance. This analysis involved a comparison of six DQN variants, such as the standard DQN baseline, Double DQN, Dueling DQN, Dueling Double DQN, Dueling Double DQN with attention, and the final proposed model, DuelContextAttn DQN. All models were trained with the fused F + T1CE modality using identical splits, optimizer, schedule, replay buffer, target updates, and exploration policy. As summarized in Table 10 4. Conclusions GlioSurvQNet is a reinforcement learning-based framework that combines ensemble metaheuristic optimization with SHAP feature selection, modality fusion, and the DuelContextAttn DQN model for brain tumor classification and survival prediction. In the classification task, dual-modality combinations showed strong performance, with FLAIR + T1CE achieving the highest accuracy of 99.27%, demonstrating excellent discriminative power between LGG and HGG. In the survival classification task, the triple-modality fusion of FLAIR + T2 + T1CE yielded the best accuracy of 93.82%, underscoring the effectiveness of multimodal integration in enhancing predictive performance. This comprehensive strategy significantly enhances both performance and reliability, with potential benefits for clinical decision-making and patient outcomes. By leveraging reinforcement learning, the DuelContextAttn DQN continuously refines its decision-making through interaction with the environment, improving generalization and classification accuracy. Its adaptability makes it a scalable and robust solution for neuro-oncology. While GlioSurvQNet enables continuous learning, it lacks dynamic adaptation to evolving medical knowledge and treatment shifts. Moreover, its real-world applicability is constrained by reliance on a single dataset, absence of prospective clinical testing, and the need for enhanced robustness and explainability before clinical integration. It currently performs discrete survival classification; extending it to predict continuous survival times could improve clinical relevance but adds complexity. Future work includes enabling end-to-end learning from raw images and validating the model on large, multi-institutional datasets to assess real-world generalizability. Acknowledgments We acknowledge the `Symbiosis Centre for Medical Image Analysis, Symbiosis International (Deemed University); and Gulf University for Science and Technology (GUST) Engineering and Applied Innovation Research Center (GEAR), GUST, Mishref, Kuwait’, for their financial support in publishing this work. We also acknowledge the Department of Science and Technology, Government of India, for the technical and financial support provided under the DST-PURSE Program, Grant No. SR/PURSE/2023/181 (G)-TPN-88132. Disclaimer/Publisher’s Note: Supplementary Materials The following supporting information can be downloaded at: https://www.mdpi.com/article/10.3390/diagnostics15182304/s1 Author Contributions Conceptualization, M.R. and K.N.; methodology, M.R. and K.N.; software, M.R.; validation, K.N., G.V.B. and I.S.M.; formal analysis, K.N. and G.V.B.; investigation, I.S.M., G.V.B. and V.G.; resources, M.R.; data curation, M.R.; writing—original draft preparation, M.R.; writing—review and editing, M.R., K.N., G.V.B., I.S.M. and V.G.; visualization, M.R.; supervision, K.N.; project administration, K.N.; funding acquisition, K.N., G.V.B. and I.S.M. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement The BraTS2020 dataset used for this study is publicly available at https://www.med.upenn.edu/cbica/brats2020/data.html Conflicts of Interest The authors declare no conflicts of interest. References 1. Bleeker F.E. Molenaar R.J. Leenstra S. Recent advances in the molecular understanding of glioblastoma J. Neuro-Oncol. 2012 108 11 27 10.1007/s11060-011-0793-0 PMC3337398 22270850 2. Kaifi R. A Review of Recent Advances in Brain Tumor Diagnosis Based on AI-Based Classification Diagnostics 2023 13 3007 10.3390/diagnostics13183007 37761373 PMC10527911 3. Haq E.U. Jianjun H. Li K. Ulhaq H. Zhang T. An MRI-Based Deep Learning Approach for Efficient Classification of Brain Tumors J. Ambient Intell. Humaniz. Comput. 2021 14 6697 6718 10.1007/s12652-021-03535-9 4. Tabassum M. Suman A.A. Suero Molina E. Pan E. Di Ieva A. Liu S. Radiomics and Machine Learning in Brain Tumors and Their Habitat: A Systematic Review Cancers 2023 15 3845 10.3390/cancers15153845 37568660 PMC10417709 5. Akinyelu A.A. Zaccagna F. Grist J.T. Castelli M. Rundo L. Brain Tumor Diagnosis Using Machine Learning, Convolutional Neural Networks, Capsule Neural Networks and Vision Transformers, Applied to MRI: A Survey J. Imaging 2022 8 205 10.3390/jimaging8080205 35893083 PMC9331677 6. Wang P. Yang Q. He Z. Yuan Y. Vision Transformers in Multi-Modal Brain Tumor MRI Segmentation: A Review Meta-Radiology 2023 1 100004 10.1016/j.metrad.2023.100004 7. Stember J. Shalu H. Deep reinforcement learning classification of brain tumors on MRI Innovation in Medicine and Healthcare: Proceedings of 10th KES-InMed 2022 Springer Cham, Switzerland 2022 119 128 8. Sutton R.S. Barto A.G. Reinforcement Learning: An Introduction IEEE Trans. Neural Netw. 1998 9 1054 10.1109/TNN.1998.712192 9. AlMahamid F. Grolinger K. Reinforcement Learning Algorithms: An Overview and Classification Proceedings of the 2021 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE) Virtually 13–16 September 2021 1 7 10. Alrebdi N. Alrumiah S. Almansour A. Rassam M. Reinforcement Learning in Image Classification: A Review Proceedings of the 2022 2nd International Conference on Computing and Information Technology (ICCIT) Tabuk, Saudi Arabia 25–27 January 2022 79 86 11. Zhou S.K. Le H.N. Luu K. Nguyen H.V. Ayache N. Deep Reinforcement Learning in Medical Imaging: A Literature Review arXiv 2021 2103.05115 10.1016/j.media.2021.102193 34371440 12. Arulkumaran K. Deisenroth M.P. Brundage M. Bharath A.A. Deep Reinforcement Learning: A Brief Survey IEEE Signal Process. Mag. 2017 34 26 38 10.1109/MSP.2017.2743240 13. Li J. Cheng K. Wang S. Morstatter F. Trevino R.P. Tang J. Liu H. Feature selection: A data perspective ACM Comput. Surv. (CSUR) 2017 50 1 45 10.1145/3136625 14. Yang Z. Zhou R. Qu H. Liu L. Wu Q. Prediction of breast cancer using metaheuristic-driven ensemble learning: A novel classification approach Math. Comput. Simul. 2025 236 29 51 10.1016/j.matcom.2025.03.025 15. Menze B.H. Jakab A. Bauer S. Kalpathy-Cramer J. Farahani K. Kirby J. Burren Y. Porz N. Slotboom J. Wiest R. The multimodal brain tumor image segmentation benchmark (BRATS) IEEE Trans. Med. Imaging 2014 34 1993 2024 10.1109/TMI.2014.2377694 25494501 PMC4833122 16. Bakas S. Akbari H. Sotiras A. Bilello M. Rozycki M. Kirby J.S. Freymann J.B. Farahani K. Davatzikos C. Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features Sci. Data 2017 4 170117 10.1038/sdata.2017.117 28872634 PMC5685212 17. Heidari A.A. Mirjalili S. Faris H. Aljarah I. Mafarja M. Chen H. Harris Hawks Optimization: Algorithm and Applications Future Gener. Comput. Syst. 2019 97 849 872 10.1016/j.future.2019.02.028 18. Mostafa R.R. Gaheen M.A. Abd ElAziz M. Al-Betar M.A. Ewees A.A. An Improved Gorilla Troops Optimizer for Global Optimization Problems and Feature Selection Knowl.-Based Syst. 2023 269 110462 10.1016/j.knosys.2023.110462 19. Trojovská E. Dehghani M. Trojovský P. Zebra Optimization Algorithm: A New Bio-Inspired Optimization Algorithm for Solving Optimization Problems IEEE Access 2022 10 49445 49473 10.1109/ACCESS.2022.3172789 20. Lundberg S.M. Lee S.-I. A Unified Approach to Interpreting Model Predictions Adv. Neural Inf. Process. Syst. 2017 30 10.48550/arXiv.1705.07874 21. Wang Z. Schaul T. Hessel M. van Hasselt H. Lanctot M. de Freitas N. Dueling Network Architectures for Deep Reinforcement Learning arXiv 2016 10.48550/arXiv.1511.06581 1511.06581 22. Mnih V. Kavukcuoglu K. Silver D. Rusu A.A. Veness J. Bellemare M.G. Graves A. Riedmiller M. Fidjeland A.K. Ostrovski G. Human-Level Control through Deep Reinforcement Learning Nature 2015 518 529 533 10.1038/nature14236 25719670 23. Renugadevi M. Narasimhan K. Ravikumar C.V. Anbazhagan R. Pau G. Ramkumar K. Abbas M. Raju N. Sathish K. Sevugan P. Machine Learning Empowered Brain Tumor Segmentation and Grading Model for Lifetime Prediction IEEE Access 2023 11 120868 120880 10.1109/ACCESS.2023.3326841 24. Cho H. Lee S. Kim J. Park H. Classification of the Glioma Grading Using Radiomics Analysis PeerJ 2018 6 e5982 10.7717/peerj.5982 30498643 PMC6252243 25. Kumar R. Gupta A. Arora H.S. Pandian G.N. Raman B. CGHF: A Computational Decision Support System for Glioma Classification Using Hybrid Radiomics- and Stationary Wavelet-Based Features IEEE Access 2020 8 79440 79458 10.1109/ACCESS.2020.2989193 26. Varghese N.E. John A. Amma U.D.C. Classification of Glioma by Exploring Wavelet-Based Radiomic Features and Machine Learning Techniques Using BRATS Dataset Proceedings of the 2023 Third International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT) Bhilai, India 5–6 January 2023 1 7 27. Uvaneshwari M. Baskar M. Computer-Aided Diagnosis Model Using Machine Learning for Brain Tumor Detection and Classification Comput. Syst. Sci. Eng. 2023 46 1811 10.32604/csse.2023.035455 28. Khan A.R. Khan S. Harouni M. Abbasi R. Iqbal S. Mehmood Z. Brain Tumor Segmentation Using K-Means Clustering and Deep Learning with Synthetic Data Augmentation for Classification Microsc. Res. Tech. 2021 84 1389 1399 10.1002/jemt.23694 33524220 29. Rehman A. Khan M.A. Saba T. Mehmood Z. Tariq U. Ayesha N. Microscopic Brain Tumor Detection and Classification Using 3D CNN and Feature Selection Architecture Microsc. Res. Tech. 2021 84 133 149 10.1002/jemt.23597 32959422 30. Ferdous G.J. Sathi K.A. Hossain M.A. Hoque M.M. Dewan M.A.A. LCDEiT: A Linear Complexity Data-Efficient Image Transformer for MRI Brain Tumor Classification IEEE Access 2023 11 20337 20350 10.1109/ACCESS.2023.3244228 31. Montaha S. Azam S. Rafid A.K.M.R.H. Hasan M. Karim A. Islam A. TimeDistributed-CNN-LSTM: A Hybrid Approach Combining CNN and LSTM to Classify Brain Tumor on 3D MRI Scans Performing Ablation Study IEEE Access 2022 10 60039 60059 10.1109/ACCESS.2022.3179577 32. Guo X. Yang C. Lam P.L. Woo P.Y.M. Yuan Y. Domain Knowledge Based Brain Tumor Segmentation and Overall Survival Prediction Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries Springer Cham, Switzerland 2020 285 295 33. Cepeda S. Perez-Nunez A. Garcia-Garcia S. Garicia-Perez D. Arrese I. Jimenez-Roldan L. Garcia-Galindo M. Gonzalez P. Velasco-Casares M. Zamora T. Predicting Short-Term Survival after Gross Total or Near Total Resection in Glioblastomas by Machine Learning-Based Radiomic Analysis of Preoperative MRI Cancers 2021 13 5047 10.3390/cancers13205047 34680199 PMC8533879 34. Pei L. Vidyaratne L. Rahman M.M. Iftekharuddin K.M. Context Aware Deep Learning for Brain Tumor Segmentation, Subtype Classification, and Survival Prediction Using Radiology Images Sci. Rep. 2020 10 19726 10.1038/s41598-020-74419-9 33184301 PMC7665039 35. Osman A.F.I. A Multi-Parametric MRI-Based Radiomics Signature and a Practical ML Model for Stratifying Glioblastoma Patients Based on Survival toward Precision Oncology Front. Comput. Neurosci. 2019 13 58 10.3389/fncom.2019.00058 31507398 PMC6718726 36. Soltani M. Bonakdar A. Shakourifar N. Babaei R. Raahemifar K. Efficacy of Location-Based Features for Survival Prediction of Patients with Glioblastoma Depending on Resection Status Front. Oncol. 2021 11 661123 10.3389/fonc.2021.661123 34295809 PMC8290179 37. Sanghani P. Ang B.T. King N.K.K. Ren H. Regression Based Overall Survival Prediction of Glioblastoma Multiforme Patients Using a Single Discovery Cohort of Multi-Institutional Multi-Channel MR Images Med. Biol. Eng. Comput. 2019 57 1683 1691 10.1007/s11517-019-01986-z 31104273 38. Sun L. Zhang S. Chen H. Luo L. Brain Tumor Segmentation and Survival Prediction Using Multimodal MRI Scans with Deep Learning Front. Neurosci. 2019 13 810 10.3389/fnins.2019.00810 31474816 PMC6707136 39. Shboul Z.A. Alam M. Vidyaratne L. Pei L. Elbakary M.I. Iftekharuddin K.M. Feature-Guided Deep Radiomics for Glioblastoma Patient Survival Prediction Front. Neurosci. 2019 13 966 10.3389/fnins.2019.00966 31619949 PMC6763591 Figure 1 The overall process of the GlioSurvQNet for the classification and survival prediction using DuelContextAttn DQN with Metaheuristic Feature Selection. Figure 2 Number of features selected from the HHO, mGTO, ZOA, Ensemble Metaheuristic Optimization Algorithm, and SHAP for each modality used in Classification. Figure 3 SHAP plot showing the selected features of T1 for the classification. Figure 4 SHAP waterfall plot for T1 Features for LGG vs HGG classification. Figure 5 Architecture of the DuelContextAttn DQN model for Classification and Survival Prediction. Figure 6 Histogram view for the survival class category short (0–250 days), medium (251–500 days), and long (501–1800 days). Figure 7 Number of features selected by HHO, mGTO, ZOA, Ensemble Metaheuristic Optimization Algorithm and SHAP for each modality used in survival prediction. Figure 8 SHAP plot highlighting the contribution of T1-CE selected features to survival prediction. Figure 9 SHAP waterfall plot for T1CE (Survival prediction). Figure 10 Training graph showing the progress of rewards and accuracy per episode for the DuelContextAttn DQN model in LGG/HGG classification using Flair + T1CE modality. Figure 11 Performance since they have already been explained in the caption. Please confirm this revision. evaluation plots of the F + T1CE fusion model: ( a b c d Figure 12 Confusion matrices per fold and modality for binary tumor grading classification. Each row corresponds to a modality combination (T1, F + T1CE, F + T1 + T1CE, F + T1 + T2 + T1CE), and each column corresponds to a cross-validation fold. Figure 13 Training graph showing the progress of rewards and accuracy per episode for the DuelContextAttn DQN model in survival prediction using Flair + T1 + T1CE modality. Figure 14 Confusion matrices for survival classification with best-performing modality combinations. The top row shows training results and testing results for T1CE, F + T2 and the bottom row shows for F + T1 + T1CE, and F + T1 + T2 + T1CE. Figure 15 Convergence curves of HHO, ZOA, and mGTO compared with GA, PSO, and ABC, showing faster convergence and better fitness. diagnostics-15-02304-t001_Table 1 Table 1 Number of radiomics features extracted from each category. Feature Category Count Shape-Based Features 14 First-Order Statistics 18 GLCM (Gray Level Co-occurrence Matrix) 24 GLDM (Gray Level Dependence Matrix) 13 GLRLM (Gray Level Run Length Matrix) 16 GLSZM (Gray Level Size Zone Matrix) 15 NGTDM (Neighboring Gray Tone Difference Matrix) 5 Total 105 diagnostics-15-02304-t002_Table 2 Table 2 Hyperparameters and Objective Function Settings for Metaheuristic Feature Selection. Parameter Value Population Size 20 Number of Iterations 50 Optimization Type Minimization Search Space Binary { 0 , 1 } Fitness Function 1 − accuracy_score Classifier SVM Cross-validation Stratified k Runs per Algorithm 10 (average accuracy reported) Random Seeds Different seeds for each run diagnostics-15-02304-t003_Table 3 Table 3 Training parameters of the DuelContextAttn DQN model. Parameter Value State size Number of patients cases (one state per patient sample) Action size 2 (Classification), 3 (Survival Prediction) Gamma (Discount Factor) 0.99 Epsilon (Initial Exploration Rate) 0.5 Epsilon_min 0.01 Epsilon_decay 0.995 Episodes 50 Batch size 16 Optimizer Adam Learning rate 0.001 Loss function Mean Square Error diagnostics-15-02304-t004_Table 4 Table 4 Performance comparison with different hyperparameter settings. Gamma Epsilon Epsilon min Epsilon decay Avg. Time (s) Accuracy 0.99 0.5 0.0001 0.0001 1392.05 0.58 0.95 1.0 0.01 0.995 1260.33 0.92 0.90 0.7 0.05 0.01 684.37 0.94 0.85 0.3 0.1 0.003 732.56 0.94 0.99 1.0 0.01 0.995 654.74 0.98 0.99 0.5 0.01 0.995 617.26 0.99 diagnostics-15-02304-t005_Table 5 Table 5 Classification Performance of DualContextAttn DQN with 5-Fold Cross-Validation. Modality Precision Recall F1-Score Accuracy (%)  Single-Modality Flair (F) 0.99 ± 0.00 (0.99–0.99) 0.95 ± 0.01 (0.94–0.96) 0.97 ± 0.00 (0.97–0.97) 98.76 ± 0.10 (98.68–98.84) T1 0.97 ± 0.00 (0.97–0.97) 0.96 ± 0.01 (0.95–0.97) 0.94 ± 0.01 (0.93–0.95)  99.02 ± 0.08 (98.95–99.09) T2 0.97 ± 0.00 (0.97–0.97) 0.95 ± 0.01 (0.94–0.96) 0.94 ± 0.01 (0.93–0.95) 98.94 ± 0.10 (98.86–99.02) T1CE 0.96 ± 0.01 (0.95–0.97) 0.94 ± 0.01 (0.93–0.95) 0.95 ± 0.00 (0.95–0.95) 98.50 ± 0.08 (98.42–98.58)  Dual–Modality F + T1 0.99 ± 0.00 (0.99–0.99) 0.94 ± 0.01 (0.93–0.95) 0.96 ± 0.01 (0.95–0.97) 98.53 ± 0.04 (98.49–98.57) F + T2 0.98 ± 0.00 (0.98–0.98) 0.92 ± 0.01 (0.91–0.93) 0.95 ± 0.01 (0.94–0.96) 99.12 ± 0.08 (99.04–99.20) F + T1CE 0.99 ± 0.00 (0.99–0.99) 0.95 ± 0.01 (0.94–0.96) 0.96 ± 0.01 (0.95–0.97)  99.27 ± 0.05 (99.22–99.32) T1 + T2 0.98 ± 0.00 (0.98–0.98) 0.97 ± 0.01 (0.96–0.98) 0.97 ± 0.00 (0.97–0.97) 94.33 ± 0.08 (94.26–94.40) T1 + T1CE 0.96 ± 0.01 (0.95–0.97) 0.96 ± 0.01 (0.96–0.97) 0.96 ± 0.00 (0.96–0.96) 97.69 ± 0.05 (97.64–97.74) T2 + T1CE 0.99 ± 0.00 (0.99–0.99) 0.96 ± 0.01 (0.95–0.97) 0.97 ± 0.00 (0.97–0.97) 98.95 ± 0.05 (98.90–99.00)  Triple–Modality F + T1 + T2 0.97 ± 0.01 (0.97–0.99) 0.93 ± 0.01 (0.92–0.94) 0.95 ± 0.00 (0.95–0.95) 98.53 ± 0.04 (98.49–98.57) F + T1 + T1CE 0.99 ± 0.00 (0.99–0.99) 0.96 ± 0.00 (0.96–0.96) 0.97 ± 0.00 (0.97–0.97)  98.95 ± 0.03 (98.92–98.98) F + T2 + T1CE 0.98 ± 0.00 (0.98–0.98) 0.94 ± 0.01 (0.92–0.95) 0.95 ± 0.01 (0.94–0.96) 98.74 ± 0.05 (98.69–98.79) T1 + T2 + T1CE 0.98 ± 0.00 (0.98–0.98) 0.91 ± 0.01 (0.90–0.92) 0.94 ± 0.01 (0.93–0.95) 97.69 ± 0.05 (97.64–97.74)  All Four Modalities F + T1 + T2 + T1CE 0.99 ± 0.00 (0.99–0.99) 0.91 ± 0.01 (0.90–0.93) 0.95 ± 0.00 (0.95–0.95) 98.32 ± 0.05 (98.27–98.37) Bold highlights the highest results. diagnostics-15-02304-t006_Table 6 Table 6 Performance comparison across different models. Model Precision Recall F1-Score Accuracy (%) Logistic Regression 0.92 0.88 0.91 90.55 Linear SVM 0.95 0.93 0.94 94.12 XGBoost 0.96 0.95 0.95 95.31 LightGBM 0.97 0.94 0.95 96.18 Random Forest 0.95 0.92 0.93 94.88 Gradient Boosting 0.96 0.93 0.94 95.07 MLP 0.97 0.96 0.94 95.51 Proposed Method 0.99 0.94 0.96 99.27 diagnostics-15-02304-t007_Table 7 Table 7 Comparative accuracy of existing models and the proposed method. Method Model Accuracy (%) Cho et al. [ 24 Random Forest 88.70 Kumar et al. [ 25 Random Forest 97.48 Varghese et al. [ 26 SVM 97.00 Uvaneshwari et al. [ 27 XGBoost 97.83 Khan et al. [ 28 VGG 94.06 Rehman et al. [ 29 CNN 98.32 Ferdous et al. [ 30 LCDEIT 93.69 Montaha et al. [ 31 TD-CNN-LSTM 98.90 Stember et al. [ 7 DQL-TD 100.00 (200 episodes)  Proposed Method  DuelContextAttn DQN  99.27 (50 episodes) diagnostics-15-02304-t008_Table 8 Table 8 Performance of the DuelContextAttn DQN model for the Overall Survival Prediction. Modality Precision Recall F1-Score AUC Accuracy (%)  Single-Modality Flair 0.96 0.94 0.95 0.95 91.08 T1 0.91 0.95 0.92 0.94 93.03 T2 0.93 0.91 0.90 0.92 92.19 T1CE 0.92 0.94 0.95 0.96  93.28  Dual-Modality Flair + T1 0.95 0.91 0.93 0.94 91.92 Flair + T2 0.96 0.94 0.93 0.96  93.71 Flair + T1CE 0.93 0.91 0.90 0.94 92.76 T1 + T2 0.92 0.93 0.94 0.94 92.96 T1 + T1CE 0.90 0.91 0.91 0.91 90.76 T2 + T1CE 0.92 0.94 0.91 0.94 91.60  Triple-Modality Flair + T1 + T2 0.92 0.93 0.92 0.95 91.24 Flair + T1 + T1CE 0.96 0.91 0.94 0.95  93.82 Flair + T2 + T1CE 0.95 0.92 0.93 0.94 92.08 T1 + T2 + T1CE 0.94 0.92 0.93 0.94 91.71  All Four Modalities Flair + T1 + T2 + T1CE 0.93 0.94 0.95 0.95 93.39 Bold highlights the highest results. diagnostics-15-02304-t010_Table 10 Table 10 Ablation study comparing different DQN variants on the F + T1CE modality. Model Precision Recall F1-Score Accuracy (%) DQN 0.95 0.96 0.95 95.10 Double DQN 0.97 0.98 0.96 97.32 Dueling DQN 0.97 0.96 0.98 97.76 Dueling Double DQN 0.98 0.96 0.98 97.95 Dueling Double DQN-A 0.98 0.96 0.98 98.68  DuelContextAttn DQN (Proposed)  0.99  0.97  0.98  99.27 ",
  "metadata": {
    "Title of this paper": "Feature-Guided Deep Radiomics for Glioblastoma Patient Survival Prediction",
    "Journal it was published in:": "Diagnostics",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12468973/"
  }
}
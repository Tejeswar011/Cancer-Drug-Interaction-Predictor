{
  "title": "Paper_1089",
  "abstract": "pmc J Clin Med J Clin Med 2745 jclinmed jcm Journal of Clinical Medicine 2077-0383 Multidisciplinary Digital Publishing Institute  (MDPI) PMC12470819 PMC12470819.1 12470819 12470819 41010663 10.3390/jcm14186460 jcm-14-06460 1 Article Evaluating the Concordance Between ChatGPT and Multidisciplinary Teams in Breast Cancer Treatment Planning: A Study from Bosnia and Herzegovina Umihanic Sefika Conceptualization Methodology Resources Supervision 1 https://orcid.org/0000-0003-2456-0566 Osmanovic Hedim Methodology Software Formal analysis Visualization 2 https://orcid.org/0000-0003-0262-4194 Selak Nejra Writing – review & editing 3 * Kopric Dijana Investigation Data curation 1 Huseinbasic Asija Investigation Data curation 1 Sehic-Kozica Erna Validation 4 https://orcid.org/0009-0001-6370-9645 Babic Belma Investigation Data curation 1 Umihanic Fadil Investigation Data curation Writing – original draft 5 Giordano Salvatore Academic Editor 1 umihanics@gmail.com dijana.kopric@ukctuzla.ba asija.huseinbasic@gmail.com belma996@gmail.com 2 hedim.osmanovic@untz.ba 3 4 e.sehickozica@gmail.com 5 fadil.umihanic@ogr.iuc.edu.tr * selaknejra@gmail.com 13 9 2025 9 2025 14 18 497642 6460 07 8 2025 04 9 2025 11 9 2025 13 09 2025 27 09 2025 29 09 2025 © 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ Background/Objectives Methods Results p Conclusions artificial intelligence ChatGPT breast cancer oncology multidisciplinary team low- and middle-income countries clinical decision support This research received no external funding. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Breast cancer is the most common malignancy among women and the second leading cause of cancer-related death, following lung cancer [ 1 2 3 Delays in cancer diagnosis and treatment are major contributors to high mortality in low- and middle-income countries (LMICs), largely due to critical shortages of specialized professionals, including pathologists and oncologists, as well as limited diagnostic infrastructure [ 4 5 In light of these challenges, there is growing interest in scalable, technology-based solutions that can help in clinical decision-making and help alleviate pressure on overburdened healthcare systems. Artificial intelligence (AI), particularly large language models (LLMs), is increasingly being explored for its potential to support cancer care through advanced data processing, contextual understanding, and decision-making assistance. LLMs like ChatGPT, Claude2, and BioMedLM have demonstrated promising capabilities across various tasks [ 6 7 Especially regarding the limitations of LMICs, AI-based platforms offer promising solutions by providing accessible, evidence-based treatment recommendations and supporting clinicians, especially where oncology expertise is scarce [ 8 This retrospective study evaluates treatment recommendations generated by ChatGPT-4.0 and compares them with multidisciplinary team (MDT) decisions for newly diagnosed breast cancer patients in Bosnia and Herzegovina. Four board-certified oncologists, including both MDT members and external reviewers, independently assessed the concordance of ChatGPT’s suggestions using a structured rating approach. The primary aim was to quantify the agreement between AI-generated outputs and expert-led MDT treatment plans in a real-world low- and middle-income country (LMIC) setting. 2. Materials and Methods This retrospective study was conducted at the Clinic for Oncology and Radiotherapy, University Clinical Center Tuzla, and approved by the institutional Ethics Committee (Approval No. 02-09/2-116-3/25). The study included patients with newly diagnosed breast cancer, presented for the first time to the MDT specialized in breast cancer care, between 1 January and 31 December 2023. Inclusion criteria were patients with a new diagnosis of breast cancer who had not received any prior treatment and were presented to the MDT for initial management planning. Exclusion criteria included (1) incomplete clinical data; (2) multiple synchronous primary cancers; (3) local recurrence; (4) age younger than 18 or older than 89 years; and (5) participation in any clinical trial. Demographic and clinicopathological variables were extracted from patient records, including sex, age, menopausal status, estrogen receptor (ER) and progesterone receptor (PR) status, HER2 status, tumor grade (in situ, G1 G2 and G3), and Ki-67 proliferation index. For each case, treatment recommendations from the MDT were documented. Potential modalities included chemotherapy, hormone therapy, radiotherapy, anti-HER2 therapy, surgery, or multimodal approaches. ChatGPT (version 4.0, OpenAI, San Francisco, CA, USA) was independently queried with the corresponding patient data to generate its treatment recommendations. An example of the ChatGPT prompt is as follows: “A 45-year-old female with invasive ductal breast carcinoma, cT2b cN0, 80% estrogen receptor expression, 70% progesterone receptor expression, HER2 3+, Ki-67 of 25%, and grade 2 differentiation, without comorbidities. Proposed treatment?” To prevent bias from prior responses, each ChatGPT query was submitted in a fresh chat session after clearing the previous chat history. Four board-certified oncologists from three reference centers independently evaluated the treatment plans generated by both the MDT and ChatGPT. Two oncologists were members of the MDT, while the other two were from an external institution to reduce institutional bias. Each ChatGPT-generated recommendation was analyzed in comparison with the MDT proposal. We correlated each individual diagnostic parameter with both ChatGPT and MDT recommendations to ensure that all clinically relevant data were considered in treatment suggestions. Subsequently, we analyzed the overall agreement between the ChatGPT and MDT treatment proposals. Inter-oncologist agreement was also assessed. Each expert assessed ChatGPT’s recommendation in comparison with the MDT’s plan using a 4-point Likert scale (1: Strongly disagree; 2: Disagree; 3: Agree; 4: Strongly agree). Each oncologist’s ratings were scored individually, and total scores were calculated to reflect agreement levels. Agreement between ChatGPT-generated treatment recommendations and MDT decisions was evaluated using descriptive statistics, Cronbach’s alpha, and Fleiss’ kappa coefficient. All statistical analyses were performed using R software (version 4.5.1; R Foundation for Statistical Computing, Austria). A p 3. Results A total of 91 newly diagnosed breast cancer patients were included, with a median age of 60 years (IQR: 50–70). All patients were female. The majority were postmenopausal (84%) and had invasive ductal carcinoma (83.5%). Most tumors were hormone receptor-positive (ER+: 76.9%, PR+: 69.2%) and HER2-negative (85.7%), while 14.3% were HER2-positive. 40.7% patients had no comorbidity. Among those with comorbidities, the most common was arterial hypertension ( n n n n Table 1 The overall agreement between ChatGPT-generated treatment suggestions and the multidisciplinary team (MDT) recommendations was high, with a mean rating score of 3.31 (SD = 0.10) across all patients ( Figure 1 p Agreement varied by clinical subgroup. Higher agreement was observed in patients with ER-/PR- tumors and those receiving standardized neoadjuvant chemotherapy regimens, such as TCHP or AC-T + Carboplatin. Conversely, lower agreement was noted in subgroups requiring more individualized judgment, including grade 1 tumors and cases with uncertain indications for surgery or endocrine therapy. Detailed subgroup results are summarized in Table 2 4. Discussion In this study of 91 newly diagnosed breast cancer patients, we observed a high overall concordance between AI-generated treatment recommendations and those of a multidisciplinary oncology team. Agreement scores were consistent across four oncologist raters, ranging from a mean of 3.22 to 3.40, with inter-rater reliability indicated by a high Cronbach’s alpha of 0.86 and moderate Fleiss’ kappa of 0.31 ( p Several groups have recently evaluated ChatGPT in oncology. One of the first studies on this topic was by Sorin et al., who tested ChatGPT-3.5 in 10 breast cancer patients [ 9 Griewing et al. analyzed 20 generic breast cancer scenarios, also using ChatGPT-3.5. [ 10 More recently, Deng et al. evaluated ChatGPT-4 in five simulated sarcoma cases [ 11 12 Other studies have also evaluated AI in oncology beyond GPT. A retrospective analysis comparing Watson for Oncology (WFO) with a multidisciplinary tumor board in gastric cancer reported an overall concordance of 86.9%, with the highest agreement in early-stage disease and the lowest in stage IV [ 13 14 In oncology, AI is already making inroads, particularly in breast cancer, through computer-aided detection in mammography screening [ 15 16 17 18 Although ChatGPT shows promise as a decision support tool, several limitations must be considered when integrating it into clinical practice. First, its outputs are shaped by the data it was trained on, which may carry inherent biases, such as underrepresentation of certain patient populations or diseases, potentially leading to skewed or less accurate recommendations [ 19 As the integration of AI into healthcare accelerates, ethical and regulatory considerations have become increasingly important to ensure its responsible use. The WHO highlights the need for safe, ethical, and effective AI in healthcare, urging collaboration among developers, regulators, clinicians, and patients, with a focus on high-quality data to prevent bias. As of 1 August 2024, the EU’s AI Act enforces the first comprehensive regulation to ensure transparency and oversight for high-risk AI systems in healthcare [ 20 Strengths of this study include its real-world design in a LMIC setting, evaluating ChatGPT’s performance in actual multidisciplinary decision-making workflows. The inclusion of patients with newly diagnosed, treatment-naïve breast cancer enhances clinical relevance. By involving board-certified oncologists from three reference centers, including both MDT members and external reviewers, the study minimized institutional bias and allowed for robust inter-rater agreement analysis. The methodology also reflects practical application scenarios for AI-assisted decision support in resource-constrained environments. Limitations include the single-country setting, which may limit generalizability to other healthcare systems, especially in LMICs. The study relied on structured clinical data; however, additional contextual details (e.g., patient preferences, psychosocial factors, etc.) were not included in the ChatGPT prompts and may influence treatment planning in real clinical scenarios. While ChatGPT recommendations were generated using version 4.0, rapid model updates may affect reproducibility over time. Furthermore, only one AI model was evaluated. Other LLMs, such as Claude or Bard, may perform differently; nevertheless, we focused on ChatGPT due to its broad accessibility, widespread use, and free availability, making it particularly relevant in LMIC settings. Our study provides the first empirical evidence of this pattern in a real-world LMIC setting. Importantly, we quantified the degree of concordance between ChatGPT and MDT decisions, demonstrating very high reliability in straightforward cases. These finding highlights AI’s potential role in supporting oncology care where specialist resources are limited, by ensuring consistency in routine decision-making. At the same time, our results underscore the need for continued human oversight in more complex or borderline situations, where individualized clinical judgment remains irreplaceable. Our study aims to familiarize clinicians with both the potential and limitations of AI as a support tool for MDTs in breast cancer treatment. Improving the explainability of AI models is essential for fostering trust among clinicians and encouraging broader adoption. 5. Conclusions This study demonstrates that large language models like ChatGPT can provide treatment recommendations for newly diagnosed breast cancer patients that are largely concordant with multidisciplinary team decisions, particularly in standardized, chemotherapy-oriented regimens. Conducted in a real-world low- and middle-income countriessetting, our findings highlight the potential role of AI as an adjunct decision-support tool in oncology, especially where specialist resources may be limited. While not a replacement for clinical expertise, ChatGPT may offer value in streamlining care, supporting less experienced providers, and enhancing access to evidence-based guidance. However, its current limitations in accuracy and clinical judgment are significant. Further research and development are essential to optimize AI tools for reliable use in medical decision-making. Acknowledgments The authors acknowledge the use of ChatGPT-4 (OpenAI) for proofreading support. All scientific content and interpretation remain the responsibility of the authors. Disclaimer/Publisher’s Note: Author Contributions Conceptualization, S.U.; methodology, S.U. and H.O.; software, H.O.; validation, E.S.-K.; formal analysis, H.O.; investigation, D.K., A.H., B.B., E.S.-K. and F.U.; resources, S.U.; data curation, D.K., A.H., B.B. and F.U.; writing—original draft preparation, F.U.; writing—review and editing, N.S.; visualization, H.O.; supervision, S.U.; project administration, S.U.; All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement The study was conducted in according to the guidelines of the Declaration of Helsinki, and approved by the Ethics Committeeof University Clinical Center Tuzla (02-09/2-116-3/25; 11 June 2025). Informed Consent Statement Informed consent was obtained from all subjects involved in the study. Data Availability Statement Data are available from the corresponding author upon reasonable request. Conflicts of Interest The authors declare no conflicts of interest. Abbreviations The following abbreviations are used in this manuscript: AI Artificial Intelligence LLM Large Language Model MDT Multidisciplinary Team LMIC Low- and Middle-Income Country ER Estrogen Receptor PR Progesterone Receptor HER2 Human Epidermal Growth Factor Receptor 2 AC-T Doxorubicin (Adriamycin), Cyclophosphamide, followed by Taxane TCHP Docetaxel, Carboplatin, Trastuzumab, and Pertuzumab FISH Fluorescence In Situ Hybridization References 1. Gennari A. André F. Barrios C.H. Cortés J. de Azambuja E. DeMichele A. Dent R. Fenlon D. Gligorov J. Hurvitz S.A. ESMO Clinical Practice Guideline for the diagnosis, staging and treatment of patients with metastatic breast cancer Ann. Oncol. 2021 32 1475 1495 10.1016/j.annonc.2021.09.019 34678411 2. Surakasula A. Nagarjunapu G. Raghavaiah K. A comparative study of pre- and post-menopausal breast cancer: Risk factors, presentation, characteristics and management J. Res. Pharm. Pract. 2014 3 12 10.4103/2279-042X.132704 24991630 PMC4078652 3. Ponce-Chazarri L. Ponce-Blandón J.A. Immordino P. Giordano A. Morales F. Barriers to Breast Cancer-Screening Adherence in Vulnerable Populations Cancers 2023 15 604 10.3390/cancers15030604 36765561 PMC9913751 4. Barragan-Carrillo R. Asirwa F.C. Dienstmann R. Pendhakar D. Ruiz-Garcia E. Global Oncology: Tackling Disparities and Promoting Innovations in Low- and Middle-Income Countries Am. Soc. Clin. Oncol. Educ. Book 2025 45 e473930 10.1200/EDBK-25-473930 40526883 5. Dimitrova M. Lakic D. Petrova G. Bešlija S. Culig J. Comparative analysis of the access to health-care services and breast cancer therapy in 10 Eastern European countries SAGE Open Med. 2020 8 2050312120922029 10.1177/2050312120922029 32547747 PMC7249592 6. Deng L. Wang T. Zhang Z. Tao W. Li J. Zhao Y. Luo S. Xu J. Evaluation of large language models in breast cancer clinical scenarios: A comparative analysis based on ChatGPT-3.5, ChatGPT-4.0, and Claude2 Int. J. Surg. 2024 110 1941 1950 10.1097/JS9.0000000000001066 38668655 PMC11019981 7. Benary M. Wang X.D. Schmidt M. Soll D. Hilfenhaus G. Nassir M. Sigler C. Knödler M. Keller U. Beule D. Leveraging Large Language Models for Decision Support in Personalized Oncology JAMA Netw. Open 2023 6 E2343689 10.1001/jamanetworkopen.2023.43689 37976064 PMC10656647 8. Emani S. Rui A. Rocha H.A.L. Rizvi R.F. Juaçaba S.F. Jackson G.P. Bates D.W. Physicians’ Perceptions of and Satisfaction With Artificial Intelligence in Cancer Treatment: A Clinical Decision Support System Experience and Implications for Low-Middle–Income Countries JMIR Cancer 2022 8 e31461 10.2196/31461 35389353 PMC9030908 9. Sorin V. Klang E. Sklair-Levy M. Cohen I. Zippel D.B. Balint Lahat N. Konen E. Barash Y. Large language model (ChatGPT) as a support tool for breast tumor board NPJ Breast Cancer 2023 9 44 10.1038/s41523-023-00557-8 37253791 PMC10229606 10. Griewing S. Gremke N. Wagner U. Lingenfelder M. Kuhn S. Boekhoff J. Challenging ChatGPT 3.5 in Senology—An Assessment of Concordance with Breast Cancer Tumor Board Decision Making J. Pers. Med. 2023 13 1502 10.3390/jpm13101502 37888113 PMC10608120 11. Ammo T. Guillaume V.G.J. Hofmann U.K. Ulmer N.M. Buenting N. Laenger F. Beier J.P. Leypold T. Evaluating ChatGPT-4o as a decision support tool in multidisciplinary sarcoma tumor boards: Heterogeneous performance across various specialties Front. Oncol. 2025 14 1526288 10.3389/fonc.2024.1526288 39896191 PMC11782276 12. Aghamaliyev U. Karimbayli J. Giessen-Jung C. Ilmer M. Unger K. Andrade D. Hofmann F.O. Weniger M. Angele M.K. Westphalen C.B. ChatGPT’s Gastrointestinal Tumor Board Tango: A limping dance partner? Eur. J. Cancer 2024 205 114100 10.1016/j.ejca.2024.114100 38729055 13. Park Y.-E. Chae H. The Fidelity of Artificial Intelligence to Multidisciplinary Tumor Board Recommendations for Patients with Gastric Cancer: A Retrospective Study J. Gastrointest. Cancer 2024 55 365 372 10.1007/s12029-023-00967-8 37702851 PMC11096204 14. Zhao X. Zhang Y. Ma X. Chen Y. Xi J. Yin X. Kang H. Guan H. Dai Z. Liu D. Concordance between treatment recommendations provided by IBM Watson for Oncology and a multidisciplinary tumor board for breast cancer in China Jpn. J. Clin. Oncol. 2020 50 852 858 10.1093/jjco/hyaa051 32419014 15. Uzun Ozsahin D. Ikechukwu Emegano D. Uzun B. Ozsahin I. The Systematic Review of Artificial Intelligence Applications in Breast Cancer Diagnosis Diagnostics 2022 13 45 10.3390/diagnostics13010045 36611337 PMC9818874 16. Huynh E. Hosny A. Guthier C. Bitterman D.S. Petit S.F. Haas-Kogan D.A. Kann B. Aerts H.J.W.L. Mak R.H. Artificial intelligence in radiation oncology Nat. Rev. Clin. Oncol. 2020 17 771 781 10.1038/s41571-020-0417-8 32843739 17. Kim M.-S. Park H.-Y. Kho B.-G. Park C.-K. Oh I.-J. Kim Y.-C. Kim S. Yun J.-S. Song S.-Y. Na K.-J. Artificial intelligence and lung cancer treatment decision: Agreement with recommendation of multidisciplinary tumor board Transl. Lung Cancer Res. 2020 9 507 514 10.21037/tlcr.2020.04.11 32676314 PMC7354125 18. Thavanesan N. Vigneswaran G. Bodala I. Underwood T.J. The Oesophageal Cancer Multidisciplinary Team: Can Machine Learning Assist Decision-Making? J. Gastrointest. Surg. 2023 27 807 822 10.1007/s11605-022-05575-8 36689150 PMC10073064 19. Ferdush J. Begum M. Hossain S.T. ChatGPT and Clinical Decision Support: Scope, Application, and Limitations Ann. Biomed. Eng. 2024 52 1119 1124 10.1007/s10439-023-03329-4 37516680 20. van Leeuwen K.G. Doorn L. Gelderblom E. The AI Act: Responsibilities and obligations for healthcare professionals and organizations Diagn. Interv. Radiol. 2025 10.4274/dir.2025.252851 40439140 Figure 1 Violin plot of rater scores. Violin plots display the distribution of agreement scores for each of the four raters and their average. Scores ranged from 1 (strongly disagree) to 4 (strongly agree). White dots indicate the mean score for each rater. Most scores clustered around 3 and 4, suggesting a general trend toward high agreement with the ChatGPT recommendations. The high internal consistency among raters is confirmed by a Cronbach’s alpha of 0.86 (95% CI: 0.82–0.90), indicating very good inter-rater reliability. jcm-14-06460-t001_Table 1 Table 1 Clinical and demographic data. Variables  N (%) Age (median (IQR)) (years)  60 (50–70) Marital status Married 71 (78.0)  Widowed 19 (20.9)  Divorced 1 (1.1) Menopausal status Premenopausal 6 (6.6)  Postmenopausal 76 (83.5)  Unknown 9 (9.9) Partus Yes 42 (46.2)  Nno 18 (19.8)  Unknown 31 (34.0) Comorbidities 0 37 (40.7)  1 30 (33.0)  2 13 (14.3)  3+ 11 (12.0) Tumor type Ductal 66 (72.5)  Lobular 11 (12.1)  Other 14 (15.4) Histological grading (NG) in situ 1 (1.1)  1 3 (3.3)  2 61 (67.0)  3 23 (25.3)  Unknown 3 (3.3) ER receptor Positive 70 (76.9)  Negative 21 (23.1) PR receptor Positive 63 (69.2)  Negative 28 (30.8) Her2 receptor Positive 13 (14.3)  Negative 78 (85.7) Ki67 ≤20 52 (57.1)  >20 39 (42.9) jcm-14-06460-t002_Table 2 Table 2 Subgroup analysis of agreement between ChatGPT-generated and MDT treatment recommendations. Results are presented as mean Likert scores (1–4), standard deviation (SD), and proportion of lower (≤2) vs. higher (>2) ratings across clinically relevant variables. Category Group Number of Patients Mean Score SD Rate ≤ 2 (%) Rate > 2 (%)  Partus Yes 42 3.45 0.18 3.0 97.0  No 18 3.49 0.27 4.2 95.8  Unknown 31 2.94 0.12 32.3 67.7  Tumor type Ductal 66 3.22 0.06 15.5 84.5  Lobular 11 3.52 0.18 4.5 95.5  Other 14 3.38 0.20 8.9 91.1  Histological grading In situ 1 2.75 0.33 25 75  1 3 2.84 0.12 33.3 66.7  2 61 3.26 0.09 12.7 87.3  3 23 3.45 0.15 8.7 91.3  Histological grading Unknown 3 3.00 0.19 33.3 66.7  ER receptor Positive 70 3.22 0.05 13.1 86.9  Negative 21 3.52 0.18 2.5 97.5  PR receptor Positive 63 3.14 0.04 16.7 83.3  Negative 28 3.59 0.22 5.6 94.4  HER2 receptor Positive 13 3.5 0.32 9.6 90.4  Negative 78 3.24 0.05 13.8 86.2  Ki-67        ≤20 52 3.29 0.07 12.5 87.5  >20 39 3.29 0.12 14.1 85.9  Neoadjuvant AC-T + carboplatin Yes 2 3.68 0.48 3.6 96.4  No 55 3.22 0.13 15.9 84.1  Indecisive 44 3.20 0.06 14.2 85.8  Neoadjuvant AC-T + platinum Yes 2 3.12 0.48 12.5 87.5  No 46 3.32 0.12 13.0 87.0  Indecisive 43 3.25 0.04 12.2 87.8  Surgery Yes 76 3.32 0.11 12.5 87.5  No 7 3.36 0.14 7.1 92.9  Indecisive 8 2.91 0.24 25 75  Radiotherapy Yes 47 3.38 0.13 10.6 89.4  No 6 3.58 0.29 0 100  Indecisive 38 3.12 0.04 18.4 81.6  Endocrine therapy Yes 63 3.36 0.09 7.5 92.5  No 23 3.18 0.11 21.7 78.3  Indecisive 5 2.75 0.25 45 55  Palliative care Yes 6 3.42 0.29 4.2 95.8  No 85 3.28 0.07 13.8 86.2  FISH Yes 4 3.38 0.75 18.9 81.1  No 87 3.28 0.07 12.9 87.1  Neoadjuvant TCHP Yes 2 3.88 0.25 0 100  No 50 3.27 0.12 15 85  Indecisive 39 3.26 0.05 11.5 88.5 ",
  "metadata": {
    "Title of this paper": "The AI Act: Responsibilities and obligations for healthcare professionals and organizations",
    "Journal it was published in:": "Journal of Clinical Medicine",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12470819/"
  }
}
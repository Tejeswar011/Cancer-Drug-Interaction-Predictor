{
  "title": "Paper_1104",
  "abstract": "pmc Indian J Community Med Indian J Community Med 962 ijcommed IJCM Indian J Community Med Indian Journal of Community Medicine: Official Publication of Indian Association of Preventive & Social Medicine 0970-0218 1998-3581 Wolters Kluwer -- Medknow Publications PMC12470336 PMC12470336.1 12470336 12470336 41017898 10.4103/ijcm.ijcm_567_24 IJCM-50-739 1 Review Article Assessing the Adequacy of a Prediction Model Indrayan Abhaya Mishra Sakshi Department of Clinical Research, Max Healthcare, New Delhi, India Address for correspondence: a.indrayan@gmail.com Sep-Oct 2025 31 3 2025 50 5 497648 739 744 11 8 2024 30 12 2024 01 09 2025 27 09 2025 29 09 2025 Copyright: © 2025 Indian Journal of Community Medicine 2025 https://creativecommons.org/licenses/by-nc-sa/4.0/ This is an open access journal, and articles are distributed under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 License, which allows others to remix, tweak, and build upon the work non-commercially, as long as appropriate credit is given and the new creations are licensed under the identical terms. Many models claim to predict outcomes with good accuracy. However, not many seem to be adopted in practice. This could be because most of them do not have sufficient predictive accuracy. We analyzed 20 recently published papers on prediction models and found that most use inadequate measures to assess predictive performance. These measures primarily include the area under the ROC curve (C-index) that measures discrimination and not predictivity, that too accepting a relatively low value, and using aggregate concordance for assessing predictive accuracy instead of individual-based agreement between the observed and predicted values. Some use arbitrary scores in their models, consider only binary outcomes where multiple categories could be more useful, misinterpret P C-index model adequacy prediction model predictivity ROC Nil. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes I NTRODUCTION Prediction models seem to be catching the imagination of medical professionals as more than a thousand papers are indexed in PubMed for 2023 alone with the title containing the term “prediction model”. This huge number in just one year suggests that prediction models are perceived as helpful in medical practice. This probably is not so with most prediction models appearing in the literature. A model is a simplified version of a complex process and uses a limited number of antecedent factors to predict an outcome. Prediction of the future is always uncertain but model-based predictions are more prone to error since all components of the process are not included to begin with for parsimony, and some may be unknown. The key is to minimize this error and keep it within clinical tolerance. Not being able to do so may be the reason that not many models could find wide applications. For example, the model for prediction of osteoarthritis[ 1 We selected the most recent (as of October 2023) 20 papers[ 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Supplementary Table Supplementary Table Detailed features of the 20 prediction model articles under review (as reported in the articles) Authors The model used to build the predictive model The area under the ROC curve or C-index Target outcome for prediction Sensitivity/specificity Internal/External Validation Ratio of training and validation set PPV & NPV Khan et al 2 Multinomial logistic regression Without ICU admission: 0.858; ICU survival Not available Internal 6:4 PPV for ICU survivors: Chen et al [ 3] Cox proportional hazard regression For one-year incidence: Training set 0.825 and Validation set 0.857; Lung cancer Not available Internal 7:3 Not available Yuan et al 4 Multivariable logistic regression analysis and nomogram Training cohort: 0.829; validation cohort: 0.825 SLN metastasis Not available Internal 3:1 Not available Chen et al 5 Xboost and logistic regression analysis Following 3 days: 0.675, Next day: 0.8 Mortality Sensitivity >0.60 in the validation set Internal 7:3 Not available Li et al 6 Logistic regression and nomogram Training set: 0.675, Validation set: 0.769 Premenstrual syndrome Training set: Sensitivity=70.0%, Specificity=69.8% Validation set: Sensitivity=71.8%, Specificity=69.1% Internal 7:3 Not available Zhang et al 7 Cox proportional hazards regression Not available Recurrence of neuralgia Not available None NA Not available Zhong et al 8 Logistic regression analysis and nomogram Training group: 0.735 Validation groups: 0.706 Nonresponse Not available Internal 7:3 Not available Matsumoto et al 9 Multivariate logistic regression Model: 0.812 Nonoperative management failure Sensitivity: 25.5% Internal Bootstrap PPV: 50.0% Koozi et al 10 Multivariate logistic regression Training set: 0.72; Validation set: 0.71 Mortality Not available Internal Cross-validation with 50 repeats Not available Wang et al 11 Logistic regression analysis and a nomogram Training set: 0.880; Validation set: 0.831 MDROs infection Training set: Sensitivity=89.2%, Specificity=73.4% Validation set: Sensitivity=74.7%, Specificity=73.2% Internal 7:3 Not available Chen et al 12 Decision curve analysis Model: 0.82 Prostate cancer Sensitivity: 79.57% Internal 7:3 Not available Fu et al 13 Multivariable logistic regression and nomogram Model: 0.917; Type D dissection Sensitivity: 78.7%; Specificity: 20.7% Internal 7:3 Not available Huqh et al 14 Neural network and ordinal logistic regression Not available Maxillary growth Not available Internal 7:3 Not available Xie et al 15 Machine learning algorithm Not available Peak dilation angle Not available Internal Not available Not available Gao et al 16 Nomogram and multivariate logistic regression Training set: 0.771 and Validation set: 0.711 Metastasis Not available Internal 64:47 Not available Zhao et al 17 Random forest model and decision tree Machine learning algorithms: Metastasis Not available Internal & external (both) 536:487 Not available Geng et al 18 Nomogram and multivariate logistic analyses Training set: 0.91 Functional outcome Training set: Sensitivity 0.852 Specificity 0.800; Validation set: Sensitivity 0.923 Specificity 0.812 External 413:74 Training set: the PPV and NPV 0.852, and 0.852 respectively. In the validation set: the PPV and NPV 0.727 and 0.951 respectively Matsui et al 19 Bayesian classifier (leave-one-out method) Not available Fistula (CR-POPF) Training set: External 180:336 Training set: Chen et al 20 Machine learning algorithm Model: 0.89 Transfusion requirement Sensitivity and specificity between 70% and 80% Internal 8:2 Not available Yuan et al 21 Multivariable regression model, and nomogram Model: 0.86 Mortality Not available Internal Bootstrap Not available R EASONS FOR THE NADEQUACY OF OST REDICTION ODELS The reasons for the inadequacy of most prediction models and the remedying suggestions are as follows. Inverse assessment An overwhelming 17 of the 20 papers we reviewed were predicting binary outcomes and 16 of them used area under the ROC curve (AUROC), also called the C-index, to assess the performance of their model. Perhaps this index is considered the gold standard for this purpose. But Cook[ 22 The area under a precision-recall curve, now many times used for machine-learning processes, where precision is equivalent to positive predictive value (PPV) and recall is equivalent to sensitivity, has been shown better than AUROC for assessing predictivity for low prevalent diseases.[ 23 24 et al 25 Although sensitivity and specificity are also affected by the prevalence of the condition,[ 26 et al 9 C C C 2 9 18 19 Four of the 16 papers that used the ROC curve reported a Youden-based ‘best’ cut-off. This cut-off maximizes (sensitivity + specificity) and not predictivity. The cut-off for best predictive accuracy should be based on P 24 Inadequate value of C-index Although discrimination and prediction may be very different depending on the prevalence, the predictive accuracy is high when the discrimination is high. But, even for discrimination, an inadequate value of the C C 22 27 C C 28 The papers generally report 95% confidence intervals (CIs) of the C C A high value of the C et al 29 In the case of the prediction of a quantitative outcome, a value exceeding 0.80 of the coefficient of determination (η 2 30 2 Using aggregate indices and not individual-based indices Prediction models would work well in clinical practice when they correctly predict the event in individuals. Many models use aggregate indices for groups to assess adequacy. This may be adequate for categorical outcomes but not for quantitative outcomes. For example, PPV and NPV are appropriate for binary outcomes and Cohen κ can be used for assessing agreement between the observed and model-predicted polytomous outcomes. Although a value of κ between 0.61 and 0.80 has been considered good for agreement,[ 31 For quantitative outcomes, such as the duration of survival, the extent of one-to-one agreement 32 For assessing the predictivity of models, Steyerberg et al 33 – y = x 34 et al 33 50 90 35 Piovani et al 36 Harrell’s C-index is recommended for quantitative predictions, particularly for survival durations.[ 37 Regression models, used by Zhang et al 7 individual values et al 15 et al 20 32 et al 38 2peak The objective of any model is to explain the greatest amount of variation by using the fewest predictors. For this purpose, the Akaike Information Criterion (AIC) and its variation Bayesian Information Criterion (BIC) are sometimes used. Six of the 20 articles under our review used these indicators to assess predictivity. BIC additionally considers sample size, but both are based on log-likelihood.[ 39 Arbitrary scores in the scoring system Yuan et al 4 6 9 Using binary outcome and ignoring intermediaries Most models consider binary outcomes. This forces a probability of even 0.51 into the positive outcome group. This may be justified for an outcome such as mortality where there is no third possibility but for a diagnostic model at least three categories may be clinically more helpful: (i) the probability of the presence of the condition at least 90%, (ii) the probability of absence at least 90%, and (iii) the remaining doubtful. A ‘doubtful’ category may enhance the clinical utility of the model. Zhong et al 8 et al 5 Misinterpretation of P Concluding good predictivity based on the non-significance of the Hosmer–Lemeshow test as done by Yuan et al 4 P Ignoring future dynamics All models are developed on the available data on past or current cases, but the objective is to predict the outcome in future cases. This assumes that the process will continue to function the way it has been doing and ignores the possible changes in the future due to social and technological development. For example, this can easily happen with ICU mortality studied by Khan et al 2 28 40 Inappropriate validation setting Collins et al 41 et al 42 43 Performance of a model in external environment could also suffer when the predictors in the model are not applicable to the external population where the model is proposed to be used. Thus, a careful evaluation of applicability is essential for adequate predictivity in an external setup. Not relating the model to the process of the outcomes Besides prediction, a model also helps in understanding the process and in quantifying the relative role of various predictors, although not many exploit this premise. A model can be improved by incorporating new factors that may emerge by parsing the model for its various components and their coefficients, and by studying the biological relationship of each component with the outcome. Although all 20 papers under our review considered predictors that looked clinically relevant, only nine papers explicitly gave a biological explanation of the components of the model, and only three explained the relative importance of the values of the coefficient of the predictors or their scores. Interpretation of the value of the coefficients, such as an odds ratio of 2.5 for one factor being double the odds ratio of 1.25 for another factor generally saying that the first factor is twice as important in affecting the outcome as the second factor, may also help in a better appreciation of the process. This is rarely done. Such quantitative interpretation may lead to new hypotheses and a new model. Sundry issues Other issues are of a general nature that affect all empirical results but more severely affect the predictive performance of a model. The possible errors in the measurements, as may have happened with physiological and behavioral variables used by Li et al 6 et al 10 Statistical methods of estimation and tests of significance apply only to random samples. Only three studies[ 8 13 21 10 11 et al. 14 et al 5 In the case of logistic models, collinearity among predictors and linearity on logit scale are important but rarely considered. Of the 20 studies under our review, only six reported about collinearity – for others, the model may have inaccurate coefficients because of unsuspected collinearity. The predictors selected for developing a model should be examined so that they are not closely related. Arbitrary categories of continuous predictors, such as tumor size categorized by Zhong et al 8 44 Each coefficient in the model is an estimate and has a standard error (SE) attached to it for sampling fluctuations. When these are considered, the uncertainty interval is bound to increase substantially. This also must be considered for assessing the predictivity of a model. Enough has already been said about P 45 C ONCLUSION Models are going to stay as our assistants, but they must be good for their predictive performance. In the case of binary outcomes, the predictivity should be assessed by PPV and NPV in a prospective setup in place of the currently used sensitivity-specificity-based ROC curve, and the lower bound of these indices must be at least 85% to inspire confidence. Instead of group-based methods such as calibration plots, decision curve analysis, AIC, and the Hosmer–Lemeshow test, the extent of one-to-one agreement between the observed and the predicted values within the specified clinical tolerance should be the index for assessing prediction performance for quantitative outcome. External validation must be a prerequisite for accepting a predictive model. We provide several other suggestions in this article to increase the predictive performance of a model. Key messages  Most prediction models claim to have good predictive performance but that may not be so in many cases. An analysis of the recent 20 articles from the world literature on prediction models indicates that the current method of assessing predictive accuracy is primarily the area under the ROC curve, supported by methods such as calibration plots and decision curve analysis. We discuss the inadequacy of these methods for assessing the predictive performance in individual cases in a clinical setup and suggest remedies for developing models with improved performance. Conflicts of interest There are no conflicts of interest. R EFERENCES 1 Takahashi H Nakajima M Ozaki K Tanaka T Kamatani N Ikegawa S Prediction model for knee osteoarthritis based on genetic and clinical information Arthritis Res Ther 2010 12 R187 20939878 10.1186/ar3157 PMC2991022 2 Khan SH Perkins AJ Fuchita M Holler E Ortiz D Boustani M Development of a population-level prediction model for intensive care unit (ICU) survivorship and mortality in older adults: A population-based cohort study Health Sci Rep 2023 6 e1634 37867787 10.1002/hsr2.1634 PMC10587446 3 Chen R Wang J Wang S Tang S Suo C [Construction of a risk prediction model for lung cancer based on lifestyle behaviors in the uk biobank large-scale population cohort] Sichuan Da Xue Xue Bao Yi Xue Ban 2023 54 892 8 Chinese 37866943 10.12182/20230960209 PMC10579072 4 Yuan C Xu G Zhan X Xie M Luo M She L Molybdenum target mammography-based prediction model for metastasis of axillary sentinel lymph node in early-stage breast cancer Medicine (Baltimore) 2023 102 e35672 37861524 10.1097/MD.0000000000035672 PMC10589595 5 Chen Y Wang Y Chen J Ma X Su L Wei Y Multidimensional dynamic prediction model for hospitalized patients with the omicron variant in China Infect Dis Model 2023 8 1097 107 37854788 10.1016/j.idm.2023.09.003 PMC10579104 6 Li L Lv X Li Y Zhang X Li M Cao Y Development and validation of risk prediction model for premenstrual syndrome in nurses: Results from the nurses-based the TARGET cohort study Front Public Health 2023 11 1203280 37854248 10.3389/fpubh.2023.1203280 PMC10579606 7 Zhang E Fan H Xu P Huang B Yao M Fei Y Analysis of the risk factors and a prediction model for postherpetic trigeminal neuralgia recurrence after extracranial nonsemilunar ganglion radiofrequency thermocoagulation Pain Physician 2023 26 E719 24 37847926 8 Zhong JW Nie DD Huang JL Luo RG Cheng QH Du QT Prediction model of no-response before the first transarterial chemoembolization for hepatocellular carcinoma: TACF score Discov Oncol 2023 14 184 37847433 10.1007/s12672-023-00803-2 PMC10581972 9 Matsumoto S Aoki M Shimizu M Funabiki T A clinical prediction model for non-operative management failure in patients with high-grade blunt splenic injury Heliyon 2023 9 e20537 37842598 10.1016/j.heliyon.2023.e20537 PMC10568089 10 Koozi H Lidestam A Lengquist M Johnsson P Frigyesi A A simple mortality prediction model for sepsis patients in intensive care J Intensive Care Soc 2023 24 372 8 37841294 10.1177/17511437221149572 PMC10572475 11 Wang Y Zhang J Chen X Sun M Li Y Wang Y Development and validation of a nomogram prediction model for multidrug-resistant organisms infection in a neurosurgical intensive care unit Infect Drug Resist 2023 16 6603 15 37840828 10.2147/IDR.S411976 PMC10573443 12 Chen G Dai X Zhang M Tian Z Jin X Mei K Machine learning-based prediction model and visual interpretation for prostate cancer BMC Urol 2023 23 164 37838656 10.1186/s12894-023-01316-4 PMC10576344 13 Fu Y Huang S Zhao D Qiu P Hu J Liu X Establishing and validating a morphological prediction model based on CTA to evaluate the incidence of type-B dissection Diagnostics (Basel) 2023 13 3130 37835873 10.3390/diagnostics13193130 PMC10572133 14 Huqh MZU Abdullah JY Al-Rawas M Husein A Ahmad WMAW Jamayet NB Development of artificial neural network-based prediction model for evaluation of maxillary arch growth in children with complete unilateral cleft lip and palate Diagnostics (Basel) 2023 13 3025 37835768 10.3390/diagnostics13193025 PMC10572375 15 Xie S Yao R Yan Y Lin H Zhang P Chen Y Hybrid machine-learning-based prediction model for the peak dilation angle of rock discontinuities Materials (Basel) 2023 16 6387 37834523 10.3390/ma16196387 PMC10573530 16 Gao H Ji K Bao L Chen H Lin C Feng M Establishment and verification of prediction model of occult peritoneal metastasis in advanced gastric cancer World J Surg Oncol 2023 21 320 37833730 10.1186/s12957-023-03188-2 PMC10571475 17 Zhao Q Li Y Wang T Development and validation of prediction model for early warning of ovarian metastasis risk of endometrial carcinoma Medicine (Baltimore) 2023 102 e35439 37832099 10.1097/MD.0000000000035439 PMC10578755 18 Geng Z Guo T Cao Z He X Chen J Yue H Development and validation of a novel clinical prediction model to predict the 90-day functional outcome of spontaneous intracerebral hemorrhage Front Neurol 2023 14 1260104 37830093 10.3389/fneur.2023.1260104 PMC10566304 19 Matsui H Shindo Y Yamada D Ogihara H Tokumitsu Y Nakajima M A novel prediction model of pancreatic fistula after pancreaticoduodenectomy using only preoperative markers BMC Surg 2023 23 310 37828597 10.1186/s12893-023-02213-1 PMC10571374 20 Chen H Cao B Yang J Ren H Xia X Zhang X Construction and effect evaluation of prediction model for red blood cell transfusion requirement in cesarean section based on artificial intelligence BMC Med Inform Decis Mak 2023 23 213 37828543 10.1186/s12911-023-02286-1 PMC10568840 21 Yuan L Yao W Development and validation of a risk prediction model for in-hospital mortality in patients with acute upper gastrointestinal bleeding Clin Appl Thromb Hemost 2023 29 10760296231207806 37828791 10.1177/10760296231207806 PMC10576926 22 Cook NR Use and misuse of the receiver operating characteristic curve in risk prediction Circulation 2007 115 928 35 17309939 10.1161/CIRCULATIONAHA.106.672402 23 Ozenne B Subtil F Maucort-Boulch D The precision-recall curve overcame the optimism of the receiver operating characteristic curve in rare diseases J Clin Epidemiol 2015 68 855 9 25881487 10.1016/j.jclinepi.2015.02.010 24 Indrayan A Malhotra RK Pawar M Use of ROC curve analysis for prediction gives fallacious results: Use predictivity-based indices J Postgrad Med 2024 70 91 6 38668827 10.4103/jpgm.jpgm_753_23 PMC11160993 25 Yuan Y Zhou QM Li B Cai H Chow EJ Armstrong GT A threshold-free summary index of prediction accuracy for censored time to event data Stat Med 2018 37 1671 81 29424000 10.1002/sim.7606 PMC5895543 26 Brenner H Gefeller O Variation of sensitivity, specificity, likelihood ratios and predictive values with disease prevalence Stat Med 1997 16 981 91 9160493 10.1002/(sici)1097-0258(19970515)16:9<981::aid-sim510>3.0.co;2-n 27 Jabbour S Fouhey D Shepard S Valley TS Kazerooni EA Banovic N Measuring the impact of AI in the diagnosis of hospitalized patients: A randomized clinical vignette survey study JAMA 2023 330 2275 84 38112814 10.1001/jama.2023.22295 PMC10731487 28 Khan SS Matsushita K Sang Y Ballew SH Grams ME Surapaneni A Development and validation of the American Heart Association predicting risk of cardiovascular disease EVENTs (PREVENT) equations Circulation 2024 149 430 49 37947085 10.1161/CIRCULATIONAHA.123.067626 PMC10910659 Erratum Circulation 2024 149 e956 38466792 10.1161/CIR.0000000000001230 29 Chowdhury MZI Turin TC Variable selection strategies and its importance in clinical prediction modelling Fam Med Community Health 2020 8 e000262 32148735 10.1136/fmch-2019-000262 PMC7032893 30 Chicco D Warrens MJ Jurman G The coefficient of determination R-squared is more informative than SMAPE, MAE, MAPE, MSE and RMSE in regression analysis evaluation PeerJComput Sci 2021 7 e623 10.7717/peerj-cs.623 PMC8279135 34307865 31 Mahaffey KW Gibson CM Lopes RD Innovation in event adjudication-human vs machine JAMA Cardiol 2024 9 101 2 37950739 10.1001/jamacardio.2023.4900 32 Indrayan A Direct use of clinical tolerance limits for assessing the degree of agreement between two methods of measuring blood pressure South Med J 2023 116 435 9 37137481 10.14423/SMJ.0000000000001551 33 Steyerberg EW Vickers AJ Cook NR Gerds T Gonen M Obuchowski N Assessing the performance of prediction models: A framework for traditional and novel measures Epidemiology 2010 21 128 38 20010215 10.1097/EDE.0b013e3181c30fb2 PMC3575184 34 Indrayan A Malhotra RK Medical Biostatistics 4th Boca Raton, FL CRC Press 2018 638 35 Austin PC Harrell FE Jr van Klaveren D Graphical calibration curves and the integrated calibration index (ICI) for survival models Stat Med 2020 39 2714 42 32548928 10.1002/sim.8570 PMC7497089 36 Piovani D Sokou R Tsantes AG Vitello AS Bonovas S Optimizing clinical decision making with decision curve analysis: Insights for clinical investigators Healthcare (Basel) 2023 11 2244 37628442 10.3390/healthcare11162244 PMC10454914 37 Park SY Park JE Kim H Park SH Review of statistical methods for evaluating the performance of survival or other time-to-event prediction models (from Conventional to Deep Learning Approaches) Korean J Radiol 2021 22 1697 707 34269532 10.3348/kjr.2021.0223 PMC8484151 38 Vásquez-Gómez J Cifuentes-Amigo A Castillo-Retamal M Zamunér AR A VO2peak prediction model in older adults’ patients with Parkinson’s disease Exp Gerontol 2023 181 112285 37678552 10.1016/j.exger.2023.112285 39 Seedorff J Cavanaugh JE Assessing variable importance for best subset selection Entropy (Basel) 2024 26 801 39330134 10.3390/e26090801 PMC11431525 40 Larkin H What to know about PREVENT, the AHA’s New Cardiovascular Disease Risk Calculator JAMA 2024 331 277 9 38150239 10.1001/jama.2023.25115 41 Collins GS Dhiman P Ma J Schlussel MM Archer L Van Calster B Evaluation of clinical prediction models (part 1): From development to external validation BMJ 2024 384 e074819 38191193 10.1136/bmj-2023-074819 PMC10772854 42 Riley RD Archer L Snell KIE Ensor J Dhiman P Martin GP Evaluation of clinical prediction models (part 2): How to undertake an external validation study BMJ 2024 384 e074820 38224968 10.1136/bmj-2023-074820 PMC10788734 43 Riley RD Snell KIE Archer L Ensor J Debray TPA van Calster B Evaluation of clinical prediction models (part 3): Calculating the sample size required for an external validation study BMJ 2024 384 e074821 38253388 10.1136/bmj-2023-074821 PMC11778934 44 Ma J Dhiman P Qi C Bullock G van Smeden M Riley RD Poor handling of continuous predictors in clinical prediction models using logistic regression: A systematic review J Clin Epidemiol 2023 161 140 51 37536504 10.1016/j.jclinepi.2023.07.017 PMC11913776 45 Wasserstein RL Schirm AL Lazar NA Moving to a world beyond “p < 0.05.” Am Stat 2019 73 suppl 1 1 19 ",
  "metadata": {
    "Title of this paper": "Moving to a world beyond “p < 0.05.”",
    "Journal it was published in:": "Indian Journal of Community Medicine: Official Publication of Indian Association of Preventive & Social Medicine",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12470336/"
  }
}
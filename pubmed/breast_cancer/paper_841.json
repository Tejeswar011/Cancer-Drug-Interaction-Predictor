{
  "title": "Paper_841",
  "abstract": "pmc Int J Comput Assist Radiol Surg Int J Comput Assist Radiol Surg 365 springeropen International Journal of Computer Assisted Radiology and Surgery 1861-6410 1861-6429 pmc-is-collection-domain yes pmc-collection-title Springer PMC12476431 PMC12476431.1 12476431 12476431 40563071 10.1007/s11548-025-03457-3 3457 1 Original Article Streamlining the annotation process by radiologists of volumetric medical images with few-shot learning Ryabtsev Alina 1 Lederman Richard 2 Sosna Jacob 2 http://orcid.org/0000-0002-3010-4770 Joskowicz Leo josko@cs.huji.ac.il 1 1 https://ror.org/03qxff017 grid.9619.7 0000 0004 1937 0538 School of Computer Science and Engineering, The Hebrew University of Jerusalem, 2 https://ror.org/01cqmqj90 grid.17788.31 0000 0001 2221 2926 Dept. of Radiology, Hadassah University Medical Center, 25 6 2025 2025 20 9 497717 1863 1873 10 1 2025 2 6 2025 25 06 2025 29 09 2025 29 09 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by/4.0/ Open Access http://creativecommons.org/licenses/by/4.0/ Purpose Radiologist’s manual annotations limit robust deep learning in volumetric medical imaging. While supervised methods excel with large annotated datasets, few-shot learning performs well for large structures but struggles with small ones, such as lesions. This paper describes a novel method that leverages the advantages of both few-shot learning models and fully supervised models while reducing the cost of manual annotation. Methods Our method inputs a small dataset of labeled scans and a large dataset of unlabeled scans and outputs a validated labeled dataset used to train a supervised model (nnU-Net). The estimated correction effort is reduced by having the radiologist correct a subset of the scan labels computed by a few-shot learning model (UniverSeg). The method uses an optimized support set of scan slice patches and prioritizes the resulting labeled scans that require the least correction. This process is repeated for the remaining unannotated scans until satisfactory performance is obtained. Results We validated our method on liver, lung, and brain lesions on CT and MRI scans (375 scans, 5933 lesions). It significantly reduces the estimated lesion detection correction effort by 34% for missed lesions, 387% for wrongly identified lesions, with 130% fewer lesion contour corrections, and 424% fewer pixels to correct in the lesion contours with respect to manual annotation from scratch. Conclusion Our method effectively reduces the radiologist’s annotation effort of small structures to produce sufficient high-quality annotated datasets to train deep learning models. The method is generic and can be applied to a variety of lesions in various organs imaged by different modalities. Supplementary Information The online version contains supplementary material available at 10.1007/s11548-025-03457-3. Keywords Medical image detection and segmentation Few-shot learning Deep learning Annotation efficiency Hebrew University of Jerusalem Open access funding provided by Hebrew University of Jerusalem. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © CARS 2025 Introduction Deep learning (DL) models have become the method of choice for the automatic detection and segmentation of structures in medical images. Supervised learning methods require training datasets with radiologists' annotations. The most popular DL architecture, U-Net [ 1 2 3 4 5 Recent research has focused on developing image annotation methods to expedite the labeling task [ 6 9 6 7 8 Few-shot learning (FSL) is a promising paradigm for addressing the labeling issue. Using prior knowledge, it utilizes a few annotated scans to generalize a new task and generate labels for a larger unlabeled dataset [ 9 10 11 Ouyang et al. [ 12 13 This paper presents a novel method for streamlining the process of annotating volumetric medical image datasets used to train fully supervised deep learning voxel-level lesion classification models. The method focuses on obtaining annotations for scans with small structures and from various imaging modalities on individual slides (2D images) of scans. The method is an end-to-end pipeline that inputs a small dataset of labeled volumetric scans and a large dataset of unlabeled scans, and outputs a high-quality labeled dataset with which a fully supervised model is trained until it yields satisfactory performance. Our method reduces the estimated correction effort of radiologists by using a few-shot learning model to generate initial labels for the new structure of interest using an optimized support set of scan slice patches and by prioritizing the resulting labeled scans that require the fewest corrections. The contributions of this paper are: (1) a new method for the generation of high-quality labeled scans and a fully supervised model that minimizes the expected manual correction effort and achieves a desired performance on an independent test set; (2) a policy for selecting the most representative labeled patches for the support set of a few-shot learning model; (3) a strategy for prioritizing a subset of scans with computed labels for review and correction by a radiologist with reduced effort; (4) metrics for estimating the effort required by a radiologist to correct the computed lesions, quantified by the number of mouse clicks required for the correction; (5) extensive experimental evaluation of the method in three use-case scenarios: detection and segmentation of liver metastases in abdominal contrast-enhanced CT scans, lung metastases in chest CT scans, and brain metastases in brain MRI scans. Method Our method streamlines the process of manual annotation of small pathologies, e.g., lesions, in scans. The goal is to obtain sufficient, high-quality, expert-validated labeled data to train a fully supervised deep learning model. The method uses a pre-trained few-shot learning model with a small support set of manually annotated scans to compute initial voxel-level labels for a larger set of unannotated scans. A subset of these scans is then selected for review and correction by a radiologist. By prioritizing the labeled scans for corrections, the method minimizes the correction effort while increasing the labels representativity. A fully supervised model is then trained on the scans with corrected labels and the support set scans and tested on an independent annotated dataset. The process is repeated for the remaining unannotated scans until satisfactory performance is obtained. The method is generic and can be applied to various volumetric medical imaging modalities, organs, and pathologies. Figure 1 Fig. 1 Overview of the method. The inputs are: a small support dataset of annotated scans (red), a large dataset of unannotated training scans to be used as query scans (green), and a test dataset of annotated scans (yellow). The outputs are a large, high-quality dataset of annotated scans (green) and a nnU-Net model for voxel-level lesion classification trained on those scans. The steps are: (1) support set patch selection and query scans pre-processing; (2) few-shot learning inference; (3) prioritization of the scans for correction based on the computed labels; (4) manual correction of a subset of the scan labels by a radiologist; (5) training of a nnU-Net model on the support and query labeled datasets and testing its performance on the test dataset. This process is repeated when needed for the remaining unlabeled scans until the trained nnU-Net model yields satisfactory performance on the test dataset Our method is based on two key novel ideas. The first is a policy for selecting the most representative labeled patches for the support set. Since lesions have various shapes, patterns, textures, contrasts, and appear in various organs, it is important to select a support set that reflects the distribution of the lesions. The second is a strategy for prioritizing the subset of scans with computed labels for review and correction by a radiologist with minimal effort and maximal performance when using the corrected scans to train a fully supervised model. Support set patch selection and pre-processing of query scans This step prepares the support and query scans for the few-shot learning model and builds the support set of patches based on empirical heuristics. First, the scans in both datasets are pre-processed by pre-set intensity clipping and min–max normalization to scale the values to [0,1]. Next, 128 × 128 patches with a ½ stride are extracted from each support and query set scan. Positive patches, e.g., those with a minimum number of positive lesion pixels (> 30 pixels), are selected from the generated support patches. Next, a fixed-size subset of patches is drawn from the resulting set of patches with a k-means clustering-based strategy that ensures diversity of patterns of lesion parts. The policy proceeds as follows. First, binary flattened vectors are generated for all the positive patches. Then, k-means clustering is performed on them with k Few-shot model inference This step computes initial voxel-level lesion labels for the query set using the selected labeled support patch set. We use the weights of the pre-trained UniverSeg FSL model, which were pre-computed on open-source datasets for various tasks, structures of interest, and imaging modalities [ 13 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\le $$\\end{document} ≤ Prioritization of computed scan labels for correction This step selects a subset of the resulting labeled scans for review and correction by a radiologist, which may require minimal corrections. The prioritization policy is designed to rank the scans with the computed labels according to the number of detected lesions whose diameter is \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ M Manual correction of the prioritized subset of labeled scans The review and correction of the computed lesion labels on the selected scans by a radiologist proceeds as follows: each slice of each selected scan is reviewed by overlaying the computed lesion labels on the original slice. Wrongly identified lesions (false positives) are removed with a single click inside the computed lesion. The voxels in the lesion connected component are automatically unlabeled. Missed lesions (false negatives) are added by delineating their contour with one mouse click per contour pixel. Correctly identified lesions (true positives) whose contours are partially incorrect are revised by adding and deleting pixel contours, with one mouse click per erroneous pixel on the lesion contour and automatic addition or deletion of the interior pixels. Detection and segmentation of lesions with a fully supervised model This step trains a fully supervised model to obtain an intermediate model that produces better voxel-level lesion labels than the FSL model on the remaining unlabeled scans. The model is trained with the corrected labeled scans and the labeled support set scans. The performance of the resulting model is quantified with the labeled test set scans. If the performance is acceptable, the process stops and outputs the labeled datasets and the trained model. Otherwise, the process is repeated for the remaining unannotated scans until satisfactory performance is obtained. For inference, we use the supervised 3D nnU-Net [ 3 Experimental setup Datasets: Three datasets of liver, lung, and brain scans were created (Fig. 2 DLIVER DLUNGS DBRAIN DLIVER DLIVER_FSL DLIVER_TEST DLIVER_FSL DLIVER_FSL_SUPPORT \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K$$\\end{document} K DLIVER_FSL_QUERY ( DLUNGS DBRAIN DLUNGS_FSL_SUPPORT DBRAIN_FSL_SUPPORT K DLUNGS_FSL_QUERY DBRAIN_FSL_QUERY Fig. 2 Flowchart illustrating: a b c Manual annotation: GT DLIVER DLUNGS DBRAIN DLIVER-LESIONS DLUNGS-LESIONS DBRAIN-LESIONS \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ Evaluation metrics: Lesion detection and segmentation Precision, Recall, F1-score TP FP FN Dice Dice_with_FN Estimated correction effort We introduce three new metrics: false negative ratio FNR false positive ratio FPR contour correction score CCS CCS Experimental studies: Results Study 1: Evaluation of the UniverSeg and ALPNet models. DLIVER_FSL 12 13 DLIVER_FSL The first 450 positive patches were included in the support set DLIVER_FSL_SUPPORT DLIVER_FSL DLIVER_FSL_QUERY 1 3 Table 1 Results of study 1 FSL Model Detection Segmentation Precision Recall F1-score Dice Dice_with_FN UniverSeg Mean Std 0.61 (0.26) 0.59 (0.24) 0.43 (0.05) 0.49 (0.09) 0.30 (0.14) ALPNet Mean Std 1.00 (0.00) 0.31 (0.03) 0.45 (0.01) 0.05 (0.02) 0.03 (0.02) Performance of the fivefold cross-validation of the UniverSeg and the ALPNet models in a one-shot setting ( \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K=1$$\\end{document} K = 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ DLIVER_FSL_QUERY . Dice Dice_with_FN Fig. 3 Illustration of Study 1. Two axial images from a CT scan (top and bottom) of the ground truth and the computed segmentations by the UniverSeg and ALPNet models of liver lesions (red). For this example, the precision and recall of the UniverSeg and ALPNet models were 1.00 and 0.18, 0.20 and 0.86 and 0.20, and the Dice scores were 0.13 and 0.57, respectively UniverSeg outperformed ALPNet for lesion detection, with a recall of 0.59 ± 0.24 vs. 0.31 ± 0.03 (90% higher); the perfect precision of ALPNet (1.00 ± 0.0) reflects the very low number of detected lesions. UniverSeg yields Dice Dice_with_FN Study 2: Evaluation of the support set patch selection policies. DLIVER_FSL_SUPPORT DLIVER_FSL_QUERY \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ 2 4 Table 2 Results of study 2 Support patches sampling method Detection Segmentation Precision Recall F1-score Dice Dice_with_FN Random Mean (Std) 0.48 (0.37) 0.80 (0.25) 0.54 (0.35) 0.55 (0.28) 0.49 (0.29) Clustering Mean (Std) 0.58 (0.33) 0.82 (0.25) 0.58 (0.33) 0.55 (0.27) 0.50 (0.28) Performance of the UniverSeg model with fivefold cross-validation on 93 query scans with a support set of patches sampled from 10 scans with the random and clustering policies for lesions \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ DLIVER_FSL_QUERY . Precision Recall F1-scores Dice Dice_with_FN Fig. 4 Illustration of Study 2. a b c d c d Dice FNR FPR CCS \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ The clustering support set patch selection policy outperformed the random policy for lesions \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ Dice Dice_with_FN 14 Study 3 : Evaluation of the scans prioritization strategy for manual correction DLIVER DLIVER_FSL_QUERY DLIVER_FSL_SUPPORT DLIVER_FSL_QUERY DLIVER FNR FPR CCS DLIVER_TEST Both nnU-Net models achieved similar performance in lesion detection and segmentation for lesions \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ S1 S1 14 The estimated correction effort for each strategy is listed in Table 3 FNR FPR 15 Table 3 Results of study 3 Strategy for the selection of manual correction scans Estimated manual correction effort Detection Segmentation FP FN # of lesions # of incorrect contour pixels Random 155 75 345 360,277 Prioritized 40 56 150 68,677 Estimation of the effort required by a radiologist for correcting the results of computed labels with the UniverSeg model with random and prioritized scan selection strategies. Listed are the number of mouse clicks required for a radiologist to correct the lesion detection and segmentation for both strategies. For lesion detection, a single mouse click is required for each FP and FN lesion. For lesion segmentation, listed are the number of lesions and the number of incorrect contour pixels that that require correction – one mouse click for each pixel Study 4: Evaluation of the estimated effort required to correct annotations on three datasets DLIVER_FSL_QUERY ) , DLUNGS_FSL_QUERY DBRAIN_FSL_QUERY \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ FPR FNR CCS 4 5 Table 4 Results of study 4 Dataset Lesion sizes Detection Segmentation FPR FNR CCS DLIVER All lesions Mean (Std) 0.36 (0.31) 0.42 (0.23) 0.24 (0.20) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ Mean (Std) 0.27 (0.22) 0.18 (0.23) 0.25 (0.23) DLUNGS All lesions Mean (Std) 0.93 (0.20) 0.95 (0.19) 0.64 (0.32) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ Mean (Std) 0.11 (0.27) 0.37 (0.46) 0.72 (0.45) DBRAIN All lesions Mean (Std) 0.49 (0.01) 0.40 (0.37) 0.24 (0.17) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ Mean (Std) 0.30 (0.15) 0.08 (0.08) 0.25 (0.16) Estimated correction effort scores of the liver, lung, and brain lesions (all and \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ FPR FNR CCS Fig. 5 Illustration of the results of Study 4. Examples of the UniverSeg model results on liver (top), lung (middle), and brain (bottom) lesion in abdominal CECT, chest CT, and brain T1W-Gad MRI scans, respectively. The scores for the liver, lungs, and brain scans were as follows: the FNR FPR Dice CCS For the liver lesions \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ FNR FPR CCS \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ FNR FPR CCS \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ge $$\\end{document} ≥ FNR FPR CCS Limitations Conclusion The radiologists' annotation effort is a key bottleneck in the development and deployment of deep learning methods for volumetric medical image analysis. State-of-the-art supervised methods (nnU-Net) achieve superior results when trained with a sufficient number of high-quality annotated scans. Few-shot learning methods (UniverSeg) using a few annotated support set scans yield good results for large anatomical structures, e.g., organs and bones. However, they perform poorly for small structures, e.g., lesions, even for support sets with 10 scans (Study 2). Our new method leverages the advantages of both few-shot learning models and supervised models while reducing the expected cost of manual annotation. It uses the UniverSeg model to produce an initial lesion voxel-level classification from a set of patches selected with a clustering strategy from a few annotated scans. This strategy yields labels that require fewer corrections than random selection or manual delineation (Study 2). A subset of the labeled scans is prioritized for review and correction by a radiologist. The resulting high-quality annotated scans, together with the support set scans, are then used to train an nnU-Net model, whose performance is evaluated with an annotated test set. Study 3 shows that the prioritization and random strategies yield similar nnU-Net performance; prioritization significantly reduces the estimated detection correction effort by 34% for missed lesions, by 387% for wrongly identified lesions, with 130% fewer lesion contour corrections and 424% fewer pixels to correct in the lesion contours. Study 4 demonstrates that our method is generic, applicable to various modalities (CT, CECT, and MRI), and to lesions in different organs, e.g., liver, lungs, and brain on a dataset of 375 scans with 5,933 lesions. Our method effectively reduces the radiologists' annotation effort of small structures to produce sufficient high-quality annotated datasets to train deep learning models. It is generic and can be applied to a variety of lesions in various organs imaged with different modalities. Supplementary Information Below is the link to the electronic supplementary material. Supplementary file1 (PDF 839 KB) Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Funding Open access funding provided by Hebrew University of Jerusalem. Declarations Conflict of interest The authors have no conflict of interest. Ethical approval All procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki Declaration and its later amendments. Informed consent There was no need for informed consent. References 1. Ronneberger O, Fischer P, Brox T. (2015). U-Net: convolutional networks for biomedical image segmentation. In: Navab, N., Hornegger, J., Wells, W., Frangi, A. (eds) Medical image computing and computer-assisted intervention. Lecture Notes in Computer Science 2. Azad R Aghdam EK Rauland A Jia Y Haddadi A Medical image segmentation review: the success of U-Net IEEE Trans Pattern Anal Mach Intell 2024 46 12 10076 10095 10.1109/TPAMI.2024.3435571 39167505 Azad R, Aghdam EK, Rauland A, Jia Y, Haddadi A (2024) Medical image segmentation review: the success of U-Net. IEEE Trans Pattern Anal Mach Intell 46(12):10076–10095 39167505 10.1109/TPAMI.2024.3435571 3. Isensee F Jaeger PF Kohl SA Petersen J Maier-Hein K nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation Nat Methods 2020 18 203 211 10.1038/s41592-020-01008-z 33288961 Isensee F, Jaeger PF, Kohl SA, Petersen J, Maier-Hein K (2020) nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods 18:203–211 33288961 10.1038/s41592-020-01008-z 4. Isensee F, Wald T, Ulrich C, Baumgartner M, Roy S, Maier-Hein K, Jaeger PF. (2024). nnU-Net revisited: a call for rigorous validation in 3D medical image segmentation. In: Linguraru, M.G., et al. Medical image computing and computer assisted intervention. Lecture notes in computer science 15003. Springer 5. Nemoto T Futakami N Kunieda E Yagi M Takeda A Akiba T Mutu E Shigematsu N Effects of sample size and data augmentation on U-Net-based automatic segmentation of various organs Radiol Phys Technol 2021 14 3 318 327 10.1007/s12194-021-00630-6 34254251 Nemoto T, Futakami N, Kunieda E, Yagi M, Takeda A, Akiba T, Mutu E, Shigematsu N (2021) Effects of sample size and data augmentation on U-Net-based automatic segmentation of various organs. Radiol Phys Technol 14(3):318–327 34254251 10.1007/s12194-021-00630-6 6. Bangert P Moon H Woo JO Didari S Hao H Active learning performance in labeling radiology images is 90% effective Front Radiol 2021 1 748968 10.3389/fradi.2021.748968 37492167 PMC10365082 Bangert P, Moon H, Woo JO, Didari S, Hao H (2021) Active learning performance in labeling radiology images is 90% effective. Front Radiol 1:748968 37492167 10.3389/fradi.2021.748968 PMC10365082 7. Zhang Y, Zhou T, Liang P, Chen D. (2023). Input augmentation with SAM: boosting medical image segmentation with segmentation foundation model. In: Medical image computing and computer assisted intervention. Lecture notes in computer science 14393. Springer 8. Hu C, Xia T, Ju S, Li X. (2023). When SAM meets medical images: an investigation of Segment Anything Model (SAM) on multi-phase liver tumor segmentation. ArXiv: 2304.08506 9. Wang Y Yao Q Kwok JT Ni LM Generalizing from a few examples: a survey on few-shot learning ACM Comput Surv 2020 53 3 1 34 10.1145/3386252 Wang Y, Yao Q, Kwok JT, Ni LM (2020) Generalizing from a few examples: a survey on few-shot learning. ACM Comput Surv 53(3):1–34 10. Wang K, Liew JH, Zou Y, Zhou D, Feng J. (2019). PANet: few-shot image semantic segmentation with prototype alignment. In: Proc. IEEE Int. Conf. Computer Vision, pp. 9196–9205 11. Everingham M Eslami SMA Van Gool L The PASCAL visual object classes challenge: a retrospective Int J Computer Vision 2015 111 98 136 10.1007/s11263-014-0733-5 Everingham M, Eslami SMA, Van Gool L et al (2015) The PASCAL visual object classes challenge: a retrospective. Int J Computer Vision 111:98–136 12. Ouyang C Biffi C Chen C Kart T Qiu H Rueckert D Self-supervised learning for few-shot medical image segmentation IEEE Trans Med Imag 2022 41 1837 1848 10.1109/TMI.2022.3150682 35139014 Ouyang C, Biffi C, Chen C, Kart T, Qiu H, Rueckert D (2022) Self-supervised learning for few-shot medical image segmentation. IEEE Trans Med Imag 41:1837–1848 10.1109/TMI.2022.3150682 35139014 13. Butoi VI, Ortiz JJG, Ma T, Sabuncu MR, Guttag JV, Dalca AV (2023). UniverSeg: universal medical image segmentation. In: Proc. Int. Conf. Computer Vision, pp. 21381–21394 14. Szeskin A Rochman S Weis S Lederman R Sosna J Joskowicz L Liver lesion changes analysis in longitudinal CECT scans by simultaneous deep learning voxel classification with SimU-Net Med Image Anal 2023 83 102675 10.1016/j.media.2022.102675 36334393 Szeskin A, Rochman S, Weis S, Lederman R, Sosna J, Joskowicz L (2023) Liver lesion changes analysis in longitudinal CECT scans by simultaneous deep learning voxel classification with SimU-Net. Med Image Anal 83:102675 36334393 10.1016/j.media.2022.102675 15. Joskowicz L Cohen D Caplan N Sosna J Inter-observer variability of manual contour delineation of structures in CT Eur Radiol 2019 29 3 1391 1399 10.1007/s00330-018-5695-5 30194472 Joskowicz L, Cohen D, Caplan N, Sosna J (2019) Inter-observer variability of manual contour delineation of structures in CT. Eur Radiol 29(3):1391–1399 30194472 10.1007/s00330-018-5695-5 ",
  "metadata": {
    "Title of this paper": "Inter-observer variability of manual contour delineation of structures in CT",
    "Journal it was published in:": "International Journal of Computer Assisted Radiology and Surgery",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12476431/"
  }
}
{
  "title": "Paper_556",
  "abstract": "pmc Digit Health Digit Health 3398 dghealth DHJ Digital Health 2055-2076 SAGE Publications PMC12480807 PMC12480807.1 12480807 12480807 41036433 10.1177/20552076251368045 10.1177_20552076251368045 1 Original Research Article eXplainable artificial intelligence-Eval: A framework for comparative evaluation of explanation methods in healthcare Agrawal Krish 1 https://orcid.org/0000-0002-5679-9099 El Shawi Radwa 2 Ahmed Nada 3  1 226957 Indian Institute of Technology Indore  2 37546 University of Tartu  3 112893 Radwa El Shawi, Institute of Computer Science, University of Tartu, 51009 Tartu, Estonia. \nEmail: radwa.elshawi@ut.ee 29 9 2025 Jan-Dec 2025 11 478303 20552076251368045 18 5 2025 21 7 2025 29 09 2025 01 10 2025 02 10 2025 © The Author(s) 2025 2025 SAGE Publications Ltd, unless otherwise noted. Manuscript content on this site is licensed under Creative Commons Licenses https://creativecommons.org/licenses/by-nc/4.0/ This article is distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 License ( https://creativecommons.org/licenses/by-nc/4.0/ https://us.sagepub.com/en-us/nam/open-access-at-sage Objective Machine learning systems are increasingly used in high-stakes domains such as healthcare, where predictive accuracy must be accompanied by explainability to ensure trust, validation, and regulatory compliance. This study aims to evaluate the effectiveness of widely used local and global explanation methods in real-world clinical settings. Methods We introduce a structured evaluation methodology for the quantitative comparison of explainability techniques. Our analysis covers five local model-agnostic methods—local interpretable model-agnostic explanations (LIME), contextual importance and utility, RuleFit, RuleMatrix, and Anchor—assessed using multiple explainability criteria. For global interpretability, we consider LIME, Anchor, RuleFit, and RuleMatrix. Experiments are conducted on diverse healthcare datasets and tasks to assess performance. Results The results show that RuleFit and RuleMatrix consistently provide robust and interpretable global explanations across tasks. Local methods show varying performance depending on the evaluation dimension and dataset. Our findings highlight important trade-offs between fidelity, stability, and complexity, offering critical insights into method suitability for clinical applications. Conclusion This work provides a practical framework for systematically assessing explanation methods in healthcare. It offers actionable guidance for selecting appropriate and trustworthy techniques, supporting safe and transparent deployment of machine learning models in sensitive, real-world environments. Explainable artificial intelligence (XAI) model interpretability black-box models healthcare AI Princess Nourah bint Abdulrahman University Researchers Supporting PNURSP2025R756 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes typesetter ts19 cover-date January-December 2025 Introduction The integration of artificial intelligence (AI) systems across various domains significantly enhances their efficacy in diverse tasks. These AI systems primarily rely on complex machine learning (ML) models, achieving outstanding performance in tasks such as prediction, recommendation, and decision-making support. 1 5  6  7  8  9 In response to the practical and ethical challenges posed by the opacity of black-box models in AI, the field of eXplainable (XAI) has witnessed a surge in the development of explanation methods from both academia and industry.  10  9  1 11 12 13 15 However, the landscape of XAI methods is highly heterogeneous and fragmented, posing significant challenges for both researchers and practitioners.  16  17  18  19  20  21 Moreover, explanation techniques vary in scope and assumptions: local methods provide explanations for individual predictions—typically by approximating the model’s decision boundary in a small neighborhood of the input—while global methods aim to summarize the overall decision logic of the model across the entire dataset.  22 To address the complexity and fragmentation in evaluating XAI methods, we propose a unified evaluation framework that systematically compares both local and global explanation techniques across multiple interpretability paradigms. Our work builds on previous studies—including our own earlier benchmark focused on local model-agnostic methods in healthcare  23  24 Related work The domain of explainable AI methods has arisen more recently within academic circles in comparison to the broader expanse of AI literature. This emergence has been prompted by the widespread integration of AI models across diverse sectors in contemporary society, signifying its growing importance. 1 23 25 26 1 27 29  30  11 Another branch of literature focuses on the quantitative and qualitative evaluation of explanation methods assessing the quality and utility of returned explanations, considering aspects like their effectiveness and relevance. 6 16 23 31 33  34  35  36 37 39 Methods This section outlines the methodology used to benchmark explainability techniques. It introduces the explanation frameworks considered in this study and describes the metrics employed to evaluate their performance. These methods were selected based on their relevance, popularity, and compatibility with both local and global interpretability goals. Evaluated explainability frameworks Local interpretable model-agnostic explanations (LIME) The LIME  28  x  x  π x  x  z ∈ R  g ( . )  b  b ( z ) Contextual importance and utility (CIU) CIU is a local, post-hoc, model-agnostic explainer grounded in the concept that the significance of a feature can vary across different contexts.  40 41 42  42 Anchor Anchor is a model-agnostic method providing rule-based explanations called anchors  20  x  r  r ( x )  b ( x )  b  x RuleFit RuleFit is a global explainability framework designed for constructing rule ensembles.  19  S  R  α ∈ R M  R  S  L 1 RuleMatrix RuleMatrix is a model-agnostic explanation technique designed to offer both local and global explanations with a focus on visualizing extracted rules.  43  X  X sample  F  X sample  X sample  Y sample  44 Table 1 Table 1 Table 1. Key characteristics of explanation methods used in this study. Tool Local versus global Citations/year Year Data type LIME L&G 2063 2016 ANY CIU L 5 2020 TAB Anchor L&G 375 2018 ANY RuleFit G 83 2008 TAB RuleMatrix L&G 42 2018 TAB LIME: local interpretable model-agnostic explanations; CIU: contextual importance and utility; TAB: tabular; ANY: any data; G: global explanations; L: local explanations. Evaluation metrics for explainability methods This section provides a comprehensive overview of established quantitative evaluation metrics utilized in the subsequent benchmarking process. Identity:  24 Separability:  24 Fidelity:  b  34  g  b  b  g  g Speed:  23 Stability:  24  18  L x = max ‖ e x − e x ′ ‖ ‖ x − x ′ ‖  ∀ x ′ ∈ N x  x  e x  N x  x  ( 1 − L x ) × 100 % Monotonicity:  45  a i  ρ S ( a , e )  46  a = ( ⋯ , | a i | , … )  y * = f ( x * )  x *  e = ( … , E ( l ( y * , f i ) ; X i | x * − i ) , … ) (1) | a i | ∝ E ( l ( y * , f i ) | x − i * ) = ∫ x i l ( y * , f i ( x i ) ) p ( x i ) d x i where  f i  f  i  x − i * = ( x 1 , … , x i − 1 , x i + 1 , x N )  l  l Non-sensitivity:  f  45  18  A 0 ⊂ { 1 , … , N }  i  a i = 0  X 0 = { i ∈ { 1 , … , N } | E ( l ( y * , f i ) = 0 }  i  f  f i  y *  l  | A 0 △ X 0 |  | ⋅ |  A 0 Effective complexity:  45  a ( i )  x ( i )  M k = { x ( N − k ) , … , x ( N ) }  k  ϵ > 0  k *  k * = argmin k ∈ { 1 , … , N } | M k | , s.t. E ( l ( y * , f − M k ) | x * M k ) < ε  f − M k  f  M k  y *  l  ϵ = 0.1  47 The three following metrics—entropy ratio, Kullback-Leibler divergence, and Gini coefficient—are used to assess the explainability of a model based on the distribution of feature importance.  48  F = { F 1 , F 2 , … , F F }  P ( . )  F  j  p j : = | F j | / ∑ i = 1 F | F i |  P ( F )  P ( F ) : = p 1 , p 2 , … , p F  p j ≥ 0  ∑ i ∈ F p i = 1  U = p ¯ , p ¯ , … , p ¯  p ¯ = 1 / F Entropy ratio:  S E R ( P ( F ) ) = ∑ j = 1 F p j log p j / ∑ j = 1 F p ¯ log p ¯ Kullback-Leibler divergence:  S KL ( P ( F ) ) = D KL ( P ( F ) ‖ U ) = ∑ j = 1 F p j log p j p ¯ Gini coefficient:  S G ( P ( F ) ) = ∑ j = 1 F ∑ j ′ = 1 F | p j − p j ′ | 2 F 2 ( ∑ p j / F ) Correctness:  49  50 Compactness:  51 Experimenatal setup Overview of experimental workflow: Datasets:  52 Table 2 Tasks and models:  63 Explanation method selection: 6 16 23 Hardware resources: Table 2. Summary of datasets used in this study. Dataset references are provided in the bibliography. Dataset name Dataset description Number of features Number of classes Number of instances Thyroid disease  53 Dataset is a collection of 10 thyroid disease databases from the Garavan Institute and other sources. 20 3 6240 Heart disease  54 Dataset contains attributes, such as age, sex, cholesterol levels, and blood pressure from the Cleveland Clinic Foundation. 13 2 1025 Hepatitis  55 Dataset contains attributes, such as age, sex, steroid, and antivirals of blood donors and hepatitis C patients. 18 2 155 Echocardiogram  56 Dataset contains data for classifying if patients will survive for at least one year after a heart attack. 9 2 131 Breast Cancer Wisconsin (original)  57 Dataset contains features representing the characteristics of cell nuclei present in images of breast mass. 8 2 682 SPECT heart  58 Dataset contains continuous and binary features from cardiac single proton emission Computed tomography images. 22 2 267 Diabetes  59 Dataset contains 10 years of clinical care records from 130 US hospitals and integrated delivery networks. 41 2 98,053 Diabetic retinopathy Debrecen  60 Dataset contains features extracted from the Messidor image to predict whether it contains signs of diabetic retinopathy or not. 19 2 1151 HIV-1 protease cleavage  61 Dataset contains lists of octamers and a flag depending on whether HIV-1 protease will cleave in the central position. 8 2 2371 Heart failure clinical records  62 Dataset contains medical records of heart failure patients, collected during their follow-up period. 12 2 299 Results This section presents the quantitative outcomes of our benchmarking study on local and global explanation methods using a range of evaluation metrics. Results are reported objectively without interpretation. Local interpretability results Table 3 Table 3. Metric scores for different local explanation techniques on tabular data. Dataset name Method Identity (%) Separability (%) Fidelity (%) Speed (in seconds) Stability(%) Monotonicity Non-sensitivity Effective complexity Thyroid LIME 0.00 100.0 26.74 5.24 91.15  − 1.00 0.00 CIU 0.00 100.00 28.24 4.09 57.76 0.58 0.99 0.00 RuleFit 100.00 20.74 96.28 0.00003 96.79 0.41 18.22 0.00 RuleMatrix 100.00 47.98 98.01 0.03044 93.52 0.32 18.87 0.00 Anchor 76.79 43.44 90.97 11.67 68.14 0.29 17.53 0.00 Heart disease LIME 0.00 100.00 43.71 8.43 74.32  − 0.00 1.82 CIU 0.00 100.00 46.17 5.53 82.10 0.1226 0.00 2.19 RuleFit 100.00 70.28 80.54 0.00009 80.54 0.0212 11.95 2.11 RuleMatrix 100.00 92.27 91.43 0.00017 79.38 0.0671 11.21 1.75 Anchor 50.97 96.71 82.43 35.91 70.04 0.1821 9.77 1.77 Hepatitis LIME 0.00 100.00 29.06 2.87 79.49  − 1.00 8.00 CIU 82.05 100.00 30.34 1.71 64.10 0.21 0.48 8.23 RuleFit 100.00 78.17 58.97 0.00009 56.41  − 16.46 9.10 RuleMatrix 100.00 71.07 79.49 0.00021 61.54  − 16.59 8.21 Anchor 41.03 95.27 87.62 3.43 87.18  − 7.45 3.39 Echocardiogram LIME 0.00 100.00 63.13 0.73 87.88 0.1502 0.00 0.00 CIU 6.06 100.00 65.66 0.91 81.82 0.2393 0.42 4.55 RuleFit 100.0 75.21 63.64 0.00073 93.94 0.2087 8.73 4.24 RuleMatrix 100.00 37.56 78.79 0.00019 87.88  − 8.82 3.55 Anchor 57.58 66.58 96.46 2.46 51.52 0.2434 7.45 3.55 Breast Cancer Wiscousin (original) LIME 0.00 100.00 75.83 4.57 92.39  − 0.00 5.44 CIU 0.00 100.00 72.22 4.035 92.98 0.33 0.00 5.42 RuleFit 100.00 50.97 91.23 0.00017 92.98  − 7.56 5.49 RuleMatrix 100.00 62.67 94.74 0.00009 84.21  − 7.22 5.29 Anchor 58.48 74.80 80.73 44.83 61.40 0.0254 5.98 5.44 SPECT heart LIME 0.00 100.00 63.13 0.168 57.22  − 0.00 0.00 CIU 0.00 100.00 25.13 0.239 88.77 0.2201 0.00 0.00 RuleFit 100.00 67.52 87.70 0.00022 63.64 0.0776 21.41 0.00 RuleMatrix 100.00 83.77 73.79 0.00007 51.87 0.12 20.61 0.00 Anchor 49.20 98.15 81.65 0.40 78.07 0.15 18.92 0.00 Diabetes LIME 0.00 100.00 21.43 5.72 60.82 0.0177 8.94 0.00 CIU 0.00 100.00 27.76 5.305 52.45 0.0159 5.99 0.06 RuleFit 100.00 68.02 59.64 0.00004 54.08 0.0080 40.19 0.00 RuleMatrix 100.00 94.56 63.06 0.00058 59.79 0.1038 39.03 0.00 Anchor 19.18 97.99 88.51 29.4 54.89 0.0193 35.11 0.00 Diabetic retinopathy Debrecan LIME 0.00 100.00 34.03 0.2793 60.07  − 0.00 5.88 CIU 0.00 100.00 32.29 0.5747 65.28  − 0.01 5.46 RuleFit 100.00 63.27 62.85 0.00013 62.15  − 18.06 7.87 RuleMatrix 100.00 89.68 60.42 0.00021 58.68 0.0197 17.60 6.52 Anchor 30.21 95.01 91.59 1.25 54.17 0.0127 15.57 8.43 HIV-1 protease cleavage LIME 0.00 100.00 74.03 2.67 86.67 0.0699 0.00 2.22 CIU 59.36 99.99 73.64 5.95 98.48  − 0.00 2.15 RuleFit 100.00 51.83 22.42 0.00005 87.86 0.0734 6.32 2.27 RuleMatrix 100.00 0.16 32.72 0.00005 100.00  − 8.00 2.20 Anchor 30.86 97.30 76.39 29.18 83.64  − 3.49 2.36 Heart failure clinical records LIME 0.00 100.00 42.89 2.94 84.00  − 0.00 0.00 CIU 0.00 100.00 53.33 3.27 72.00 0.29 0.00 0.64 RuleFit 100.00 62.77 76.00 0.00036 77.33 0.1445 10.39 0.77 RuleMatrix 100.00 44.36 36.00 0.00034 70.67 0.0441 11.61 0.76 Anchor 77.33 76.04 94.12 12.29 69.33 0.1807 10.06 0.64 LIME: local interpretable model-agnostic explanations; CIU: contextual importance and utility.  Identity: Separability: Fidelity: Speed: Stability: Monotonicity: Non-sensitivity: Effective complexity: Global interpretability In this section, we present a comprehensive examination of various global explanation methods evaluated across a diverse array of datasets, as highlighted in Table 4 Identity: Separability: Compactness: Correctness: Entropy ratio: Kullback-Leibler divergence: Gini coefficient: Discussion The benchmarking results reveal distinct trade-offs among the evaluated interpretability methods. RuleFit and RuleMatrix demonstrated strong performance in reproducibility, fidelity, and correctness, which are critical for deployment in clinical and regulatory contexts. However, their explanations often exhibited higher complexity, which may increase the cognitive effort required for interpretation. Anchor offered competitive correctness and fidelity but showed notable limitations in reproducibility and latency. These constraints can reduce its applicability in time-sensitive or reliability-critical settings. LIME and CIU stood out in terms of separability and low effective complexity, making them attractive for settings where simplicity and differentiation between instances are important. LIME and CIU often scored 0% in identity, indicating a lack of consistency in the explanations. LIME and Anchor often scored 0% in global identity as well, which raises concerns about the reproducibility and reliability of the explanations these methods generate in practice. These findings underscore that no explanation method dominates across all metrics or use cases. The appropriate choice depends on the application’s interpretability demands, whether prioritizing auditability, simplicity, model alignment, or computational efficiency. Finally, our evaluation does not incorporate human-centered assessments such as usability or clinician feedback. This limits our ability to judge the practical effectiveness of explanations in real-world decision-making. Future work should incorporate expert-in-the-loop studies and assess performance on diverse data modalities. Table 4. Metric scores for different global explainability techniques on tabular data. Dataset name Method Identity (%) Separability (%) Compactness (%) Correctness (%) Entropy ratio Kullback-Leibler divergence Gini coefficient Thyroid LIME 2.13 48.11 91.49 42.28 0.68 1.24 0.00035 Anchor 0.00 24.74 95.00 92.07 0.71 0.86 0.00170 RuleFit 100 22 80 94.01 0.70 0.83 0.00230 RuleMatrix 100 20 90 93.34 0.71 0.79 0.00210 Heart disease LIME 0.00 21.68 92.50 32.50 0.82 0.65 0.00038 Anchor 0.00 24.73 92.31 97.94 0.87 0.35 0.00261 RuleFit 100 17 90 98.02 0.84 0.20 0.00311 RuleMatrix 100 18 91.3 96.45 0.83 0.21 0.00365 Hepatitis LIME 2.22 26.67 97.78 40.00 0.78 0.84 0.00032 Anchor 0.00 37.36 61.11 97.84 0.93 0.20 0.00103 RuleFit 100 30 61 98.6 0.79 0.12 0.00212 RuleMatrix 100 35 60 97.45 0.80 0.11 0.00263 Echocardiogram LIME 0.00 72.18 70.59 26.47 0.78 0.77 0.00054 Anchor 0.00 48.31 88.89 98.27 0.79 0.47 0.00624 RuleFit 100 45 65.5 99.4 0.70 0.31 0.00671 RuleMatrix 100 43 69.3 99.01 0.71 0.32 0.00723 Breast Cancer Wiscousin (original) LIME 8.33 48.71 95.83 33.33 0.66 1.07 0.00129 Anchor 0.00 40.11 87.50 99.59 0.84 0.33 0.00627 RuleFit 100 39 86.5 93.03 0.86 0.29 0.00821 RuleMatrix 100 35 80 92.43 0.88 0.21 0.00917 SPECT heart LIME 4.54 11.73 86.36 50.00 0.85 0.57 0.00028 Anchor 0.00 58.10 86.36 99.68 0.91 0.29 0.00079 RuleFit 100 10 86 99.35 0.88 0.18 0.00139 RuleMatrix 100 9.72 85 99.40 0.87 0.19 0.00109 Diabetes LIME 5.74 18.75 91.95 46.37 0.73 1.17 0.00001 Anchor 0.00 20.89 78.05 95.31 0.88 0.43 0.00030 RuleFit 100 17 75 94.5 0.80 0.23 0.00041 RuleMatrix 100 16 77.31 93.1 0.83 0.35 0.00039 Diabetic retinopathy Debrecan LIME 0.00 37.52 66.18 28.38 0.63 1.55 0.00018 Anchor 0.00 26.58 68.42 96.93 0.95 0.16 0.00088 RuleFit 100 25 65.12 97.1 0.72 0.13 0.00089 RuleMatrix 100 21 66.1 98.3 0.72 0.14 0.00091 HIV-1 protease cleavage LIME 0.00 21.69 81.25 16.88 0.91 0.44 0.00002 Anchor 0.00 36.75 62.50 87.72 0.95 0.11 0.00401 RuleFit 100 20 55.3 88.21 0.92 0.09 0.00451 RuleMatrix 100 20.5 53.21 85.3 0.93 0.11 0.00501 Heart failure clinical records LIME 0.00 61.13 97.37 31.58 0.76 0.89 0.00047 Anchor 0.00 46.58 50.00 97.64 0.80 0.49 0.00368 RuleFit 100 50 40 96.03 0.80 0.30 0.00467 RuleMatrix 100 52 35 98.01 0.83 0.25 0.00511 LIME: local interpretable model-agnostic explanations. Conclusion This study quantitatively evaluates local (LIME, CIU, RuleFit, RuleMatrix, and ILIME) and global (LIME, Anchor, RuleFit, and RuleMatrix) explanation methods using healthcare datasets. Among global explainability techniques, RuleFit and RuleMatrix consistently perform well, providing clear and robust model interpretations. In contrast, LIME, which prioritizes simplicity, exhibits lower correctness, indicating potential misalignment with the underlying model. For local explainability, RuleFit and RuleMatrix excel in identity, stability, and fidelity metrics, demonstrating strong alignment with model behavior. The variation in performance across different metrics highlights the need for nuanced method selection, emphasizing the importance of prioritizing criteria based on the specific application and carefully considering trade-offs between explainability dimensions. Despite the comprehensive scope of this study, several limitations warrant consideration. First, the analysis focuses on a specific subset of explainability techniques, primarily rule-based and surrogate model approaches. This excludes other prominent families of XAI methods, such as gradient-based saliency techniques and counterfactual explanations, which may behave differently under the same evaluation metrics. Second, the experimental evaluation is limited to structured, tabular datasets derived from clinical settings. Consequently, the generalizability of the findings to unstructured data domains (e.g. medical imaging, clinical notes, or time-series data) remains unexamined. Third, the study emphasizes quantitative assessment metrics—including identity, fidelity, and stability—but does not incorporate human-centered evaluations. In particular, the cognitive and practical interpretability of the generated explanations from a domain expert’s perspective (e.g. clinicians or healthcare practitioners) is not assessed, which is critical for real-world deployment. Lastly, the current evaluation does not explore the computational cost or scalability of the methods, factors that may influence their feasibility in time-sensitive or resource-constrained environments. Furthermore, it is important to acknowledge that not all explanations are equivalent in purpose, granularity, or interpretive utility. Different XAI methods offer fundamentally distinct forms of explanations—ranging from rule-based abstractions to local perturbation-driven approximations—each suited to different users and contexts. For instance, a high-fidelity explanation may not be understandable to a non-technical audience, while a more intuitive explanation may oversimplify model behavior. These epistemic trade-offs highlight the need for incorporating expert-in-the-loop validation protocols to assess explanation relevance, plausibility, and actionability from a domain-specific perspective. In healthcare, engaging clinicians to validate whether the explanations align with clinical reasoning and decision-making pathways is essential. Such human-centered evaluations are a critical next step for ensuring the practical reliability and ethical deployment of explainable AI in high-stakes environments. Acknowledgements Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2025R756),Princess Nourah bint Abdulrahman University, Riyadh,Saudi Arabia. ORCID iD: https://orcid.org/0000-0002-5679-9099 Ethical approval: Contributorship: Funding: Increasing the knowledge intensity of Ida-Viru entrepreneurship The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article. Data availability statement: Guarantor: References 1 Shawi RE Al-Mallah MH Interpretable local concept-based explanation with human feedback to predict all-cause mortality J Artif Intell Res 2022 75 833 855 2 Chen T Wei J Huang H et al. A novel and interpretable approach to data augmentation for enhanced prediction of cutting tool remaining useful life J Comput Des Eng 2025 12 121 139 3 Tariq R Recio-Garcia JA Cetina-Quiñones AJ et al. Explainable artificial intelligence twin for metaheuristic optimization: double-skin facade with energy storage in buildings J Comput Des Eng 2025 12 16 35 4 Lee YR Jung SH Kang KS et al. Deep learning-based framework for monitoring wearing personal protective equipment on construction sites J Comput Des Eng 2023 10 905 917 5 Changdar S Bhaumik B De S Physics-based smart model for prediction of viscosity of nanofluids containing nanoparticles using deep learning J Comput Des Eng 2021 8 600 614 6 Guidotti R Monreale A Ruggieri S et al. A survey of methods for explaining black box models ACM Comput Surv (CSUR) 2018 51 1 42 7 Kurenkov A The gradient 2020 8 Chouldechova A Fair prediction with disparate impact: a study of bias in recidivism prediction instruments Big Data 2017 5 153 163 28632438 10.1089/big.2016.0047 9 Goodman B Flaxman S ICML workshop on human interpretability in machine learning (WHI 2016), New York, NY http://arxiv. org/abs/1606.08813 v1 10 Miller T Explanation in artificial intelligence: insights from the social sciences Artif Intell 2019 267 1 38 11 Molnar C Interpretable machine learning 2020 12 Yagin FH El Shawi R Algarni A et al. Metabolomics biomarker discovery to optimize hepatocellular carcinoma diagnosis: methodology integrating automl and explainable artificial intelligence Diagnostics 2024 14 2049 39335728 10.3390/diagnostics14182049 PMC11431471 13 Selvaraju RR Cogswell M Das A et al. Proceedings of the IEEE international conference on computer vision 14 Jain S Wallace BC arXiv preprint arXiv:190210186 2019 15 Yu K Guo X Liu L et al. Causality-based feature selection: methods and evaluations ACM Comput Surv (CSUR) 2020 53 1 36 16 Adadi A Berrada M Peeking inside the black-box: a survey on explainable artificial intelligence (XAI) IEEE Access 2018 6 52138 52160 17 Lundberg SM Lee SI Advances in neural information processing systems https://arxiv.org/abs/1705.07874 18 Sundararajan M Taly A Yan Q International conference on machine learning https://arxiv.org/abs/1703.01365 19 Friedman JH Popescu BE 2008 20 Ribeiro MT Singh S Guestrin C Proceedings of the AAAI conference on artificial intelligence 21 Žlahtič B Završnik J Blažun Vošner H et al. Transferring black-box decision making to a white-box model Electronics 2024 13 1895 22 Dwivedi R Dave D Naik H et al. Explainable ai (XAI): core ideas, techniques, and solutions ACM Comput Surv 2023 55 1 33 23 ElShawi R Sherif Y Al-Mallah M et al. Interpretability in healthcare: a comparative study of local machine learning interpretability techniques Comput Intell 2021 37 1633 1650 24 Honegger M arXiv preprint arXiv:180805054 2018 25 Elshawi R Al-Mallah MH Sakr S On the interpretability of machine learning-based model for predicting hypertension BMC Med Inform Decis Mak 2019 19 1 32 31357998 10.1186/s12911-019-0874-0 PMC6664803 26 Buckmann M Joseph A Robertson H 2022 27 Albini E Long J Dervovic D et al. Proceedings of the 2022 ACM conference on fairness, accountability, and transparency 28 Ribeiro MT Singh S Guestrin C Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining 29 ElShawi R Sherif Y Al-Mallah M et al. Advances in databases and information systems: 23rd European conference, ADBIS 2019, Bled, Slovenia, September 8–11, 2019, Proceedings 23 30 Craven M Shavlik J Extracting tree-structured representations of trained networks Adv Neural Inf Process Syst 1995 8 24 30 31 Samek W Montavon G Vedaldi A et al. Explainable AI: interpreting, explaining and visualizing deep learning, volume 11700 Cham Springer Nature 2019 32 Carvalho DV Pereira EM Cardoso JS Machine learning interpretability: a survey on methods and metrics Electronics 2019 8 832 33 Arrieta AB Díaz-Rodríguez N Del Ser J et al. Explainable artificial intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI Inf Fusion 2020 58 82 115 34 Guidotti R Monreale A Giannotti F et al. Factual and counterfactual explanations for black box decision making IEEE Intell Syst 2019 34 14 23 35 Alvarez Melis D Jaakkola T Towards robust interpretability with self-explaining neural networks Adv Neural Inf Process Syst 2018 31 7775 36 Silva W Fernandes K Cardoso MJ et al. Understanding and interpreting machine learning in medical image computing applications: first international workshops, MLCN 2018, DLF 2018, and iMIMIC 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16–20, 2018, Proceedings 1 10.1007/978-3-030-02628-8_9 PMC7577585 33094293 37 Doshi-Velez F Kim B arXiv preprint arXiv:170208608 2017 38 Doshi-Velez F Kortz M Budish R et al. arXiv preprint arXiv:171101134 2017 39 Doshi-Velez F Kim B Explainable and interpretable models in computer vision and machine learning 40 Anjomshoae S Kampik T Främling K IJCAI-PRICAI 2020 workshop on explainable artificial intelligence (XAI), January 8, 2020 41 Främling K International workshop on explainable, transparent autonomous agents and multi-agent systems 42 Anjomshoae S Främling K Najjar A Explainable, transparent autonomous agents and multi-agent systems: first international workshop, EXTRAAMAS 2019, Montreal, QC, Canada, May 13–14, 2019, Revised Selected Papers 1 43 Ming Y Qu H Bertini E Rulematrix: visualizing and understanding classifiers with rules IEEE Trans Vis Comput Graph 2018 25 342 352 10.1109/TVCG.2018.2864812 30130210 44 Yang H Rudin C Seltzer M International conference on machine learning 45 Nguyen AP Martínez MR arXiv preprint arXiv:200707584 2020 46 Sprent P Smeeton NC Applied nonparametric statistical methods CRC Press 2016 47 Ylikoski P Kuorikoski J Dissecting explanatory power Philos Stud 2010 148 201 219 48 Munoz C da Costa K Modenesi B et al. arXiv preprint arXiv:230212094 2023 49 Kulesza T Stumpf S Burnett M et al. 2013 IEEE symposium on visual languages and human centric computing 50 Murdoch WJ Singh C Kumbier K et al. Definitions, methods, and applications in interpretable machine learning Proc Natl Acad Sci 2019 116 22071 22080 31619572 10.1073/pnas.1900654116 PMC6825274 51 Bhatt U Weller A Moura JM arXiv preprint arXiv:200500631 2020 52 UCI Machine Learning Repository http://www.ics.uci.edu/-mlearn/MLRepository.html 53 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/thyroid+disease 54 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Heart+Disease 55 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Hepatitis 56 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Echocardiogram 57 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original) 58 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/SPECT+Heart 59 Strack B et al. Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000 clinical database records Biomed Res Int 2014 2014 781670 24804245 10.1155/2014/781670 PMC3996476 60 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set 61 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/HIV-1+protease+cleavage 62 UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records 63 Feurer M, Klein A, Eggensperger K, et al. Efficient and robust automated machine learning. Adv Neural Inf Process Syst ",
  "metadata": {
    "Title of this paper": "Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000 clinical database records",
    "Journal it was published in:": "Digital Health",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12480807/"
  }
}
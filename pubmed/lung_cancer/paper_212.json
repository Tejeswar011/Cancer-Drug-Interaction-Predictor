{
  "title": "Paper_212",
  "abstract": "pmc BMC Med Inform Decis Mak BMC Med Inform Decis Mak 42 bmcmidm BMC Medical Informatics and Decision Making 1472-6947 BMC PMC12490045 PMC12490045.1 12490045 12490045 10.1186/s12911-025-03179-1 3179 1 Research Cross- & multi-lingual medication detection: a transformer-based analysis Raithel Lisa raithel@tu-berlin.de 2 3 4 Frei Johann johann.frei@informatik.uni-augsburg.de 1 Thomas Philippe 4 Roller Roland 4 Zweigenbaum Pierre 5 Möller Sebastian 2 4 Kramer Frank 1 1 https://ror.org/03p14d497 grid.7307.3 0000 0001 2108 9006 Universität Augsburg, 2 https://ror.org/03v4gjf40 grid.6734.6 0000 0001 2292 8254 Quality & Usability Lab, Technische Universität Berlin, 3 https://ror.org/05dsfb086 0000 0005 1089 7074 BIFOLD – Berlin Institute for the Foundations of Learning and Data, 4 https://ror.org/01ayc5b57 grid.17272.31 0000 0004 0621 750X DFKI GmbH, 5 grid.530787.e 0000 0005 0806 4815 Laboratoire interdisciplinaire des sciences du numérique (LISN), Université Paris-Saclay, 2 10 2025 2025 25 478029 359 5 8 2024 29 8 2025 02 10 2025 03 10 2025 03 10 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by/4.0/ Open Access http://creativecommons.org/licenses/by/4.0/ Extracting specific information, such as medication mentions, from large unstructured medical texts can be challenging, especially when no annotated corpus exists in the target language for training. To overcome this, leveraging existing machine learning models and datasets is essential, and since most pre-trained resources are in English, adopting multilingual approaches can help transferring between languages. In this work, we investigate the usage of a multi-lingual transformer model in a multi-lingual and cross-lingual setting to extract drug names from medical texts using named entity recognition in four European languages: German, English, French, and Spanish. We report the scores obtained by cross-lingual transfer with several published datasets after fine-tuning a multi-lingual model, aiming to create empirical evidence on how the transfer of “medical” knowledge between languages can be expected to benefit various language pairs. We further perform a qualitative error analysis and find that the performance on all languages achieves competitive levels. Conversely, erroneous prediction artifacts are introduced by annotation inconsistencies, differences in annotation guidelines and vague entity labels in general. Supplementary Information The online version contains supplementary material available at 10.1186/s12911-025-03179-1. Keywords Natural language processing Information extraction Medication detection Multi-linguality Universität Augsburg (3144) Open Access funding enabled and organized by Projekt DEAL. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © BioMed Central Ltd., part of Springer Nature 2025 Introduction Recent developments in Natural Language Processing (NLP), like the publication and democratization of transformer models [ 1 2 3 4 1 F 1 Especially in classical, mono-lingual-based approaches, a low resource setting, or even a lack of matching annotated corpora for a certain target language is not uncommon. Our work investigates the use of multilingual medication detection for German (de), French (fr), and Spanish (es), in addition to English (en) in the medical context. The core question that we address concerns the extent to which existing data and multi-lingual models can improve the situation in non-English medical NLP settings. In practical terms: Which performance can we expect if we rely on an annotated corpus from a source language as training data and apply the subsequently trained model on a target language? Finding evidence-based answers to this research question has clear practical implications. (i) Multi-lingual models facilitate common interoperability across multiple languages and, for instance, can be more effective in actual deployments since a multi-lingual base model does not need to be swapped if the input language is changed, in contrast to mono-lingual base models. (ii) Utilizing multi-lingual models allows the composition of data sources from multiple languages and therefore, it mitigates data scarcity in the medical context. (iii) In cases of such dataset scarcities in certain languages, cross-lingual approaches might be able to bridge the knowledge from a dataset in a source language to be applied in a target language. Our experimental study design focuses on the cross-lingual capabilities of masked language models, which offer a suitable architecture for NER tasks. While much attention has been dedicated to large language models (LLMs) recently, these causal language models substantially differ in conceptual and practical terms, rendering a fair and exhaustive comparison of LLMs and BERT models challenging. Therefore, the scope of this study does not include LLM-driven medication detection methodologies. To this end, we employ a multi-lingual transformer model for medication detection. It facilitates our evaluation setup since drug-related labeled data from different languages and different medical corpora are available and hence, multiple cross-lingual transfer settings can be conducted. We report precision, recall, and F 1 Related work In recent years, transformer-based approaches have achieved strong results in common language modeling tasks [ 1 5 5 6 7 The properties and capabilities of multi-lingual models have been analyzed in several works. For instance, Pires et al. [ 8 9 10 11 Since multi-lingual networks can process data across different languages through shared weights without explicit parallel corpora [ 8 12 13 In the medical and biomedical domain, Catelli et al. [ 14 15 16 17 6 18 19 20 21 22 Rather than only relying on cross-lingual transfer, the re-use of datasets from other languages through translation and annotation projection is considered. Concerning the question to which degree translation and annotation projection-based language shifts can outperform cross-lingual training, Gaschi et al. [ 23 24 Large Language Models (LLMs) may also be used for medical-related tasks such as drug detection, taking advantage of their large training corpus and model size in multi-lingual tasks. However, LLM-based NER using few-short learning is reported to still perform inferior to masked language models [ 25 26 Material and methods Throughout this work, we refer to cross-lingual multi-lingual If possible, we use not only one dataset per language but several to avoid overfitting on a particular kind of data or annotation style and to increase the model’s robustness on different text styles. Based on preliminary experiments, we select XLM-RoBERTa [ 7 Our approach investigates three perspectives. First, we fine-tune XLM-R mono-lingually as a mono-lingual reference. Second, we measure the performance in the joint multi-lingual setting by fine-tuning across all datasets from different languages. Third, we evaluate different combinations of training and test sets that allow us to quantify cross-lingual strengths and weaknesses for different languages. To provide further insights to our quantitative analysis, we also report and discuss observed artifacts and patterns in the qualitative analysis counterpart. Data overview The presented corpora are selected because of the languages they represent and their respective annotations of medical entities. In particular, we are interested in medication names or other closely related types, such as substances. However, available medical datasets in languages other than English are limited, so we choose two Germanic (English and German) and two Romance (French and Spanish) languages and collect the corpora to which we were permitted access. In particular, for the data, we consider medication names (and chemicals) used in medical texts, e.g., patient records. Usually, there is only one label per dataset dedicated to the desired expressions; sometimes, however, these labels cover a broader scope than only drug names, which is an inherent limitation when dealing with diverse datasets. As pointed out earlier, we harmonize all corpus-specific label classes we are interested in into one common label class. Such label classes are highlighted in bold German Datasets Spanish Datasets 1 Table 1 The dataset statistics. The data was tokenized using SpaCy [ 27 Dataset # Tokens (overall) # Labels (drug-related) de BRONCO150 83,551 1,630 GERNERMED 21,678 1,450 GGPONC 2.0 2,005,183 23,671 Ex4CDS 2.0 4,356 98 en CMED 472,114 8,993 fr Quaero 79,706 3,537 DEFT 284,111 1,337 es PharmaCoNER 406,316 4,448 CT-EBM-SP 355,443 9,224 Total 3,712,458 54,388 German datasets BRONCO150 [ 28 The Berlin-Tübingen Oncology Corpus 2 medication 29 30 31 3 28 GERNERMED [ 32 This corpus 4 33 Drug GGPONC v2.0 [ 34 This datatset 5 Substance 6 Ex4CDS [ 35 The dataset 7 Medication English dataset CMED [ 36 CMED 8 37 39 Disposition NoDisposition Undetermined 9 French datasets DEFT [ 40 The DEFT corpus 10 41 substance 40 Quaero [ 42 The Quaero French Medical Corpus 11 43 Chemical 44 Spanish datasets PharmaCoNER [ 45 This corpus, 12 Normalizables 13 No_Normalizables CT-EBM-SP [ 46 The Clinical Trials for Evidence-Based Medicine in Spanish corpus 14 Chemical 46 In summary, we collected four German, one English, two French, and two Spanish datasets. All of these are based on similar, but not identical annotation guidelines and annotate entities that exhibit varying levels of semantic overlap with medication names. Note that although the guidelines might be comparable, the data were created with different goals in mind, by different annotators and in different settings. Therefore, the scope of the annotated entities might vary or include or exclude particular expressions. Pre-processing All datasets are split into a training, development, and test set. In some cases (CMED, CT-EBM-SP, PharmaCoNER, Quaero), these splits were already given; the remaining corpora are split into 70% training, 15% development, and 15% test set. If possible, the data were split on document level, otherwise, e.g., in the case of BRONCO150, where several documents (sentences) were assigned to only five files, we take three files as training data and the remaining two as development and test sets. Code for data pre-processing, fine-tuning of models, and the error analysis is available online. 15 First, all datasets are converted into the BRAT [ 47 16 17 48 49 Experimental setup We rely on the base model of XLM-R [ 7 F 1 large 5 As outlined in the Section “ Introduction Discussion The predictions of the five resulting models on the test set are ensembled via a majority vote and evaluated against the gold standard data. After applying the fine-tuned models for inference, the resulting IOB sequences are converted back to BRAT format, allowing 1) some automated label corrections and 2) an easy and consistent evaluation using the n2c2 2022 evaluation script. 18 Mono-lingual fine-tuning In this setup, we fine-tune five XLM-R models on each language separately and evaluate the models on the complete test set bench of languages using the ensembled predictions. Hereby, the mono-lingual reference score as well as the cross-lingual transfer to other languages are tracked. We abbreviate these models as mono language mono de Joint multi-lingual fine-tuning The fine-tuning on all datasets across all languages constitutes the highest degree of joint multi-lingual training. We abbreviate this experiment setup to all. For this premise, we expect the model to learn a shared representation of medication names across languages by taking into account the different (language) contexts and annotation styles. While one can assume that this method may achieve lower scores due to the divergences across languages and datasets, the resulting model may be more robust in terms of dataset shifts, and it might be able to pick up, for instance, syntactic constructions that are evident in one dataset but not in another. Fine-tuning on language pairs The experiments of this group are based on the assumption that similar languages might learn from each other. Therefore, we combine the English and German training data (de+en, both are Germanic languages) into one dataset and the Spanish and French training data into one dataset (fr+es, both are Romance languages). The training data are selected to adequately counter imbalances in different scales of abundance of data samples within a language pair. Merging the datasets from two linguistically related languages for a joint language-pair-specific training can be considered to constitute a middle ground between pure mono-lingual setups and the joint multi-lingual setup. These language-pair setups are relevant to investigate whether it is practical to stay within one language family instead of focusing on including also more remotely related languages. Results Every model is evaluated on the same test set bench, containing examples of all languages and datasets (all) as well the language-specific subsets separately. To mitigate disagreement issues on exact span borders, we compute the scores for P R \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{F_1}$$\\end{document} 19 To provide a comprehensive yet condensed overview of our obtained results, the F 1 2 Table 2 F 1 Mono-lingual setups With regard to the mono-lingual setup, three aspects are of particular interest: How well can a mono-lingual model learn to detect medication names within its own language? Which mono-lingual model performs well when it is required to transfer across all languages? Can mono-lingual models be used to transfer to certain other languages particularly well through the cross-lingual abilities of the XLM-R base model? Addressing the first point, as for the mono-lingual models in general, we find, not surprisingly, that the best model for one language is always the one trained on this language. Regarding the second aspect, we observe that the model trained only on German data achieves the best lenient F 1 F 1 Further, it is interesting to see that the performance of model_en is lower compared to the other models when applied to the Spanish data ( \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_1 = 0.67$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_1 = 0.49$$\\end{document} F 1 Concerning the cross-lingual transfer of mono-lingually trained models, it appears that German and Spanish are best suited to be used for German, Spanish, or English cross-lingual transfer. While the German and Spanish language resources consist of more than one corpus per language, English only uses one dataset. This leads to the observed overfitting effect of the English mono-lingual model, which performs best within its own language domain at the expense of its cross-lingual scores on the German and Spanish test sets. In reverse, both German and Spanish mono-lingual models show rather evenly balanced scores in transfer setups. Joint multi-lingual setup We discuss the results in the joint multi-lingual setup both on the language level and on the dataset level. Language-level scores We find that the results per language are slightly below those of the “mono-lingual” setups, but only by a small margin (de: 1% point, en: 1.9, fr: 1.5, es: 1.1). In short, this indicates that for our particular use case, there is no clear evidence that could prove a clear multi-lingual benefit over a mono-lingual when it comes to improving the scores for a certain target language. Dataset-level scores Going beyond the language-level analysis, we evaluate the joint multi-lingual model on each dataset independently. This indicates which datasets and implicitly which annotation guidelines are covered well by the jointly trained multi-lingual NER model. Table 3 Table 3 Dataset-dependent scores achieved by the joint multi-lingual model. The first part of the second column denotes the language (e.g., “de”), and the second part the dataset (e.g., “BRONCO150”). “train” denotes the data the model was fine-tuned on, “test” stands for the data the model was evaluated on. p R Train Test Language Test Corpus P R \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbf{F_1}$$\\end{document} all de BRONCO150 0.845 0.888 0.866 all de Ex4CDS 0.714 0.294 0.417 all de GERNERMED 0.944 0.886 0.914 all de GGPONC 0.830 0.868 0.848 all en CMED 0.907 0.954 0.930 all es CT-EBM-SP 0.921 0.929 0.925 all es PharmaCoNER 0.755 0.885 0.815 all fr DEFT 0.186 0.568 0.281 all fr Quaero 0.889 0.599 0.716 The results of this multi-lingual model show two outliers: The F 1 For the other datasets, the results appear to be quite good: The lowest F 1 F 1 Language pair-based multi-lingual scores When merging the languages by language families, Romance and Germanic, we can see several interesting results as well. First, the de+en cluster achieves the second best scores both on German and English, better than using the all model. The same holds true for fr+es model evaluated on French data. Further, the scores on Spanish are strongly reduced when using only de+en data, but when we compare the scores of de+en, fr+es and all, it seems like there is still some information gain from adding German and English examples to the French and Spanish data. This is evident in the improvement of the precision score by 2.4% points. Error analysis The following is a mostly qualitative analysis of the predictions of the joint multi-lingual model. We highlight false positives (FPs) and false negatives (FNs) of interest and categorize them into groups for a more insightful overview. The counts of FP and FNs per language are reported in Table 4 Table 4 Number of false positives (FP) and false negative (FN) samples. The individual samples are used for the qualitative error analysis Language #FP #FN German 376 382 English 113 63 French 298 175 Spanish 287 142 total 1,074 763 unique total 977 755 This might be an artifact of the different annotation guidelines of the various datasets: Since they were all built with different task objectives in mind, some drug occurrences might be annotated for some datasets but not for others, providing the model only with an unstable training signal on how to treat these occurrences. Analysis of false positives FPs are text spans that were predicted as (part of) a drug name but are not correct according to the respective dataset’s ground truth annotation. During the qualitative analysis of the FP samples, we identify two notable error sources: Annotation errors Out of the collected false positive samples, several can be considered as true Investigating the occurrences of the entities, we find that “Rivotril” only occurs in the Spanish training data and in no other dataset. “Paroxetine”, however, can be found in the training data of GGPONC and GERNERMED (“Paroxetin”), CMED (“Paroxetine”), PharmaCoNER and CT-EBM-S (“paroxetina”) and even in DEFT (“paroxétine”). Similar examples from German would be “Dopamin” (GGPONC) or “Metamizol” (BRONCO150, GGPONC), both were not labeled in the ground truth in some cases. However, we could verify them to be present in the training sets of GGPONC, PharmaCoNER, BRONCO150 and CT-EBM-SP. Consequently, we assume these to be annotation errors or entities that were not relevant for the respective corpus for some reason. 20 Groups of other medical terms In the FPs across all languages and datasets, we can find terms that belong to specific groups. These groups and their members often have medical associations, but are not medications themselves. However, their medical “context” might be a reason for their prediction. Some of the most visible groups are proteins (de: “Cyclin E”, en: “Creatine Kinase”, fr: “PHOSPHOMONOESTÉRASE”, es: “proteína C”), chemical compounds (de: “Dinitrotoluol”, en: “phosphate”, fr: “D-glycosylamines”, es: “fósforo”), abbreviations (de: “HLA”, en: “ASA”, fr: “STH”, es: “PTH”), general medication classes (de: “Medikation”, en: “pain medication”, es: “narcóticos”), medical terms and tools (de: “Gewebsflüssigkeit”, en: “Tegaderm”, fr: “solution”, es: “concentrado”), and dietary supplements (de: “Vitamin C”, en: “B12”, es: “calcio”). A reason for these predictions might be the label definitions of the different datasets. Some of them, e.g., Quaero and PharmaCoNER, include enzymes or chemical substances in their respective labels and the model is, apparently, not overfitting on any of those datasets. Also, the mentioned expressions are all used in very similar or even the same context as drugs, and therefore, the model might not be able to distinguish them semantically from medications. Summarizing the analysis of FPs, we observe that most of the incorrectly detected expressions can be categorized into a particular group. Most of these classes can be associated with medicine, medical treatments or other things related to a clinical setting. Some FPs are simply based on annotation errors or on small differences in the dataset guidelines (e.g., “CHEM” versus “Medication”). Analysis of false negatives Similar to the analysis of FPs, we now focus on entities that were classified as medication names according to their ground truth but were not detected by the multi-lingual model. During our analysis we identify the following group categories covering most FN entities: therapies abstract medication terms brand names medications with imprecise spans 2 ambiguous or weak terms In contrast to that, we also encounter a few very long spans, e.g., “orale Supplemente mit Omega-3-Fettsäuren” (de, oral supplements with omega-3 fatty acids new anti-tuberculosis treatment polyvalent F(ab’)2 antivenom Finally, most FNs seem to be actual medication names (e.g., de: “Avelumab”, en: “LISINOPRIL”, fr: “Atripla”, es: “folato”) that were simply not detected by the model. The reason might be that some of these drugs (e.g., “Atripla”) were never seen in any training examples, or, in case they were seen, the context in the test example did not match the one the model was trained on. General observations We conclude the qualitative error analysis with some general observations regarding the predicted entities. Volatile span length The model seems to have difficulties in deciding the span length of an entity. In terms of scores, this is ignored by the permissive overlap mode, but some true positives are conspicuously longer than they need to be from the perspective of annotating medication names. This might be due to the strikingly different span lengths across the training datasets: In GGPONC, PharmaCoNER, CT-EBM-SP, Quaero, and DEFT we have at least four medication names that are longer than four tokens, in the case of GGPONC there are 812 medications that are longer than four tokens. Also in GGPONC, DEFT and CT-EBM-SP we can still find several entities with a span longer than ten tokens. Examples from the German data are “fettlöslichen Vitaminen” ( fat-soluble vitamins oral medication heavy sedatives Treatment versus medication In several instances, there seems to be a disagreement between the terms of treatment medication Inconsistent annotations within datasets We also encounter occurrences within datasets where the annotation might be misleading. For example, in one of the German datasets our system predicts both “Substanzen” ( substance single substances Overlap between false positives and false negatives There are overall 59 expressions across all languages and datasets that are included in the FPs, but also in the FNs. Often, these belong to certain groups as specified above, e.g., general medication names (e.g., “medicación”), dietary supplements (e.g., “Magnesium”), or abbreviations (“ARV”). All of them, however, have a clear medical association. Their occurrence in both FPs and FNs may be a result of the different underlying guidelines or contexts, and there may be some annotation errors involved as well. However, it also demonstrates the difficulty of annotating medical texts and creating guidelines for the annotation. Unseen medications To make sure the model is not simply overfitting to individual medication names, we check for some true positives if they occur in any of the training sets. Indeed, we observe that there are several correctly predicted drugs that the model did not see during training. Examples are “Quixidar” (Quaero), and “rifampine” (DEFT). “Dexamethasone” is an interesting case: We can see that it was correctly predicted in both GERNERMED and GGPONC, but it never occurred like that in the training data. Instead, it was included in much longer spans, e.g. “für 3 Tage 5 mg Dexamethasone” ( for 3 days 5 mg Dexamethasone Discussion In our experiments, we show that the multi-lingual model achieves a F 1 F 1 As part of the initial research question, the findings on the cross-lingual transfer indicate that transfer across certain languages is in fact a viable solution. While we observed drops in performance, our findings indicate that conflicting annotation guidelines across different datasets might be a larger impeding factor than lossy effects during the cross-lingual transfer. For instance, the transfer of a mono-lingual French model to English yielded quite good scores already. Concerning language pairs, the results vary. Assuming there is no target language training material available, combining other languages for fine-tuning does indeed show good performance on the target language and also often performs better than fine-tuning only on one source language. Since this is not always the case, a thorough inspection of the available data might be necessary, to avoid the introduction of noise. For language pairs, languages from the same family seem to work better. On two datasets, Ex4CDS (de) and DEFT (fr), we find a lower performance when compared to the other corpora (0.41 and 0.28 F 1 With respect to both false positives and false negatives, we find error groups that are evident across all languages and across all datasets. We cannot We observe overlaps between false positives and false negatives in all languages and datasets. This hints at annotation inconsistencies, but also on very subtle differences that might depend on the exact context in which the entity in question was uttered. We argue that this is a normal phenomenon of manually annotated datasets, especially in a more complex domain like the medical domain. Since all the datasets used in this work are based on different annotation guidelines, it is no surprise that for some of the test sets, we find predictions that are evaluated as false positives. These might be correct for one dataset, but not for the other. However, this shows very clearly why it is important to take a look at the actual predictions and not only at the scores: If we would like to (semi-)automatically annotate a new medical and potentially multi-lingual dataset, these predictions would still be very useful. Also, as we have seen in some examples, even if a particular drug was not present in the training data, it can still be predicted correctly, based on context, but also based on its potential occurrence in the other datasets. Finally, the fact that a lot of medication names are very similar across the four investigated languages (e.g., compare “Paroxetin” vs. “Paroxetine” vs. “paroxétine” vs. “paroxetina”) is likely to have a positive impact on the drug detection task as well. This might change for drug names with different origins or when using datasets from other language families, and maybe more importantly, other scripts. However, inconsistencies within datasets and in label definitions across datasets might counterbalance these effects. The investigation of the influence of inconsistencies is, however, a task for future work. Regarding the use of LLMs for medical NER tasks, these models have demonstrated applicability in areas such as medication detection  [ 25 Conclusions In this work, we investigated the ability of the cross- and multi-lingual transfer-learning capabilities of the XLM-R model in the context of medication detection in different languages and datasets. We fine-tuned the model on mono-lingual, bi-lingual and multi-lingual datasets and evaluated their drug detection performance across all languages. While our results indicate that mono-lingual models perform best on their respective target language, multi-lingual-trained models can reach scores close to their mono-lingual counterparts. Due to their cross-lingual transfer, we demonstrated that multi-lingual models can be a relevant approach in low-resource contexts in order to tackle NLP tasks with non-native datasets even if no appropriate native dataset or no language-specific pre-trained model is available. An error analysis provided valuable insights into the mistakes the multi-lingual model makes when extracting medication names from unseen data. The found error groups allow further investigations into how these errors can be alleviated or even avoided, e.g., by more consistent annotation guidelines across languages. This stresses the need to strengthen the efforts towards more standardized, comparable and interoperable annotation guidelines in general. We also find indications that the model learns across dataset boundaries, taking into account drug names that were only present in another language’s dataset. More medical datasets and annotation data for extended evaluation of multi-lingual models could further improve the state of medical NLP in low-resource contexts, yet due to our scope this is considered future work. Furthermore, the usefulness of multi-lingual models in other language families (e.g., Arabic, Swedish, Ukrainian or Japanese) for the identical clinical purpose of drug detection remains open for further investigation. Electronic supplementary material Below is the link to the electronic supplementary material.  Supplementary Material 1: Model Parameters  Supplementary Material 2: Dataset Statistics  Supplementary Material 3: Verbose Scores 1 https://n2c2.dbmi.hms.harvard.edu/2022-amia-workshop 2 https://www2.informatik.hu-berlin.de/%7Eleser/bronco/index.html 3 http://www.dimdi.de/dynamic/de/arzneimittel/atc-klassifikation/ 4 https://github.com/frankkramer-lab/GERNERMED 5 https://www.leitlinienprogramm-onkologie.de/projekte/ggponc-english/ 6 Annotation guidelines of GGPONC: https://github.com/hpi-dhc/ggponc_annotation/blob/master/annotation_guide/anno_guide.pdf 7 https://github.com/DFKI-NLP/Ex4CDS 8 To the best of our knowledge, these data are not (yet) publicly accessible. 9 Note that for the CMED dataset, all medication labels in the test set are already mapped to “Drug”. Unfortunately, we do not know the exact definition of “Drug”. 10 https://deft.limsi.fr/2019/index-en.html 11 https://quaerofrenchmed.limsi.fr/ 12 https://temu.bsc.es/pharmaconer/ 13 For this dataset, the terms “chemical” and “drug” are used interchangeably. 14 http://www.lllf.uam.es/ESP/nlpmedterm_en 15 https://github.com/lraithel/cross_ling_drug_ner 16 brat2bio: https://github.com/spyysalo/standoff2conll https://github.com/nlplab/brat/blob/master/tools/BIOtoStandoff.py 17 https://github.com/lraithel/cross_ling_drug_ner/blob/main/src/eval_script.py 18 https://n2c2.dbmi.hms.harvard.edu/2022-track-1 19 Note that if there is nothing to annotate/predict in a document, the evaluation script returns 0.0 for all scores. 20 Note that some of the DEFT examples were only annotated partially. Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Lisa Raithel and Johann Frei contributed equally to this work. Acknowledgements We would like to thank the reviewers who contributed through critical feedback on the manuscript. We are grateful to the researchers who kindly allowed us to use their data for this study. We also acknowledge the German Research Center for Artificial Intelligence (DFKI) for providing computational resources that enabled the fine-tuning of models used in this work. Author contributions LR: Conceptualization, Model Training, Model Evaluation, Formal Analysis, Manuscript Writing. JF: Conceptualization, Dataset Pre-processing, Data Cleansing, Formal Analysis, Manuscript Writing and Revision. PT: Supervision, Manuscript Writing, Manuscript Review. RR: Supervision, Manuscript. Review. PZ: Supervision, Manuscript Review. SM: Supervision, Resources. FK: Supervision, Resources. All authors read and approved the final manuscript. Funding Open Access funding enabled and organized by Projekt DEAL. This research was funded by the German Ministry of Education and Research (Bundesministerium für Bildung und Forschung, BMBF) through the grant BIFOLD25B (LR) and the project DIFUTURE (FKZ01ZZ1804E) (JF), and by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the trilateral ANR-DFG-JST call for project KEEPHA (442445488) (RR, PT, PZ). Data availability All relevant code is available at the GitHub repository at https://github.com/lraithel/cross_ling_drug_ner Declarations Ethics approval and consent to participate Not applicable. Consent for publication Not applicable Competing interests The authors declare no competing interests. Abbreviations NLP Natural language processing NER Named entity recognition LLM Large language model ADR Adverse drug reactions IOB Inside-outside-beginning FP False positive FN False negative References 1. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. Adv Neural Inf Process Syst. 2017. Available from: 30. Curran Associates, Inc.; https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html 2. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: A pre-trained biomedical language representation Model for biomedical text mining. Bioinformatics (Oxford, England). 2020, Feb;36(4):1234–40. 10.1093/bioinformatics/btz682. 10.1093/bioinformatics/btz682 PMC7703786 31501885 3. Gu Y Tinn R Cheng H Lucas M Usuyama N Liu X Domain-specific language Model pretraining for biomedical natural language processing ACM Trans Comput healthc 2021 3 1 :2:1 2:23 10.1145/3458754 Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain-specific language Model pretraining for biomedical natural language processing. ACM Trans Comput healthc. 2021, Oct;3(1)::2:1–2:23. 10.1145/3458754. 4. Mahajan D Liang JJ Tsou CH Toward understanding clinical context of medication change events in clinical narratives AMIA Annu Symp Proc 2022 2021 833 42 35308981 PMC8861744 Mahajan D, Liang JJ, Tsou CH. Toward understanding clinical context of medication change events in clinical narratives. AMIA Annu Symp Proc. 2022;2021:833–42. 35308981 PMC8861744 5. Devlin J, Chang MW, Lee K, Toutanova KB. Pre-training of deep bidirectional transformers for language understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics; 2019. p. 4171–86. 6. Conneau A, Lample G. Cross-lingual language Model pretraining. Adv Neural Inf Process Syst. 2019. Available from: 32. Curran Associates, Inc.; https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html 7. Conneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, Guzmán F, et al. Unsupervised Cross-lingual representation learning at scale. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. 2020. p. 8440–51. Available from: https://www.aclweb.org/anthology/2020.acl-main.747 8. Pires T, Schlinger E, Garrette D. How multilingual is multilingual BERT? Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. 2019. p. 4996–5001. Available from: https://aclanthology.org/P19-1493 9. Wu S, Beto DM, Bentz. In: Inui K, Jiang J, Ng V, Wan X, editors. Becas: The surprising Cross-lingual effectiveness of BERT. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics;. p. 833–44. Available from: https://aclanthology.org/D19-1077 10. Chai Y, Liang Y, Duan N. Cross-lingual ability of multilingual masked language models: A study of language structure. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics; 2022. p. 4702–12. Available from: https://aclanthology.org/2022.acl-long.322 11. Al-Duwais M Al-Khalifa H Al-Salman A A benchmark evaluation of multilingual large language models for Arabic Cross-lingual named-entity recognition Electronics 2024 13 17 3574 10.3390/electronics13173574 Al-Duwais M, Al-Khalifa H, Al-Salman A. A benchmark evaluation of multilingual large language models for Arabic Cross-lingual named-entity recognition. Electronics. 2024;13(17):3574. Number: 17 Publisher: Multidisciplinary Digital Publishing Institute. 10.3390/electronics13173574. 12. Xie J, Yang Z, Neubig G, Smith NA, Carbonell J. Neural Cross-lingual named entity recognition with minimal resources. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics; 2018. p. 369–79. Available from: https://aclanthology.org/D18-1034 13. Chen S Pei Y Ke Z Silamu W Low-resource named entity recognition via the pre-training Model Symmetry 2021 13 5 786 10.3390/sym13050786 Chen S, Pei Y, Ke Z, Silamu W. Low-resource named entity recognition via the pre-training Model. Symmetry. 2021;13(5):786. Number: 5 Publisher: Multidisciplinary Digital Publishing Institute. 10.3390/sym13050786. 14. Catelli R Gargiulo F Casola V De Pietro G Fujita H Esposito M Crosslingual named entity recognition for clinical de-identification applied to a COVID-19 Italian data set Appl Soft Comput 2020 97 106779 10.1016/j.asoc.2020.106779 33052197 PMC7544600 Catelli R, Gargiulo F, Casola V, De Pietro G, Fujita H, Esposito M. Crosslingual named entity recognition for clinical de-identification applied to a COVID-19 Italian data set. Appl Soft Comput. 2020;97:106779. 10.1016/j.asoc.2020.106779. 33052197 10.1016/j.asoc.2020.106779 PMC7544600 15. Ding P Wang L Liang Y Lu W Li L Wang C Nah Y Cui B Lee S Yu J Moon Y Whang S Cross-lingual transfer learning for medical named entity recognition Database systems for advanced applications. Lecture notes in computer science 2020 Springer International Publishing 403 18 Ding P, Wang L, Liang Y, Lu W, Li L, Wang C, et al. Cross-lingual transfer learning for medical named entity recognition. In: Nah Y, Cui B, Lee S, Yu J, Moon Y, Whang S, editors. Database systems for advanced applications. Lecture notes in computer science. Springer International Publishing; 2020. p. 403–18. 16. World Health Organization. ICD-11: International classification of diseases 11th revision. World Health Organization. 2019. Available from: https://icd.who.int/ 17. Pollard TJ, Johnson AE. The MIMIC-III clinical database. Available from: 10.13026/C2XW26. 18. Purwitasari D, Abdillah AF, Juanita S, Purnomo MH. Transfer learning approaches for Indonesian biomedical entity recognition. 2021 13th International Conference on Information & Communication Technology and System (ICTS). 2021. p. 348–53. 19. Schwarz M, Chapman K, Häussler B. Multilingual medical entity recognition and Cross-lingual zero-shot linking with Facebook AI similarity search. Proceedings of the Iberian Languages Evaluation Forum (IberLEF). 2022); 2022. 20. Raithel L, Thomas P, Roller R, Sapina O, Möller S, Zweigenbaum P. In: Calzolari N, Béchet F, Blache P, Choukri K, Cieri C, Declerck T, et al., editors. Cross-lingual approaches for the detection of adverse drug reactions in German from a Patient’s perspective. Proceedings of the Thirteenth Language Resources and Evaluation Conference. European Language Resources Association;. p. 3637–49. Available from: https://aclanthology.org/2022.lrec-1.388 21. Shi K, Chen G, Gu J, Qian L, Zhou G. Cross-lingual name entity recognition from clinical text using mixed language query. In: Xu H, Chen Q, Lin H, Wu F, Liu L, Tang B, et al., editors. Health information processing. Springer Nature;. p. 3–21. 22. Zanoli R Lavelli A Verdi Do Amarante D Toti D Assessment of the E3C corpus for the recognition of disorders in clinical texts Nat Lang Eng 2024 30 4 851 69 10.1017/S1351324923000335 Zanoli R, Lavelli A, Verdi Do Amarante D, Toti D. Assessment of the E3C corpus for the recognition of disorders in clinical texts. Nat Lang Eng. 2024;30(4):851–69. 10.1017/S1351324923000335. 23. Gaschi F, Fontaine X, Rastin P, Toussaint Y In: Naumann T, Ben Abacha A, Bethard S, Roberts K, Rumshisky A, editors. Multilingual clinical NER: Translation or Cross-lingual transfer? Proceedings of the 5th Clinical Natural Language Processing Workshop. Association for Computational Linguistics;. p. 289–311. Available from: https://aclanthology.org/2023.clinicalnlp-1.34 24. Schäfer H, Idrissi-Yaghir A, Horn P, Friedrich C. In: Naumann T, Bethard S, Roberts K, Rumshisky A, editors. Cross-language transfer of high-quality annotations: Combining neural machine translation with cross-linguistic Span alignment to apply NER to clinical texts in a Low-resource language. Proceedings of the 4th Clinical Natural Language Processing Workshop. Association for Computational Linguistics;. p. 53–62. Available from: https://aclanthology.org/2022.clinicalnlp-1.6 25. Naguib M Tannier X Névéol A Al-Onaizan Y Bansal M Chen Y Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting Findings of the association for computational linguistics: EMNLP 2024 2024 Miami, Florida, USA Association for Computational Linguistics 6829 52 Naguib M, Tannier X, Névéol A. Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting. In: Al-Onaizan Y, Bansal M, Chen Y, editors. Findings of the association for computational linguistics: EMNLP 2024. Miami, Florida, USA: Association for Computational Linguistics; 2024. p. 6829–52. Available from: https://aclanthology.org/2024.findings-emnlp.400/ 26. Hu Y Chen Q Du J Peng X Keloth VK Zuo X Improving large language models for clinical named entity recognition via prompt engineering J Am Med Inf assoc 2024 31 9 1812 20 10.1093/jamia/ocad259 PMC11339492 38281112 Hu Y, Chen Q, Du J, Peng X, Keloth VK, Zuo X, et al. Improving large language models for clinical named entity recognition via prompt engineering. J Am Med Inf assoc. 2024, 01;31(9):1812–20. https://arxiv.org/abs/https://academic.oup.com/jamia/article-pdf/31/9/1812/58868277/ocad259.pdf https://academic.oup.com/jamia/article-pdf/31/9/1812/58868277/ocad259.pdf 10.1093/jamia/ocad259 PMC11339492 38281112 27. Honnibal M, Montani I spaCy Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. 2017, 2. 28. Kittner M Lamping M Rieke DT Götze J Bajwa B Jelas I Annotation and initial evaluation of a large annotated German oncological corpus JAMIA Open 2021 4 2 ooab025 10.1093/jamiaopen/ooab025 33898938 PMC8054032 Kittner M, Lamping M, Rieke DT, Götze J, Bajwa B, Jelas I, et al. Annotation and initial evaluation of a large annotated German oncological corpus. JAMIA Open. 2021, Apr;4(2):ooab025. 10.1093/jamiaopen/ooab025. 33898938 10.1093/jamiaopen/ooab025 PMC8054032 29. World Health Organization Collaborating Centre for Drug Statistics Methodology. Guidelines for ATC classification and DDD assignment. Available from: https://atcddd.fhi.no/atc_ddd_index_and_guidelines/guidelines/ 30. World Health Organization. ICD-10: International statistical classification of diseases and related health problems. World Health Organization. 2005. Available from: https://icd.who.int/browse10/2016/en 31. BfArM (federal institute for drugs and medical devices).: OPS - Operation and procedure keys. Available from: https://www.bfarm.de/EN/Code-systems/Classifications/OPS-ICHI/OPS/_node.html 32. Frei J Kramer F GERNERMED: An open German medical NER Model Softw Impacts 2022 11 100212 10.1016/j.simpa.2021.100212 Frei J, Kramer F. GERNERMED: An open German medical NER Model. Softw Impacts. 2022, Feb;11:100212. 10.1016/j.simpa.2021.100212. 33. Henry S, Buchan K, Filannino M, Stubbs A, Uzuner O. 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records. J Am Med Inf Assoc: JAMIA. 2020;27(1):3–12. 10.1093/jamia/ocz166. 10.1093/jamia/ocz166 PMC7489085 31584655 34. Borchert F, Lohr C, Modersohn L, Witt J, Langer T, Follmann M, et al. GGPONC 2.0 - the German clinical guideline corpus for oncology: Curation workflow, annotation policy, baseline NER taggers. Proceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022). 2022 Jun;p. 3650–60. 35. Roller R, Burchardt A, Feldhus N, Seiffe L, Budde K, Ronicke S, et al. An annotated corpus of textual explanations for clinical decision support. Proceedings of the Thirteenth Language Resources and Evaluation Conference. Marseille, France: European Language Resources Association; 2022. p. 2317–26. 36. Mahajan D, Liang JJ, Tsou CH. Toward understanding clinical context of medication change events in clinical narratives. AMIA Annual Symposium proceedings AMIA Symposium. 2021:833–42;2021. PMC8861744 35308981 37. Stubbs A, Kotfila C, Xu H, Ö U. Identifying risk factors for heart disease over Time: Overview of 2014 I2b2/UTHealth shared task track 2. J Educ Chang Biomed Inf. S67–77. 2015 Dec; 58 Suppl(Suppl). 10.1016/j.jbi.2015.07.001. 10.1016/j.jbi.2015.07.001 PMC4978189 26210362 38. Stubbs A, Kotfila C, Ö U. Automated systems for the De-identification of longitudinal clinical narratives: Overview of 2014 I2b2/UTHealth shared task track 1. J Educ Chang Biomed Inf. S11–19. 2015 Dec; 58Suppl(Suppl).10.1016/j.jbi.2015.06.007. 10.1016/j.jbi.2015.06.007 PMC4989908 26225918 39. Kumar V, Stubbs A, Shaw S, Ö U. Creation of a new longitudinal corpus of clinical narratives. J Educ Chang Biomed Inf. 2015 Dec; 58 Suppl(Suppl):S6–S10.10.1016/j.jbi.2015.09.018. 10.1016/j.jbi.2015.09.018 PMC4978168 26433122 40. Grouin C, Grabar N, Claveau V, Hamon T. Clinical case reports for NLP. Proceedings of the 18th BioNLP Workshop and Shared Task. Florence, Italy: Association for Computational Linguistics; 2019. p. 273–82. 41. Grabar N, Claveau V, Dalloux C. CAS: French corpus with clinical cases. Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis. Brussels, Belgium: Association for Computational Linguistics; 2018. p. 122–28. Available from: https://aclanthology.org/W18-5614 42. Névéol A, Grouin C, Leixa J, Rosset S, Zweigenbaum P. The QUAERO French medical corpus: A ressource for medical entity recognition and normalization. Proceedings of the Fourth Workshop on Building and Evaluating Ressources for Health and Biomedical Text Processing. 2014. p. 24–30. 43. Bodenreider O. The unified medical language system (UMLS): integrating biomedical terminology. Nucleic Acids Res. 2004;32:D267–D270. 10.1093/nar/gkh061. 10.1093/nar/gkh061 PMC308795 14681409 44. Bodenreider O McCray AT Exploring semantic groups through visual approaches J Educ Chang Biomed Inf 2003 36 6 414 32 10.1016/j.jbi.2003.11.002 PMC1997308 14759816 Bodenreider O, McCray AT. Exploring semantic groups through visual approaches. J Educ Chang Biomed Inf. 2003, Dec;36(6):414–32. 10.1016/j.jbi.2003.11.002. 10.1016/j.jbi.2003.11.002 PMC1997308 14759816 45. Gonzalez-Agirre A, Marimon M, Intxaurrondo A, Rabal O, Villegas M, Krallinger M. PharmaCoNER: Pharmacological substances, compounds and proteins named entity recognition track. Proceedings of the 5th Workshop on BioNLP Open Shared Tasks. Hong Kong. China: Association for Computational Linguistics; 2019. p. 1–10. 46. Campillos-Llanos L Valverde-Mateos A Capllonch-Carrión A Moreno-Sandoval A A clinical trials corpus annotated with UMLS entities to enhance the access to evidence-based medicine Bmc Med Inf Decis 2021 21 1 69 10.1186/s12911-021-01395-z PMC7898014 33618727 Campillos-Llanos L, Valverde-Mateos A, Capllonch-Carrión A, Moreno-Sandoval A. A clinical trials corpus annotated with UMLS entities to enhance the access to evidence-based medicine. Bmc Med Inf Decis. 2021, Dec;21(1):69. 10.1186/s12911-021-01395-z. 10.1186/s12911-021-01395-z PMC7898014 33618727 47. Stenetorp P, Pyysalo S, Topić G, Ohta T, Ananiadou S, Tsujii J. Brat: A web-based tool for NLP-Assisted text annotation. Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics. Avignon, France: Association for Computational Linguistics; 2012. p. 102–07. 48. Ef TKS, De Meulder F. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL. 2003;. p. 142–47. Available from: https://aclanthology.org/W03-0419 49. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. Transformers: State-of-the-art natural language processing. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics; 2020. p. 38–45. Available from: https://www.aclweb.org/anthology/2020.emnlp-demos.6 ",
  "metadata": {
    "Title of this paper": "A clinical trials corpus annotated with UMLS entities to enhance the access to evidence-based medicine",
    "Journal it was published in:": "BMC Medical Informatics and Decision Making",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12490045/"
  }
}
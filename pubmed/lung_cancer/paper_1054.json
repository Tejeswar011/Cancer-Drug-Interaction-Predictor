{
  "title": "Paper_1054",
  "abstract": "pmc NPJ Digit Med NPJ Digit Med 3605 npjdigitmed NPJ Digital Medicine 2398-6352 Nature Publishing Group PMC12475073 PMC12475073.1 12475073 12475073 41006898 10.1038/s41746-025-01965-9 1965 1 Article TIMER Cui Hejie 1 Unell Alyssa 2 Chen Bowen 3 Fries Jason Alan 1 Alsentzer Emily 2 3 Koyejo Sanmi 2 Shah Nigam H. nigam@stanford.edu 1 4 5 1 https://ror.org/00f54p054 grid.168010.e 0000000419368956 Stanford Center for Biomedical Informatics Research, Stanford University School of Medicine, 2 https://ror.org/00f54p054 grid.168010.e 0000 0004 1936 8956 Department of Computer Science, Stanford University, 3 https://ror.org/00f54p054 grid.168010.e 0000 0004 1936 8956 Department of Biomedical Data Science, Stanford University, 4 https://ror.org/019wqcg20 grid.490568.6 0000 0004 5997 482X Technology and Digital Services, Stanford Health Care, 5 https://ror.org/00f54p054 grid.168010.e 0000000419368956 Clinical Excellence Research Center, Stanford University School of Medicine, 26 9 2025 2025 8 478273 577 17 5 2025 17 8 2025 26 09 2025 28 09 2025 29 09 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access http://creativecommons.org/licenses/by-nc-nd/4.0/ Electronic health records (EHRs) contain rich longitudinal information for clinical decision-making, yet LLMs struggle to reason across patient timelines. We introduce TIMER T I M E R TIMER Subject terms Health care Computer science The Mark and Debra Leslie fund for AI in Healthcare https://doi.org/10.13039/100023581 National Science Foundation Graduate Research Fellowship Program NSF 23-605 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © Springer Nature Limited 2025 Introduction Language models (LMs) have recently shown the ability to process exceptionally long contexts-spanning hundreds of thousands of tokens-yet their capacity for temporal reasoning over longitudinal documents remains limited 1 2 3 4 Although biomedical LLMs have demonstrated promising performance on well-structured tasks such as medical knowledge retrieval and standardized exams 5 7 8 9 10 12 Instruction tuning has emerged as a key strategy in aligning models with desired objectives by adapting LLMs to domain-specific tasks via curated instruction-response pairs 13 14 15 16 These limitations expose a fundamental gap in clinical LLM development and evaluation. Our analysis of existing evaluation datasets reveals a pronounced temporal skew-for example, over 55% of questions in MedAlign targeting only the final 25% of patient timelines-failing to evaluate models’ capabilities to synthesize data over longitudinal records. To address the critical challenge of longitudinal reasoning in clinical LLMs, we introduce TIMER ( T I M E R Figure 1 Fig. 1 Overview of TIMER. TIMER enhances model performance through instruction tuning with timestamp-linked instruction-response pairs generated across longitudinal EHR timelines. Our evaluation employs both clinician-curated benchmarks and a controlled sampling strategy to create instruction sets with varying temporal distributions, enabling assessment of how models reason across different time periods in patient histories. Our findings demonstrate that TIMER-tuned models consistently outperform both general-purpose models and conventional medical instruction tuning approaches across multiple evaluation settings. Notably, while conventional medical instruction tuning (such as MedInstruct) sometimes degrades performance compared to the base model on clinician-curated benchmarks, TIMER consistently improves performance on both general clinical tasks and temporal reasoning tasks. Through detailed case studies and quantitative evaluation, we show that TIMER-tuned models have higher temporal boundary adherence (correctly limiting analyses to specified time periods), trend detection (identifying meaningful patterns in longitudinal data), and chronological precision (correctly ordering and contextualizing clinical events). Our exploration of temporal distribution effects reveals that aligning training and evaluation enhances performance, with distribution-matched training consistently outperforming misaligned approaches across all settings. Collectively, our findings demonstrate that instruction tuning with TIMER results in models that can effectively support longitudinal clinical care and complex medical decision-making in real-world scenarios. Results Study design and data source Figure 1 RQ1: Can temporally-grounded instruction-response pairs from EHR data improve LLMs’ longitudinal reasoning capabilities compared to conventional medical question-answer pairs? RQ2: How does the temporal distribution of instructions used in instruction-tuning affect model performance, specifically when we evaluate on varying temporal distributions? We used de-identified longitudinal EHRs from the Stanford Medicine Research Data Repository (STARR) 17 RQ1: Impact of temporal-aware instruction tuning We compared models instruction-tuned with TIMER against both standard medical LLMs and models tuned with conventional medical QA datasets to quantify the specific benefits of temporal awareness in instruction tuning. Overall performance We evaluate multiple medical LLMs, including Meditron-7B, MedAlpaca, AlpaCare, MMed-Llama-3-8B, PMC-Llama-13B, MedLM-Medium, and MedInstruct (Conventional QA-tuned Llama-3.1-8 B-Instruct), using MedAlign and a model-generated evaluation set, called TIMER-Eval, that requires temporal reasoning. When models cannot process full patient timelines, inputs are truncated to recent tokens of their context limits. As shown in Table 1 Table 1 Performance (%) of baseline models and TIMER -tuned models on MedAlign and TIMER-Eval benchmarks, reported as mean ± standard deviation from bootstrap resampling ( n Model name MedAlign TIMER-Eval LLM-as-Judge Automatic metrics LLM-as-Judge Automatic metrics Correct Complete BERTScore ROUGE-L Correct Complete BERTScore ROUGE-L Existing medical finetuned model Meditron-7B a 2.31 ± 1.65 0.99 ± 1.16 60.60 ± 1.20 3.10 ± 0.50 2.24 ± 1.37 0.50 ± 0.62 65.23 ± 1.00 5.39 ± 0.61 MedAlpaca a 8.58 ± 3.14 2.64 ± 1.82 65.90 ± 1.10 4.80 ± 0.90 4.73 ± 2.11 1.24 ± 1.12 72.06 ± 0.74 9.25 ± 0.79 AlpaCare a 17.16 ± 4.29 8.58 ± 3.14 66.50 ± 2.70 11.30 ± 1.20 4.98 ± 2.11 0.75 ± 0.87 75.07 ± 0.10 14.39 ± 0.75 MMed-LLaMA-3-8B a 6.93 ± 2.81 3.30 ± 1.98 65.60 ± 0.80 4.90 ± 0.60 11.44 ± 3.11 2.99 ± 1.62 72.77 ± 0.55 10.80 ± 0.60 PMC-LLaMA-13B a 7.59 ± 2.97 3.96 ± 2.15 65.00 ± 1.10 3.70 ± 0.60 1.24 ± 1.12 0.50 ± 0.62 29.17 ± 2.81 0.77 ± 0.35 MedLM-Medium b 34.32 ± 5.45 21.78 ± 4.62 75.61 ± 0.90 13.21 ± 1.34 30.85 ± 4.48 13.93 ± 3.36 83.27 ± 0.47 24.33 ± 1.26 MedInstruct c 29.70 ± 5.12 16.83 ± 4.13 70.90 ± 0.70 8.70 ± 0.60 46.52 ± 4.85 30.10 ± 4.48 80.14 ± 0.47 18.86 ± 0.73 TIMER tuned model with different base Qwen-2.5-7B-Instruct 35.97 ± 5.28 23.10 ± 4.79 73.57 ± 0.54 9.51 ± 0.62 51.49 ± 4.98 38.31 ± 4.85 80.81 ± 0.34 18.40 ± 0.64 w/ TIMER Tuning 38.41 ± 5.46 25.83 ± 4.97 73.39 ± 0.54 9.36 ± 0.62 54.73 ± 4.85 41.79 ± 4.85 81.53 ± 0.35 19.12 ± 0.66 Llama-3.1-8B-Instruct 30.69 ± 5.12 17.16 ± 4.29 70.50 ± 0.70 8.50 ± 0.70 45.02 ± 4.98 27.11 ± 4.35 79.49 ± 0.45 17.78 ± 0.70 w/ TIMER Tuning 34.32 ± 5.45 23.43 ± 4.62 76.70 ± 0.80 14.60 ± 1.40 48.51 ± 4.98 34.33 ± 4.73 83.20 ± 0.40 22.60 ± 1.07 a b c To illustrate these improvements, Table 7 Head-to-head comparison To further identify performance improvement in addition to rubric-based scoring, we perform head-to-head analyses of outputs to given questions from different models. We identify in Table 2 Table 2 Head-to-head comparison between various models and TIMER-Instruct MedAlign TIMER-Eval Model Win% Tie% Win% Tie% Existing medical finetuned models Meditron-7B a +83.10 2.30 +95.02 0.50 MedAlpaca a +72.80 2.80 +86.41 0.37 AlpaCare a +84.40 1.00 +73.82 0.01 MMed-LlaMa-3-8B a +80.14 2.60 +81.71 0.87 PMC-LlaMa-13B a +74.40 4.10 +96.14 0.12 MedLM-Medium +39.50 5.90 +20.65 1.99 Models with the same base MedInstruct (w/ Llama-3.1-8B-Instruct) +6.30 6.90 +8.45 1.99 Llama-3.1-8B-Instruct +23.80 5.60 +17.67 1.99 Each row shows the winning margin (additional wins by TIMER-Instruct) and tie rates for both benchmarks. a Additionally, we compare conventional QA-style tuning (MedInstruct) with our temporally grounded instruction-tuning (TIMER Tuning). While instruction-tuning with MedInstruct provided gains over baseline Llama-3.1-8B-Instruct performance, instruction tuning with TIMER provides additional gains of 6.3% on MedAlign and 8.45% on TIMER-Eval. This indicates the value of incorporating temporal structure into instruction tuning data. RQ2: Effect of the temporal distribution of instructions on model performance Temporal biases in existing clinical instruction sets Existing clinical instruction data have pronounced temporal biases. Using our normalized temporal position metric, we found that MedAlign 16 2 Fig. 2 Distribution of MedAlign instructions across patient timelines. The majority of human-generated instructions focus on the most recent encounters. When examining model-generated instructions, we observed a “lost-in-the-middle\" effect regarding the parts of the patient record in which the instructions were grounded (Fig. 3 Fig. 3 Normalized temporal position of evidence in model-generated instructions reveals edge-focused attention. Instructions cluster around early (0–25%) and late (75–100%) parts of patient timelines. Temporal duration vs. utilization An important distinction emerges between the temporal duration available in clinical records and the actual temporal scope utilized in instruction generation. MedAlign’s construction involved clinicians generating instructions independently without examining specific patient records, with these instructions subsequently matched to appropriate EHR data via retrieval. The pronounced recency bias (55.3% of instructions in the final 25% of timelines) reveals that MedAlign reflects the natural recency bias inherent in clinical question formulation when clinicians pose questions independent of specific patient cases. This observation motivates the need for systematic temporal control approaches that ensure evaluation coverage across extended patient timelines. Development of controlled temporal distribution evaluation Our analysis of existing temporal biases necessitated a new evaluation approach that could isolate and measure the specific effects of temporal distribution on model performance. Unlike existing approaches with inherent limitations (Table 3 3 15 Table 3 Comparison of EHR instructional evaluation set for medical LLMs Evaluation Curation Temporal coverage EHR data Support Duration Utilization Notes Tabular Rationale Attrib. Ref. Response MIMIC-Instr 15 Model Single visit (7.2 days) N/A ✓ ✗ ✗ ✗ ✓ MedAlign 16 Human Multi-visit (3895.1 days) 55.3% in final 25% ✓ ✓ ✓ ✗ ✓ TIMER-Eval Sampling (Ours) Model Multi-visit (1294.9 days) Controlled distribution ✓ ✓ ✓ ✓ ✓ The temporal coverage of evaluation requires considering both available duration and grounded utilization together. Clinician validation To ensure the validity of the model-generated evaluation data, three clinicians assessed 100 randomly sampled instruction-response pairs generated with TIMER (Table 4 7 Table 4 Clinician evaluation of TIMER evaluation samples Annotator Clinical relevance Complexity Accuracy Annotator 1 97/100 77/100 100/100 Annotator 2 89/100 63/100 96/100 Annotator 3 99/100 99/100 100/100 Effect of instruction distributions on model performance To understand how temporal distributions affect model performance, we created three distinct distribution patterns: Recency-Focused, Edge-Focused, and Uniformly-Distributed across timelines. Table 5 Table 5 Performance comparison of instruction tuning with different temporal distributions Evaluation distribution pattern Alternative training Distribution-matched training Win % advantage Tie (%) Recency-Focused Questions from last 25% of timeline Timeline-Extremes Recent-Events +3.00% 14.00 Full-Timeline Recent-Events +2.10% 15.70 Edge-Focused Questions at timeline extremes Recent-Events Timeline-Extremes +1.50% 3.10 Full-Timeline Timeline-Extremes +1.20% 4.50 Uniformly-Distributed Balanced across entire timeline Recent-Events Full-Timeline +6.50% 3.20 Timeline-Extremes Full-Timeline +1.80% 3.60 Positive values indicate win% advantage for distribution-matched models over alternatives. LLM-judge and human correlation To scale evaluation, we developed an LLM-based judge and validated it against clinician rankings for MedAlign responses. Table 6 ρ Table 6 Validation of LLM-Judge against established human rankings on MedAlign Model LLM correctness LLM completeness Human rank ↓ LLM rank ↓ GPT-4 32k 0.419 0.360 2.309 1 GPT-4 32k Multi-Step 0.383 0.365 2.292 2 Vicuna-13B 0.343 0.292 3.259 3 Vicuna-7B 0.318 0.299 3.304 4 MPT-7B-Instruct 0.193 0.149 3.688 5 LLM Correctness and Completeness show our LLM-Judge’s assessment of model performance, compared against existing human model rankings to validate LLM-Judge reliability. Spearman Correlations with Human Rank: Correctness: ρ ρ ρ Case studies: temporal reasoning behavior Table 7 Temporal boundary adherence Trend detection Temporal precision Table 7 Case studies on TIMER-Eval: Comparison of model responses between base Llama-3.1-8B-Instruct and model tuned w/ TIMER-Instruct Case 1: Weight Trend Assessment Instruction: Base (Llama-3.1-8B-Instruct) After Tuned w/ TIMER-Instruct The patient’s weight has been increasing over the past year, with a significant increase from 61.9 kg to 68 kg between 06/13/2021 and 12/18/2021. On 12/18/2021, the patient weighed 68 kg (150 lb), which was 49% for his age. On 01/21/2023, the patient weighed 64 kg (141 lb)...the patient’s weight has decreased over the past year... Analysis: Improved temporal boundary adherence Case 2: Hearing Loss Assessment Instruction: Base (Llama-3.1-8B-Instruct) After Tuned w/ TIMER-Instruct The patient’s mixed conductive and sensorineural hearing loss in the right ear has shown some improvement over time.... There is no indication of improvement... Analysis: Enhanced trend accuracy Case 3: Growth Progression Assessment Instruction: Base (Llama-3.1-8B-Instruct) After Tuned w/ TIMER-Instruct At age 9, the patient’s height was 135.3 cm (05/13/2011), which was at the 67th percentile... At age 9 (09/08/2011), the patient’s height was 139 cm, which placed her at the 67th percentile... Analysis: Better temporal precision Key differences highlighted in bold. In contrast, base models often conflate visits or provide temporally irrelevant information. Responses of models tuned with TIMER are more contextually grounded and clinically interpretable. Discussion Our results demonstrate that TIMER significantly enhances LLMs’ ability to reason over longitudinal clinical records through temporal-aware instruction tuning. By grounding instruction-response pairs at specific timestamps within patient histories, TIMER enables models to better integrate evidence across extended timeframes-a capability essential for clinical applications. Our evaluation reveals that models fine-tuned using the TIMER method consistently outperform those trained with standard medical question-answer pairs, particularly on tasks requiring synthesis across multiple timepoints. We also identify shortcomings in current clinical LLM evaluation techniques regarding timeline length, question complexity, and temporal coverage. We find that existing evaluations often exhibit strong recency bias, with most questions focusing on recent visits rather than comprehensive patient histories. By developing evaluation approaches that explicitly sample across different points in a patient timeline, we demonstrate how the alignment between instruction distribution and evaluation context impacts model performance. This insight provides a methodological foundation for future development of LLMs capable of sophisticated temporal reasoning in clinical settings. Our findings demonstrate the critical importance of temporal context when tuning LLMs for use with longitudinal clinical records, which is necessary for applications such as disease trajectory modeling, therapeutic response tracking, and longitudinal summarization. The performance improvements achieved through TIMER-a 39.50% win-rate over MedLM-Medium and 23.80% over the base model-reveal that current language models (both medical and general) lack the temporal reasoning capabilities necessary for clinical applications. The 6.3% advantage over traditional QA instruction tuning further emphasizes that temporal reasoning is a distinct capability that can be imbued with specialized training approaches. These results suggest that mere exposure to medical content is insufficient; what matters is how models learn to integrate information across time. By explicitly providing temporally-grounded instructions, TIMER instruction tunes models in a manner that mirrors the time-aware reasoning processes models have to support in healthcare workflows. Our analysis reveals two key deficiencies in current medical evaluations: they primarily rely on single-timepoint data retrieval and exhibit recency bias, overemphasizing recent patient information while neglecting longer-term clinical patterns. TIMER mitigates this bias by sampling instructions generated from across the patient timeline. This approach increases instruction diversity and encourages models to learn more generalizable temporal patterns. Notably, ablations of TIMER’s sampling strategy reveal that while aligning training distribution to evaluation distribution of instructions consistently improves model performance, this alignment is most critical with Uniformly-Distributed evaluation questions, with Full-Timeline instruction-tuned models outperforming Recent-Events instruction-tuned models by 6.5%. Human evaluation of a subset of model-generated instructions in TIMER-Eval rated the generated instructions as an average of 95/100 relevant, 80/100 complex, and 98/100 accurate. However, the use of model-generated data to investigate RQ2 remains a limitation, as these examples may not fully capture physician reasoning or align with clinical documentation patterns. Additionally, all model-generated instruction data cannot be easily screened by clinicians; therefore, this sampling assumes that other questions fall within the same distribution of quality. This study has several limitations. The model-based generation process may encode training data biases, and while we include some manual review, large-scale validation is still needed. We do not yet assess fairness across demographic subgroups or calibration of the temporal reasoning outputs. Future work should include this fairness analysis to further ensure that generated training and evaluation data adequately represent patient subgroups. Additional work could explore expanding the modalities included in TIMER to further improve the richness of queries that can be posed to models (e.g., reasoning over labs, imaging, etc.). Integrating the generation of evaluation questions with real-world deployment tasks could offer more insights into clinical impact and safety. Developing methods for scalable verification of model-generated data would enable more refined evaluation that precisely measures model capabilities for specific clinical tasks. In addition, increasing the scope of models that are both fine-tuned and evaluated with TIMER would provide more insight into the capabilities of frontier models on medical temporal reasoning tasks. Extending the current fine-tuning framework to medical models such as Me-LLaMA or Meditron-70B would further illustrate the impact of fine-tuning for temporal reasoning tasks 18 19 In conclusion, our results establish TIMER as a practical method for improving temporal reasoning capabilities in clinical LLMs. TIMER improves generalization and performance across model architectures and evaluation datasets. Llama-3.1-8B-Instruct instruction-tuned with TIMER, as well as Qwen-2.5-8B-Instruct instruction-tuned with TIMER, both demonstrate improvements compared to the base model for both physician-generated questions (MedAlign) and synthetically generated questions (TIMER-Eval). Instruction tuning that reflects the temporal complexity of healthcare data yields substantial performance improvements and better aligns models with the needs of longitudinal clinical tasks. From a deployment standpoint, our findings highlight the potential of time-aware instruction tuning to enhance retrospective chart review, and forecasting tasks. Because we release the TIMER method open-source, it allows any group with access to EHR to generate instruction tuning data as well as the evaluation set to examine and improve temporal reasoning capabilities 20 Methods Data source and preprocessing We sourced the data for our study from the Stanford Medicine Research Data Repository (STARR) 17 TIMER methodology and technical design TIMER uses timestamp-linked instruction-response pairs generated from longitudinal EHR data to fine-tune language models for improved temporal reasoning. The method leverages explicit time evidence integration to ground model responses in specific temporal contexts within patient histories. Figure 1 Temporal instruction-tuning data generation To generate temporally grounded instruction-response pairs for our instruction-tuning dataset, we used Gemini-1.5-Pro 21 2 2 Model architecture and training details We instruction-tuned Llama-3.1-8B-Instruct 22 23 3 24 Evaluation metrics We evaluated model responses using both automated metrics and LLM-based judges. For LLM-based evaluation, we used GPT-4o-mini to assess response correctness and completeness. The prompts used for LLM-Judge scoring are available in Supplementary Note 4 ρ c o r r ρ c o r r To provide additional evaluation insight, we also calculated standard automated metrics: BERTScore 25 26 27 28 6 Temporal reasoning instruction generation for evaluation To evaluate longitudinal temporal reasoning, we created TIMER-Eval, an evaluation schema that generates clinical questions requiring evidence integration from multiple timepoints. We used Gemini-1.5-Pro 21 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\bf{T}}}_{i}=\\{{T}_{i,1},...,{T}_{i,{n}_{i}}\\}$$\\end{document} T i = { T i , 1 , . . . , T i , n i } Questions were filtered to retain only those requiring synthesis across at least two distinct, time-stamped evidence snippets, ensuring each requires genuine temporal reasoning rather than simple retrieval. For instance, questions like “What medication was prescribed on 03/15/2020?” are filtered out as simple retrieval, while questions such as “How did the patient’s treatment response change between the initial chemotherapy in 2020 and the targeted therapy switch in 2021?” could be retained as they require temporal synthesis across multiple timepoints. The prompt template used for question generation is provided in Supplementary Note 1 Our quality control process included both automatic and manual review stages. Automatic filtering retained questions requiring synthesis across at least two distinct, timestamped evidence snippets from different time periods. Manual review subsequently eliminated low-quality questions that failed to meet temporal reasoning standards or contained factual inconsistencies. This two-stage process ensures that retained questions genuinely require longitudinal reasoning capabilities rather than simple information retrieval. Normalized temporal position metric To analyze temporal distribution patterns in clinical instructions, we defined a normalized position metric that enables consistent comparison across patient records of varying lengths. This normalization is applied to all datasets in this study. For each evidence timestamp T j Q i A i 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${P}_{j}=\\frac{{T}_{j}-{T}_{{\\rm{min}}}}{{T}_{{\\rm{max}}}-{T}_{{\\rm{min}}}}$$\\end{document} P j = T j − T min T max − T min Temporal distribution sampling strategy Based on our analysis of temporal distributions in existing datasets, we constructed three instruction-tuning datasets with matched size and source timelines, varying only in the distribution of instruction placement: Recent-Events: Instructions placed in the final quartile of patient timelines, mirroring the recency bias found in many real-world and annotated datasets. Timeline-Extremes: Instructions concentrated at the beginning and end of timelines, simulating LLM-generated data patterns. Full-Timeline: Instructions uniformly distributed across the entire timeline, ensuring balanced temporal coverage. This design isolates temporal placement as the sole variable, allowing evaluation of how different distributions affect models’ capacity for temporal reasoning across patient records. Supplementary information  Supplementary Information Publisher’s note These authors contributed equally: Hejie Cui, Alyssa Unell. Supplementary information The online version contains supplementary material available at 10.1038/s41746-025-01965-9. Acknowledgements We thank Kameron Black, Mehr Kashyap, and Akshay Swaminathan for their valuable clinical expertise in evaluating the quality, relevance, and accuracy of the instruction-response pairs used in this study. Their insights were instrumental in validating our approach and ensuring its clinical meaningfulness. Author contributions H.C., A.U., and J.A.F. conceptualized the study and defined the research objectives. A.U. and H.C. led the development of the machine learning framework, implemented the training procedures, and evaluated models across datasets. H.C. developed the data preprocessing and generation pipelines, ensuring compatibility with the LLM workflow. B.C. contributed to the ablation studies evaluating model robustness and benchmarking against existing LLMs. H.C. and A.U. drafted the manuscript. E.A., S.K., and N.S. provided methodological contributions and guidance, with N.S. additionally providing computational resources for experimental execution. All authors reviewed the manuscript for critical content, contributed to revisions, and approved the final version for submission. Data availability The datasets supporting the conclusions of this article are available with restrictions. The TIMER-Eval benchmark dataset generated for this study contains synthetic instruction-response pairs derived from clinical scenarios. The synthetic Electronic Health Record (EHR) data used for instruction-tuning was generated using Gemini-1.5-Pro based on prompts described in the Supplementary Information and is available under the same terms. A minimal synthetic dataset representative of the data structure and format will be released publicly alongside the code repository to facilitate understanding and reproduction of the methods. The original clinical data underlying the synthetic generation cannot be shared due to privacy and confidentiality requirements. The MedAlign 16 https://stanford.redivis.com/datasets/48nr-frxd97exb Code availability The source code for TIMER is available at https://github.com/som-shahlab/TIMER Competing interests The authors declare no competing interests. References 1. Li, T., Zhang, G., Do, Q. D., Yue, X., & Chen, W. Long-context LLMs Struggle with Long In-context Learning. Trans. Mach. Learn Res https://openreview.net/forum?id=Cw2xlg0e46 2. Kuratov, Y. et al. BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack. In A. Globerson, L. et al. (eds.) Advances in Neural Information Processing Systems (Vol. 37, pp 106519–106554) (2024). 3. Huguet N Using electronic health records in longitudinal studies: estimating patient attrition Med. Care 2020 58 S46 S52 10.1097/MLR.0000000000001298 32412953 PMC7365658 Huguet, N. et al. Using electronic health records in longitudinal studies: estimating patient attrition. Med. Care 58 32412953 10.1097/MLR.0000000000001298 PMC7365658 4. Wornow, M. et al. Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHR Data. The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=zg3ec1TdAP 5. Singhal K Large language models encode clinical knowledge Nature 2023 620 172 180 10.1038/s41586-023-06291-2 37438534 PMC10396962 Singhal, K. et al. Large language models encode clinical knowledge. Nature 620 37438534 10.1038/s41586-023-06291-2 PMC10396962 6. Lu Z Large language models in biomedicine and health: current research landscape and future directions J. Am. Med. Inform. Assoc. 2024 31 1801 1811 10.1093/jamia/ocae202 39169867 PMC11339542 Lu, Z. et al. Large language models in biomedicine and health: current research landscape and future directions. J. Am. Med. Inform. Assoc. 31 39169867 10.1093/jamia/ocae202 PMC11339542 7. Lucas MM Yang J Pomeroy JK Yang CC Reasoning with large language models for medical question answering J. Am. Med. Inform. Assoc. 2024 31 1964 1975 10.1093/jamia/ocae131 38960731 PMC11339506 Lucas, M. M., Yang, J., Pomeroy, J. K. & Yang, C. C. Reasoning with large language models for medical question answering. J. Am. Med. Inform. Assoc. 31 38960731 10.1093/jamia/ocae131 PMC11339506 8. Hager P Evaluating and mitigating limitations of large language models in clinical decision making Nat. Med. 2024 30 2613 2622 10.1038/s41591-024-03097-1 38965432 PMC11405275 Hager, P. et al. Evaluating and mitigating limitations of large language models in clinical decision making. Nat. Med. 30 38965432 10.1038/s41591-024-03097-1 PMC11405275 9. Bedi S Testing and Evaluation of Health Care Applications of Large Language Models: A Systematic Review JAMA 2025 333 319 328 10.1001/jama.2024.21700 39405325 PMC11480901 Bedi, S. et al. Testing and Evaluation of Health Care Applications of Large Language Models: A Systematic Review. JAMA 333 39405325 10.1001/jama.2024.21700 PMC11480901 10. Wang, Y., & Zhao, Y. TRAM: Benchmarking Temporal Reasoning for Large Language Models. In L.-W. Ku, A. Martins, & V. Srikumar (eds.) Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, 2024 (pp. 6389–6415). Association for Computational Linguistics. 10.18653/V1/2024.FINDINGS-ACL.382 (2024). 11. Fatemi, B. et al. Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning. The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, 2025. https://openreview.net/forum?id=44CoQe6VCq 12. Herel, D., Bartek, V. & Mikolov, T. Time awareness in large language models: benchmarking fact recall across time. Preprint at https://arxiv.org/abs/2409.13338 13. Ouyang L Training language models to follow instructions with human feedback NeurIPS 2022 35 27730 27744 Ouyang, L. et al. Training language models to follow instructions with human feedback. NeurIPS 35 14. Zhang, S. et al. Instruction tuning for large language models: a survey. Preprint at https://arxiv.org/abs/2308.10792 15. Wu, Z., Dadu, A., Nalls, M., Faghri, F. & Sun, J. Instruction tuning large language models to understand electronic health records. In NeurIPS Datasets and Benchmarks Track https://openreview.net/forum?id=Dgy5WVgPd2 16. Fleming, S. L. et al. MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records. In M. J. Wooldridge, J. G. Dy, & S. Natarajan (eds.) Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada (pp. 22021–22030). 10.1609/AAAI.V38I20.30205 (AAAI Press, 2024). 17. Datta, S. et al. A new paradigm for accelerating clinical data science at Stanford medicine. Preprint at https://arxiv.org/abs/2003.10534 18. Xie, Q. et al. Medical foundation large language models for comprehensive text analysis and beyond. npj Digit. Med. 8 10.1038/s41746-025-01533-1 PMC11882967 40044845 19. Chen, Z. et al. Meditron-70b: scaling medical pretraining for large language models. Preprint at https://arxiv.org/abs/2311.16079 20. Zeng, D., Qin, Y., Sheng, B. & Wong, T. Y. Deepseek’s “low-cost\" adoption across china’s hospital systems: Too fast, too soon? JAMA 333 10.1001/jama.2025.6571 40293869 21. Team, G. et al. Gemini 1.5: unlocking multimodal understanding across millions of tokens of context. Preprint at https://arxiv.org/abs/2403.05530 22. Llama, M. Llama-3.1-8b-instruct. https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct 23. Hu, E. J. et al. LoRA: Low-Rank Adaptation of Large Language Models. The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, 2022. https://openreview.net/forum?id=nZeVKeeFYf9 24. Qwen et al. Qwen2.5: A party of foundation models (2025). 25. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. BERTScore: Evaluating Text Generation with BERT. 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, 2020. https://openreview.net/forum?id=SkeHuCVFDr 26. Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics. (2004). 27. Maja Popović. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics. (2015). 28. Satanjeev Banerjee and Alon Lavie. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics. (2005). ",
  "metadata": {
    "Title of this paper": "Training language models to follow instructions with human feedback",
    "Journal it was published in:": "NPJ Digital Medicine",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12475073/"
  }
}
{
  "title": "Paper_1142",
  "abstract": "pmc Sensors (Basel) Sensors (Basel) 1660 sensors sensors Sensors (Basel, Switzerland) 1424-8220 Multidisciplinary Digital Publishing Institute  (MDPI) PMC12473765 PMC12473765.1 12473765 12473765 41013168 10.3390/s25185931 sensors-25-05931 1 Article Enhancing Medical Image Segmentation and Classification Using a Fuzzy-Driven Method Abduvaitov Akmal Conceptualization Formal analysis Writing – original draft Visualization 1 Shavkatovich Buriboev Abror Conceptualization Methodology Software Formal analysis Investigation 2 4 5 Sultanov Djamshid Methodology Software Resources Visualization 2 Buriboev Shavkat Software Validation Formal analysis Data curation 3 https://orcid.org/0009-0003-7146-2106 Yusupov Ozod Conceptualization Methodology Investigation Writing – review & editing 4 Jasur Kilichov Validation Investigation Data curation Visualization 1 https://orcid.org/0000-0001-6507-3081 Choi Andrew Jaeyong Writing – review & editing Supervision Project administration Funding acquisition 5 * Domingues Ines Academic Editor 1 abduvaitovakmal6@gmail.com jasurkilichov1987@gmail.com 2 abror1989@gachon.ac.kr djamshidsultanov05@gmail.com 3 abbosshav@gmail.com 4 ozodyusupov@gmail.com 5 * andrewjchoi@gachon.ac.kr 22 9 2025 9 2025 25 18 497667 5931 16 7 2025 02 9 2025 10 9 2025 22 09 2025 27 09 2025 29 09 2025 © 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ Automated analysis for tumor segmentation and illness classification is hampered by the noise, low contrast, and ambiguity that are common in medical pictures. This work introduces a new 12-step fuzzy-based improvement pipeline that uses fuzzy entropy, fuzzy standard deviation, and histogram spread functions to enhance picture quality in CT, MRI, and X-ray modalities. The pipeline produces three improved versions per dataset, lowering BRISQUE scores from 28.8 to 21.7 (KiTS19), 30.3 to 23.4 (BraTS2020), and 26.8 to 22.1 (Chest X-ray). It is tested on KiTS19 (CT) for kidney tumor segmentation, BraTS2020 (MRI) for brain tumor segmentation, and Chest X-ray Pneumonia for classification. A Concatenated CNN (CCNN) uses the improved datasets to achieve a Dice coefficient of 99.60% (KiTS19, +2.40% over baseline), segmentation accuracy of 0.983 (KiTS19) and 0.981 (BraTS2020) versus 0.959 and 0.943 (CLAHE), and classification accuracy of 0.974 (Chest X-ray) versus 0.917 (CLAHE). A classic CNN is trained on original and CLAHE-filtered datasets. These outcomes demonstrate how well the pipeline works to improve image quality and increase segmentation/classification accuracy, offering a foundation for clinical diagnostics that is both scalable and interpretable. image enhancement fuzzy approach concatenated CNN This research received no external funding. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Medical imaging, which includes modalities like computed tomography (CT), magnetic resonance imaging (MRI), and X-ray, is essential to contemporary healthcare because it makes it easier to diagnose, monitor, and plan treatments for illnesses. These methods enable crucial tasks like tumor segmentation and illness classification by providing detailed representations of anatomical components [ 1 2 3 4 5 6 7 Conventional enhancement techniques, including histogram equalization, have been widely used to improve contrast and lower noise in order to address these problems. Recent advancements in conventional techniques, such as Liu et al.’s adaptive contrast enhancement [ 8 9 10 11 12 This work presents a brand-new 12-step fuzzy-based image improvement pipeline that is intended to enhance the quality of medical images from X-ray, CT, and MRI modalities. To improve local contrast, lower noise, and preserve small features, the pipeline incorporates fuzzy logic techniques such as fuzzy entropy, fuzzy standard deviation, and histogram spread functions. We assess its performance on three publicly accessible datasets: Chest X-ray Pneumonia (available at https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19/data https://www.kaggle.com/datasets/awsaf49/brats2020-training-data https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia We train and evaluate two models—a classic Convolutional Neural Network (CNN) on the original and conventionally filtered datasets and a Concatenated CNN (CCNN) that takes advantage of the tripling of dataset size from the enhancement process (three enhanced images per input)—in order to determine the effect of the pipeline on downstream tasks. The purpose of this work is to show that the suggested pipeline considerably improves model performance in segmentation and classification tasks, in addition to improving image quality. There are three things this work contributes. First, we introduce a thorough fuzzy-based enhancement pipeline that successfully manages uncertainty across several modalities, overcoming the drawbacks of conventional techniques. Second, by contrasting the pipeline with conventional enhancement methods, we present a thorough assessment of its performance on segmentation (KiTS19, BraTS2020) and classification (Chest X-ray Pneumonia) tasks. Third, we demonstrate how the improved datasets can be used practically to increase the robustness and accuracy of deep-learning models, which may have consequences for clinical diagnoses. This study lays the groundwork for more dependable automated diagnosis systems by tackling noise and low contrast in medical images. 2. Related Works A crucial preprocessing step in medical imaging is image enhancement, which aims to improve image quality to ensure that tasks like segmentation and classification can be accurately analyzed. Numerous enhancement approaches have been developed in response to the difficulties posed by noise, low contrast, and unclear pixel intensities in modalities such as CT, MRI, and X-ray. This section examines current methodologies, classifying them into three categories: deep-learning-based methods, fuzzy logic-based approaches, and classic enhancement techniques. It then places our suggested 12-step fuzzy-based enhancement pipeline in relation to these approaches. 2.1. Traditional Enhancement Techniques Conventional techniques use filtering and pixel intensity modifications to increase contrast and lower noise. Contrast Limited Adaptive Histogram Equalization (CLAHE), first presented by Zuiderveld [ 13 14 15 16 17 18 19 20 2.2. Fuzzy Logic-Based Approaches Because fuzzy logic-based techniques portray pixel intensities as membership degrees, they effectively handle the ambiguity and uncertainty prevalent in medical pictures. This makes them ideal for tasks involving overlapping intensity distributions, including tumor segmentation. While it was only applicable to some modalities, Kim et al. [ 21 22 23 24 25 26 2.3. Deep-Learning-Based Methods Image enhancement has been transformed by deep learning [ 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 In contrast to the previously described techniques, our research presents a 12-step fuzzy-based image improvement pipeline that combines fuzzy entropy, fuzzy standard deviation, and histogram spread functions to handle poor contrast, noise, and ambiguity in a single framework. Although contrast is improved by CLAHE and conventional techniques, they frequently cannot manage pixel intensity uncertainty, which results in less-than-ideal performance on challenging tasks. Current fuzzy-based methods lack universality across modalities and tasks, despite being successful in certain situations. Despite their strength, deep-learning techniques are resource-intensive and can create artifacts that compromise reliability. Our pipeline shows adaptability with BRISQUE scores of 21.7, 23.4, and 22.1 on the tests for KiTS19 (CT), BraTS2020 (MRI), and Chest X-ray Pneumonia (X-ray), respectively. We offer a strong preprocessing framework that improves downstream task performance using a Concatenated CNN (CCNN), achieving Dice coefficients of 99.60% (KiTS19) and 91.50% (BraTS2020) and 0.989 accuracy (Chest X-ray) by producing three improved versions per dataset (fuzzy standard deviation-based, fuzzy entropy-based, and histogram-based). This research offers a scalable, interpretable method for improving medical images that may have therapeutic applications by bridging the gap between conventional, fuzzy, and deep-learning techniques. 3. Proposed Methodology 3.1. Fuzzy Enhancement Method Our research relies heavily on medical picture databases, which allow neural networks to be trained and evaluated for lung disease detection. To guarantee the diversity and representativeness required for efficient model performance, the dataset must undergo a number of crucial preprocessing processes as shown In Figure 1 Input Dataset: This dataset acts as the initial input for the enhancement process. Image Enhancement: Enhancing every input image using a fuzzy inference system (FIS) is the second crucial step. By giving each pixel in the image a different membership degree, this innovative technique increases complexity. To improve image quality, a new algorithm is added to the fuzzy logic process. Furthermore, a mathematical algorithm was created to improve the fuzzy logic process by fine-tuning the membership function. Generating Transformed Datasets: The same FIS-enhanced images are used in three new datasets that are produced by using three different kinds of local contrast characteristics. The size of all the photos triples after the fuzzy enhancement process. The high-fidelity data produced by contemporary sensors is directly employed by the picture improvement techniques used in this study, such as fuzzy entropy and standard deviation-based approaches. In order to improve the accuracy, precision, and robustness of deep-learning models in classification and segmentation tasks, these sensor-driven images make it possible to extract fine-grained, detailed information. The improved datasets serve as a basis for convolutional neural network (CNN) training and evaluation, with the aim of determining how model performance is affected by images altered by the Fuzzy Inference System (FIS). A strong foundation for neural network training is ensured by the methodical design of the image enhancement procedure, which covers a broad spectrum of illnesses. Our main objective is to improve neural network capabilities for classification, and the addition of FIS-based enhancement provides a unique perspective. In order to improve image quality inside the fuzzy logic framework, a fuzzy image enhancement approach is presented. This algorithm optimizes the fuzzy logic process by defining the membership function using a complex mathematical technique. The fuzzy-transformed images that are produced create a unique dataset that is designed specifically for CNN training. Figure 2 By giving each pixel in an image a different level of membership, the idea of fuzziness in image processing adds more complexity. This method takes into consideration ambiguity in the items’ depiction in the picture. Each pixel in the fuzzy framework model of image “F” is assigned a membership degree that indicates its association with particular groups or categories. This approach provides a more adaptable and detailed representation for analyzing and interpreting images, recognizing and addressing the inherent ambiguity and imprecision often found in real-world images. F = f ( x , y ) , μ F ( f ( x , y ) ) ) f ( x , y ) ∈ { 0 , … , L − 1 } , μ F ( f ( x , y ) ) x y Fuzzy images are particularly effective for image processing tasks involving ambiguity. Fuzziness aids in segmenting objects within an image, especially when their boundaries are not clearly defined. Traditional binary image processing often struggles to manage such ambiguity, rendering it ineffective in certain scenarios. Step 1: Normalization u ( x , y ) = l f ( x , y ) − f m i n f m i n m a x u x y x y f x y x y f min f max Step 2: Fuzzification μ F i x , y = 1 1 + u x , y − c i σ f , i = 1 , k ¯ Step 3: Fuzzification Refinement μ F i x , y = 2 ( μ F i ( x , y ) ) 2 , 0 ≤ μ F i ( x , y ) ≤ 1 2 , 1 − 2 ( 1 − μ F i ( x , y ) ) 2 , 1 2 < μ F i ( x , y ) ≤ 1 . Step 4: Local Contrast Quantification In image processing, measuring local contrast is crucial for assessing contrast variations across different regions of an image. Two distinct formulas are proposed to calculate local contrast in 8-bit grayscale digital images, enabling precise evaluation of contrast levels within specific areas. These methods allow for the quantification of contrast variations, supporting more informed and targeted image processing decisions: Local Contrast Calculation: (1) C ( x , y ) = ( C m a x − C m i n ) / 255 Global Contrast Calculation: (2) x , y = 2 ∑ j = 1 n f j − M f j 2 μ j ∑ j = 1 n μ j 0.5 255 C m a x C m i n  To fully grasp local contrast and its applications, it is necessary to analyze different categories of local neighborhoods based on their pixel luminance smoothness: Homogeneous Neighborhood: A local area where pixel brightness values are similar or identical, indicating high uniformity. Examples include regions like the sky in natural images, where brightness is nearly constant, resulting in negligible local contrast. Binary Neighborhood: A local area with pixels exhibiting extreme luminance values (e.g., black and white pixels), occupying opposite ends of the spectrum. These regions are marked by high contrast, often with abrupt brightness transitions and non-uniformity. Varied Brightness Neighborhood: A local area containing pixels with diverse luminance values but smooth transitions, lacking sharp boundaries. Such neighborhoods often feature complex details, varied textures, or objects with different brightness levels. Understanding the composition and characteristics of local neighborhoods is vital for calculating local contrast, as different neighborhood types may require distinct contrast estimation techniques or processing settings to achieve the desired results. Local features such as entropy, histogram distribution function, and standard deviation can be used to differentiate between these neighborhood categories, serving as valuable metrics for assessing the position and contrast of specific image regions. Step 5: Histogram Spread Function This step employs the Cumulative Distribution Function (CDF) to measure the proportion of pixels in an image with brightness values at or below a specified threshold: (3) h F ( x , y ) = f max − f min h max , h F ( x , y ) f min f max x y h max Step 6: Histogram Length-Based Local Contrast Transformation This step uses histogram length functions to determine the degree of local contrast transformation: (4) a = ( a m i n − a m a x ) 1 − e x p ( − ( f F − a 0 ) 2 2 π 2 ) s a a m i n a m a x α f F a 0 s π—mathematical constant (~3.1416). See Figure 3 Step 7: Entropy Entropy serves as a measure of variation or uncertainty in pixel values within a neighborhood, with higher entropy indicating a greater range of pixel intensities. In a homogeneous neighborhood, where pixels have nearly identical intensities, entropy is low due to minimal variation. In a binary neighborhood with pixels at extreme ends of the intensity spectrum, entropy can be high due to significant variation. Neighborhoods with diverse intensities and smooth transitions typically exhibit moderate entropy, reflecting moderate variability. Fuzzy entropy in a sliding local region of size n × n is defined as follows: (5) ε ( μ F ) = − a ∑ i = 1 n { μ F ( f i ) ln μ F ( f i ) + [ 1 − μ F ( f i ) ] ln [ 1 − μ F ( f i ) ] } / log ( n m ) , Summation Range: The sum is over all pixel coordinates ( i j W W x y i j x y x y µ F ( f i ) (6) μ F ( f i ) = h F f i ( x , y ) / n × m . Here, h F f i ( x , y ) f i x y x y Step 8: Fuzzy Entropy-Based Local Contrast Transformation Fuzzy entropy is used to determine the extent of local contrast transformation: (7) a = a m i n + ( a m a x − a m i n ) ε μ F − ε m i n ε m a x − ε m i n s s Step 9: Fuzzy Standard Deviation The standard deviation (σ\\sigma σ) measures the dispersion of brightness values around their mean in a neighborhood. In homogeneous regions, where data cluster closely around the mean, the standard deviation is low. In a binary neighborhood with significant differences between minimum and maximum brightness values, the standard deviation may be high. Neighborhoods with varied brightness values and smooth transitions typically have a moderate standard deviation. These features help in understanding the contrast and structural properties of different image regions, aiding in the selection of optimal processing techniques tailored to the unique characteristics of local neighborhoods. The standard deviation of brightness values in a sliding neighborhood W is computed as follows: (8) σ F ( x , y ) = 1 n m ∑ j = 1 n [ f j − M [ f j ] ] 2 μ j ∑ j = 1 n μ j , σ F x , y  x y M [ f j ] (9) M [ f j ] = 1 N M ∑ x = 1 N ∑ y = 1 M f j ( x , y ) , Here, N and M are the dimensions of the x = 1 , N , ¯ y = 1 , M ¯ Step 10: Standard Deviation-Based Local Contrast Transformation The fuzzy standard deviation of brightness data is used to determine the degree of local contrast change: (10) a x , y = a m i n σ F x , y + a m a x 1 − ( σ F ( x , y ) ) s a x , y x y a m i n a m a x σ x y k k x y s Step 11: Increasing Local Contrast Measures A nonlinear transformation is applied to enhance local contrast according to a specific rule: (11) C ∗ ( x , y ) = B 0 + R 2 − A 0 C ( x , y ) − C min C ∧ − C min α       C ( x , y ) ≤ C , ∧ R − A 0 − R 2 − A 0 C max − C ( x , y ) C max − C ∧ α     C ( x , y ) > C ∧ , x y x y x y min max 0 0 α Step 12: Defuzzification The modified image regions are reconstructed using the enhanced local contrast values. Designing a local contrast transformation function is a critical initial step in image processing. Its formulation depends on factors defined by researchers, such as constraints that dictate the degree of contrast enhancement. These limits play a key role in determining the extent of local contrast improvement across different image regions. The selection of the contrast transform function’s parameters relies on the researcher’s expertise and understanding of local statistical features. 3.2. Generation of Transformed Datasets Using Local Contrast Characteristics The image enhancement pipeline begins with a common FIS applied to the input images, which involves Steps 1–4: Normalization, Fuzzification, Fuzzification Refinement, and Local Contrast Quantification. To demonstrate Steps 1–3, consider a 2 × 2 grayscale image with 8-bit pixel intensities (0–255), representing a small region of a medical image. Table 1 Step 1: Normalization Pixel intensities are scaled from [0, 255] to [0, 1] using the following: f n o r m x , y = f ( x , y ) 255 For a 2 × 2 image with intensities [100, 150, 200, 50], the normalized values are [0.392, 0.588, 0.784, 0.196]. Step 2: Fuzzification The normalized intensities are mapped to membership values using the following sigmoidal function: μ F f x , y =  1 1 + e − a ( f x , y − b ) f n o r m x , y μ F 0.392 =  1 1 + e − 10 ( 0.392 − 0.5 ) = 1 1 + e 1.08 ≈ 0.253 This assigns a membership degree indicating the pixel’s association with a high-intensity set. Similar calculations are performed for other pixels. Step 3: Fuzzification Refinement Fuzzification Refinement adjusts membership values to optimize for medical image ambiguities. We apply a mathematical algorithm to fine-tune the membership function by introducing a contrast-enhancing factor γ μ ´ F ( x , y ) = μ ´ F x , y γ For μ F = 0.253 μ ´ F = 0.253 1.2 ≈ 0.184 The example shows how normalization standardizes intensities, fuzzification assigns membership degrees to model ambiguity, and refinement enhances contrast for downstream tasks. The refined membership values are used in subsequent steps to generate the three transformed datasets. This process ensures that the CCNN leverages diverse feature representations from these datasets, improving segmentation and classification performance. This FIS-enhanced image serves as the base for generating three transformed datasets, each produced by applying one of three distinct local contrast characteristics: histogram spread function, fuzzy entropy, and fuzzy standard deviation. These characteristics are extracted from the FIS-enhanced image and used to determine the degree of local contrast transformation, resulting in three complementary enhanced versions per original image. This triples the dataset size, providing diverse representations that capture different aspects of image quality, such as global distribution (histogram), uncertainty in pixel variations (entropy), and dispersion in brightness (standard deviation). The extraction and application process for each characteristic is as follows: Histogram Spread Function (Steps 5–6): This characteristic quantifies the distribution of pixel brightness values using the Cumulative Distribution Function (CDF) in Equation (4), computed over sliding neighborhoods W centered at each pixel ( x y min max max Fuzzy Entropy (Steps 7–8): This characteristic measures the uncertainty or variation in pixel membership degrees within neighborhoods, using Equation (5) for fuzzy entropy, where µ F ( f i ) Fuzzy Standard Deviation (Steps 9–10): This quantifies the dispersion of fuzzified brightness values around the mean, using Equations (8) and (9). Extracted over the same neighborhoods, it yields low values in homogeneous areas and high values in heterogeneous ones. The transformation using Equation (10), s > 0, typically s = 1.2 for X-ray opacity detection guides Step 11’s nonlinear enhancement, emphasizing edge preservation. Defuzzification results in a dataset that amplifies local heterogeneity, ideal for detecting dispersed features like pneumonia patterns in X-rays. These three datasets differ as follows: Image Content and Features: All start from the same FIS-enhanced base; hence, core content remains identical, but features are enhanced differently. The histogram-based dataset improves global brightness distribution, reducing over-enhancement in uniform areas. The fuzzy entropy-based dataset minimizes uncertainty, enhancing noisy or ambiguous regions. The fuzzy standard deviation-based dataset boosts dispersion-sensitive features, sharpening edges and textures. Quantitative Differences: As shown in Section 4.2 Utilization in CCNN: The CCNN processes these datasets in parallel streams, extracting complementary features (e.g., global from histogram, uncertainty-reduced from entropy, edge-enhanced from std dev). Feature maps are concatenated (64 × 64 × 768) before deeper layers, enabling robust learning by leveraging diversity, as evidenced by improved Dice coefficients. 4. Experimental Results and Discussions 4.1. Datasets This study evaluates the suggested 12-step fuzzy-based image improvement pipeline using three publicly available medical picture datasets, each of which represents a different imaging modality and workload. To evaluate the pipeline’s performance across a variety of modalities and applications, such as tumor segmentation and pneumonia classification, these datasets—KiTS19 (CT), BraTS2020 (MRI), and Chest X-ray Pneumonia (X-ray)—were chosen. The suggested pipeline was used to preprocess and improve each dataset, producing three improved versions (fuzzy standard deviation-based, fuzzy entropy-based, and histogram-based) for thorough assessment. It is important to note that the MRI sequences in KiTS19 generally exhibit a high signal-to-noise ratio (SNR), with noise levels within reasonable limits. In this setting, the primary challenge is not noise suppression but rather the enhancement in subtle local contrast differences between kidney tissue, tumor boundaries, and surrounding anatomical structures. The proposed fuzzy-based enhancement pipeline addresses this by adaptively adjusting local entropy and standard deviation features, thereby improving the visibility of clinically relevant details even in high-SNR images. This highlights that the method is not restricted to noisy modalities but can also provide benefits in high-quality imaging scenarios where fine structural contrast is essential. 4.1.1. KiTS19 210 training and 90 test instances of 3D CT scans with annotations for kidney and tumor regions make up the Kidney Tumor Segmentation 2019 (KiTS19) dataset, which was obtained from Kaggle. https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19/data Total number of axial CT slices across all patients: ≈75,000 slices. Number of slices containing a kidney region of interest (ROI): ≈22,000 slices. The remaining slices consist mostly of abdominal regions outside the kidney area and were excluded during preprocessing. This dataset serves as a standard for assessing automated segmentation algorithms in clinical settings and is intended for the semantic segmentation of kidneys and malignancies. The CT scans are appropriate for evaluating the enhancement pipeline’s capacity to increase contrast and lower noise in soft tissue areas since they provide comprehensive anatomical structures. In order to apply the enhancement pipeline for this investigation, the 3D volumes were divided into 2D slices. This produced three improved datasets for the segmentation tasks that followed. 4.1.2. BraTS2020 The BraTS2020 dataset is a publicly available benchmark designed for evaluating algorithms in brain tumor segmentation. It consists of multi-institutional, multi-parametric MRI scans of glioblastoma and lower-grade glioma patients. Each subject includes four MRI sequences acquired in clinical practice: T1-weighted (T1): provides structural brain detail. Post-contrast T1-weighted (T1Gd): highlights enhancing tumor regions. T2-weighted (T2): emphasizes edema and tissue heterogeneity. FLAIR (Fluid-Attenuated Inversion Recovery): suppresses CSF signals to highlight peritumoral edema. In total, BraTS2020 contains 369 training cases and 125 validation cases, each preprocessed with skull-stripping, resampling to 1 mm 3 This dataset is particularly valuable because it represents a challenging segmentation problem: while the overall SNR of MRI is high, the heterogeneity of tumor morphology, infiltration patterns, and multimodal appearance requires robust preprocessing and segmentation methods. In our context, the proposed fuzzy enhancement pipeline can be applied to BraTS2020 to improve local contrast across different MRI sequences before input to deep-learning architectures. 4.1.3. Chest X-Ray Pneumonia 5856 X-ray images classified as either “Normal” (1583 photos) or “Pneumonia” (4273 images) for binary classification are included in the Chest X-ray Images (Pneumonia) dataset, which was acquired via Kaggle [ https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia 4.2. BRISQUE Evaluation Results The BRISQUE, a no-reference picture quality metric that assigns scores ranging from 0 to 100, was used to assess the efficacy of the suggested 12-step fuzzy-based image enhancement pipeline. Lower values indicate better quality (fewer distortions). The three datasets—Chest X-ray Pneumonia, BraTS2020, and KiTS19—were enhanced using three different methods: fuzzy entropy, fuzzy standard deviation (FSD), and histogram spread function. The BRISQUE scores were calculated for both the original and enhanced versions of each dataset. The findings are analyzed after the results are shown in Table 1 The suggested enhancement pipeline greatly enhanced image quality across all datasets and modalities, as shown by the BRISQUE scores in Table 2 With a BRISQUE score of 23.1—a significant 7.2-point improvement over the initial score of 30.3—the fuzzy entropy-based approach once again outperformed the others for the BraTS2020 dataset. Closely behind the FSD-based approach, which improved by 6.8 points to 23.5, was the histogram-based approach, which improved by 5.4 points to 24.9. The FSD-based approach produced the lowest BRISQUE score of 22.4 in the KiTS19 dataset, which was 5.9 points higher than the initial score of 28.8. Both the histogram-based approach and the fuzzy entropy-based method improved by 5.5 and 4.8 points, respectively, to 22.8 and 23.5. FSD is highly suited for CT imaging because of its great performance, which demonstrates its capacity to improve local contrast in soft tissue areas like the kidney and tumor areas. Fuzzy entropy showed its stability across modalities by consistently achieving the lowest or near-lowest BRISQUE scores across all datasets (21.1 for Chest X-ray, 23.1 for BraTS2020, and 22.8 for KiTS19). FSD did especially well on MRI (BraTS2020: 23.5) and CT (KiTS19: 22.4), most likely because it emphasizes local contrast enhancement, which works well for intricate structures like tumors. Global contrast enhancement may introduce more distortions than fuzzy-based methods, as the histogram-based approach consistently produced the highest BRISQUE scores among the enhanced datasets (22.4 for Chest X-ray, 24.9 for BraTS2020, and 23.5 for KiTS19) while maintaining quality improvement. According to these findings, the suggested pipeline—in particular, the fuzzy entropy and FSD methods—can significantly improve image quality, which may boost the efficiency of subsequent tasks like segmentation and classification. 4.3. Task-Specific Evaluation This section uses the KiTS19, BraTS2020, and Chest X-ray Pneumonia datasets to assess how the suggested image enhancement process affects downstream tasks, such as segmentation and classification. A CNN trained on the original dataset, a CNN trained on the dataset filtered by the CLAHE algorithm, and a Concatenated CNN trained on the enhanced datasets produced by our 12-step fuzzy-based pipeline (histogram-based, fuzzy entropy-based, and fuzzy standard deviation-based) were the three models that were trained and evaluated for each dataset. Because the enhancement process generates three enhanced images for every input image, the CCNN architecture was especially built to withstand the dataset size doubling. The 12-step fuzzy-based pipeline demonstrates superior performance in enhancing medical images for segmentation and classification tasks across CT, MRI, and X-ray modalities, as evidenced by lower BRISQUE scores (e.g., 21.7 for KiTS19) and higher Dice coefficients (e.g., 99.60% for KiTS19) compared to Contrast Limited Adaptive Histogram Equalization (CLAHE) and deep-learning baselines. While other traditional image enhancement methods, such as Histogram Equalization (HE), Gamma Correction, and Multi-scale Retinex with Color Restoration (MSRCR), were considered as potential baselines, they were not included due to the substantial computational workload required. Our pipeline processes three datasets (KiTS19, BraTS2020, Chest X-ray Pneumonia), each tripling in size (e.g., 300 to 900 cases), and adding multiple methods would necessitate extensive parameter tuning, preprocessing of 3D volumes into 2D slices, and additional validation, which exceeds current resource constraints. Known limitations of these methods further justify this decision: Histogram Equalization (HE): Enhances global contrast but can amplify noise and artifacts in uniform regions, reducing effectiveness for heterogeneous medical images like MRI tumor boundaries. Gamma Correction: Adjusts brightness via a power-law function, but its performance depends heavily on the gamma parameter, often leading to over-saturation or loss of detail in low-contrast areas (e.g., CT kidney tumors). Multi-scale Retinex with Color Restoration (MSRCR): Improves dynamic range and local contrast but requires complex parameter tuning across scales and is designed for color images, making it less applicable to grayscale medical datasets without significant adaptation. 4.3.1. Proposed Neural Networks To evaluate the effect of the enhancement pipeline, two neural network topologies are used. With its convolutional, pooling, and fully connected layers, the conventional CNN is used as a baseline model ( Figure 4 A customized architecture called Concatenated CNN was created to take advantage of the upgraded datasets, which trebled in size (for example, from 300 cases to 900 for KiTS19, 369 to 1107 for BraTS2020, and 5856 to 17,568 for Chest X-ray). Prior to feeding their feature maps into dense layers for final prediction, the CCNN concatenates the parallel processing streams for the three improved versions (fuzzy standard deviation-based, fuzzy entropy-based, and histogram-based). The model can better capture small features and lower noise thanks to its design, which enables it to take advantage of the complementary information from the improved photos. A key element in proving the efficacy of the pipeline is the CCNN’s architecture, which is tailored for the larger data volume and the unique features of the improved datasets. The CNN serves as a baseline model, trained on the original and CLAHE-filtered datasets. It is designed to be lightweight yet effective for capturing spatial features from unenhanced medical images. The architecture consists of the following layers: Input Layer: Accepts 2D grayscale images (e.g., 256 × 256 for Chest X-ray, resized 2D slices of 256 × 256 for KiTS19 and BraTS2020). For 3D datasets (KiTS19, BraTS2020), volumes are processed as stacks of 2D slices. Convolutional Block 1: 32 filters of size 3 × 3, stride 1, ReLU activation, followed by batch normalization and max-pooling (2 × 2, stride 2) to reduce spatial dimensions. Convolutional Block 2: 64 filters of size 3 × 3, stride 1, ReLU activation, followed by batch normalization and max-pooling (2 × 2, stride 2). Convolutional Block 3: 128 filters of size 3 × 3, stride 1, ReLU activation, followed by batch normalization and max-pooling (2 × 2, stride 2). Fully Connected Layers: Flattened feature maps fed into a dense layer with 512 units (ReLU activation, dropout 0.5), followed by an output layer. For segmentation (KiTS19, BraTS2020): A softmax layer with two units (background, tumor) for pixel-wise classification, outputting a segmentation mask of the same size as the input (upsampled using bilinear interpolation after decoding). For classification (Chest X-ray): A sigmoid layer with one unit (Normal/Pneumonia). Training Parameters: The model is trained using the Adam optimizer (learning rate 0.001), with binary cross-entropy loss for classification and Dice loss for segmentation, over 50 epochs with a batch size of 16. Because of its simplicity and lack of specialized methods to manage increased data complexity, this architecture is limited in its capacity to use the enhanced datasets, although it is computationally efficient and appropriate for baseline comparisons. A specific architecture called Concatenated CNN was created to take advantage of upgraded datasets, which trebled in size (for example, from 300 cases to 900 for KiTS19, 369 to 1107 for BraTS2020, and 5856 to 17,568 for Chest X-ray). The three improved versions—fuzzy entropy-based, fuzzy standard deviation-based, and histogram-based—are processed by the CCNN in parallel streams, concatenating their features to gather complementary information. Table 3 Each of the three parallel input streams in the CCNN architecture starts with a 2D grayscale image (256 × 256) from one of the improved versions. 3D datasets (KiTS19, BraTS2020) are processed using 2D slices for volumes. With max-pooling layers, each stream has three convolutional blocks (64, 128, 256 filters), resulting in a feature map size of 64 × 64 × 256 per stream. A composite feature map of 64 × 64 × 768 is then produced by concatenating the feature maps along the channel axis. Two more convolutional blocks (512, 1024 filters) with max-pooling come next, bringing the feature map’s size down to 16 × 16 × 1024. Two dense layers (1024 and 512 neurons) with dropout (0.5) are used on the flattened feature map in order to avoid overfitting. The output layer varies depending on the task: According to Table 1 The output layer has two neurons (tumor, background) with softmax activation for pixel-wise classification for segmentation (KiTS19, BraTS2020). To create a segmentation mask, a decoder upsamples the feature maps to the original input size (256 × 256) using transposed convolutions. The CCNN is trained in over 50 epochs with a batch size of 8 using the Adam optimizer (learning rate 0.0005), with binary cross-entropy loss for classification and Dice loss for segmentation. This architecture is well-suited for enhanced datasets because of its parallel stream design, which allows it to take advantage of the complementary information from the three enhanced datasets. This improves its capacity to capture fine details (such as tumor boundaries in KiTS19, edema in BraTS2020, and lung opacities in Chest X-ray) and reduces noise. Training Parameters Optimizer: Adam optimizer, with an initial learning rate of 0.001, was chosen for its adaptability and efficacy in deep-learning tasks. Batch Size: A batch size of 32 balances convergence speed and computational efficiency. Epochs: 50 epochs allow the model sufficient training time without overfitting. Early Stopping: Enabled to monitor validation loss, stopping training if it fails to improve after 10 epochs. To ensure that the model was adequately trained without underfitting, we monitored the training and validation losses across 50 epochs. Figure 5 4.3.2. Overfitting Analysis The high-performance metrics raise valid concerns about overfitting, especially given comparisons to SOTA baselines like nnU-Net (88.90% Dice for BraTS2020) and TransUNet (89.20% Dice). While KiTS19 SOTA is ~0.912 composite Dice, our 99.60% is unusually high, potentially due to the enhanced dataset’s noise reduction. To assess overfitting, we monitored training-validation loss curves ( Figure 5 4.3.3. Segmentation Performance (KiTS19) The original dataset (300 cases) and the improved datasets (900 cases total: 300 × 3) were used to train the two CNN models for the KiTS19 dataset. The improved datasets were used to train CCNN, and the fuzzy entropy-based dataset produced the best-performing model (BRISQUE: 23.1). Dice coefficient, accuracy, precision, sensitivity, and recall were used to assess performance. The BRISQUE scores and segmentation accuracy of the three models are contrasted in Table 4 Both the original and CLAHE-filtered datasets fared worse than the suggested enhancement pipeline. The BRISQUE score of the original KiTS19 dataset was 28.8, but CLAHE raised it to 26.4. Our approach produced better image quality with a lower BRISQUE score of 21.7. The significant influence of our enhancement strategy on segmentation performance was demonstrated by the CCNN model trained on our enhanced dataset, which achieved an accuracy of 0.983, a 2.4% improvement over the CLAHE-filtered dataset (0.959), and a 6.2% improvement over the original dataset (0.921). Figure 5 To assess the benefit of the three-type image fusion, we considered comparing the CCNN trained with only a single enhanced image type (fuzzy entropy-based) versus the proposed three-type fusion, see Table 5 For the Fuzzy Entropy dataset with the CNN, the accuracy is 0.971, and the BRISQUE score is 23.8. In contrast, the Three-Type Fusion dataset with the CCNN achieves a higher accuracy of 0.983 and a lower BRISQUE score of 22.1. This indicates a 1.8% improvement in accuracy and a 1.7-point reduction in BRISQUE, where lower BRISQUE values signify better image quality due to reduced noise and enhanced detail. To address the omission of direct comparisons with previous fuzzy logic-based methods, we included baseline fuzzy models—fuzzy c-means (FCM) and fuzzy entropy-only—in a limited evaluation on the KiTS19 dataset. Results are summarized in Table 6 Values for FCM and entropy-only are estimated based on their known performance in segmentation and enhancement tasks; exact figures require full retraining across datasets. FCM, a clustering approach, achieves moderate enhancement (~90.50%, ~27.0), while entropy-only improves contrast (~97.10%, ~23.8). The three-type fusion outperforms both, with a ~5.40% and ~1.3 BRISQUE improvement over entropy-only, highlighting the pipeline’s integrated approach. Full comparison across all datasets (KiTS19, BraTS2020, Chest X-ray) is deferred due to computational workload from dataset tripling (e.g., 300 to 900 cases). The suggested improved CCNN model obtained the highest scores on all metrics: a Dice coefficient of 99.60%, precision of 98.70%, sensitivity of 99.30%, and recall of 98.60%. Table 7 Figure 6 There are clear patterns among the models in Table 7 4.3.4. Segmentation Performance (BraTS2020) The original dataset (369 cases) was used to train the first CNN on the BraTS2020 dataset, while the CLAHE-filtered dataset (369 cases) was used to train the second CNN. The improved datasets (1107 cases total: 369 × 3) were used to train a CCNN, and the model that performed the best used the fuzzy entropy-based dataset (BRISQUE: 23.1). Dice coefficient, accuracy, precision, sensitivity, and recall were used to evaluate performance. The segmentation accuracy and BRISQUE scores for the three models are contrasted in Table 8 The BraTS2020 dataset’s segmentation performance was considerably enhanced by the suggested enhancement pipeline, according to the results as shown in Table 9 The suggested CCNN model achieves a Dice coefficient of 91.50%, precision of 92.00%, sensitivity of 91.20%, and recall of 91.40%, indicating outstanding performance across all criteria. A considerable improvement in segmentation accuracy is indicated by the highest Dice coefficient, which calculates the overlap between predicted and ground truth tumor segmentations. It surpasses the best alternative, TransUNet, by 2.30% (89.20%). 92% of projected tumor pixels are true positives, according to precision (92.00%), which is 2.00% higher than TransUNet (90.00%) and suggests fewer false positives. TransUNet’s 88.80% and 89.10% are surpassed by 2.40% and 2.30%, respectively, by sensitivity (91.20%) and recall (91.40%), which measure the detection of actual tumor locations, underscoring the CCNN’s capacity to reduce missed detections. These outcomes support the effectiveness of the CCNN on the improved BraTS2020 dataset (1107 instances), which is in line with its reported accuracy of 0.917. A variety of model performance levels are shown in the table. The field is led by contemporary architectures like nnU-Net (88.90%), TransUNet (89.20%), and Swin Transformer (88.50%), which profit from sophisticated concepts including hierarchical attention mechanisms, transformer integration, and self-configuring networks, respectively. Older or less sophisticated methods, including ResU-Net (86.90%) and Two-Stage U-Net (86.70%), lag behind, most likely because BraTS2020 is unable to handle the complexity of multimodal MRI data (T1, T1ce, T2, and FLAIR). The CCNN’s 92.00% precision indicates a significant decrease in false positives, whereas the range of precision is 87.40% (Two-Stage U-Net) to 90.00% (TransUNet). Similar patterns can be seen in sensitivity and recall, which range from 86.20% (Two-Stage U-Net) to 88.80% (TransUNet), while the CCNN’s 91.20% and 91.40% set new records. The CCNN’s improved dataset and parallel stream architecture appear to offer a strong edge over both conventional and modern techniques, based on the consistency observed across measurements. Due to the 12-step fuzzy-based enhancement pipeline, which triples the size of the BraTS2020 dataset (369 to 1107 cases) and lowers the BRISQUE score to 23.4 (from 30.3 for the original dataset and 27.6 for CLAHE-filtered), the CCNN performs better than the other techniques. According to the best-performing model, the fuzzy entropy-based enhancement improves feature visibility and lowers noise by enhancing tumor sub-region contrast (such as tumor core and edema) across MRI modalities. By concatenating their feature maps (64 × 64 × 768) and processing the three improved versions (histogram-based, fuzzy entropy-based, and fuzzy standard deviation-based) in parallel, the CCNN can capture complementary information that is then refined by deep layers (512, 1024 filters) and dropout (0.5) to avoid overfitting. This design leverages the richness of the improved dataset to deliver a 2.30% Dice improvement, outperforming even transformer-based TransUNet and more straightforward models like ResU-Net. 4.3.5. Classification Performance (Chest X-Ray) The original dataset (5856 pictures) and the CLAHE-filtered dataset (5856 images) were used to train a classic CNN for the Chest X-ray Pneumonia dataset. The improved datasets (17,568 images total: 5856 × 3) were used to train a CCNN, and the model that performed the best used the fuzzy entropy-based dataset (BRISQUE: 21.1). Accuracy, precision, recall, F1-score, and AUC-ROC were used to assess performance in identifying pictures as either “Normal” or “Pneumonia.” The BRISQUE scores and classification accuracy of the three models are contrasted in Table 10 Table 11 The findings show that classification performance on the Chest X-ray Pneumonia dataset was considerably enhanced by the suggested enhancement pipeline. A slight improvement in image quality was indicated by CLAHE, which decreased the original dataset’s BRISQUE score of 26.8 to 25.6. With a lower BRISQUE score of 22.1, our approach demonstrated better image quality and fewer aberrations. As a result, the CCNN model trained on our improved dataset had an accuracy of 0.989, which was 10.3% better than the original dataset (0.871) and 5.7% better than the CLAHE-filtered dataset (0.917). This improvement most likely results from the fuzzy entropy-based method’s capacity to reduce noise in X-ray images and increase the visibility of pneumonia-related characteristics, like lung opacities, improving classification accuracy and robustness. A benchmark for the Chest X-ray Pneumonia classification job is established by CCNN’s outstanding performance on all five measures. By successfully classifying 98.9% of the 17,568 improved photos, the CCNN achieves an accuracy of 0.989. This translates to around 17,370 right predictions, which is 0.8% better than the nearest rival, Rahman T. et al. [ 53 There is a noticeable difference between ancient and new designs based on performance trends. With accuracies exceeding 0.964, more recent models such as the CCNN, Rahman T. et al., and MobileNetV2 outperform the others. These models gain from sophisticated designs such as lightweight architectures (MobileNetV2) and specific upgrades (CCNN’s parallel streams). On the other hand, earlier models such as VGG-19 (0.821), MobileNet (0.834), and AlexNet (0.805) from [ 55 54 4.3.6. Trade-Offs of 2D vs. 3D Processing The use of 2D slice processing simplifies the analysis of 3D volumes but introduces trade-offs. Advantages include reduced computational complexity, enabling the pipeline to handle tripling dataset sizes (e.g., 300 to 900 slices) with current resources, and compatibility with the CCNN’s 2D architecture. This approach yielded high performance, as seen in the 99.60% Dice for KiTS19. However, it oversimplifies spatial dependencies across slices, potentially missing volumetric context critical for tumor boundary delineation in CT/MRI data. 3D CNNs, while capable of capturing these dependencies (e.g., nnU-Net’s 3D U-Net achieves ~0.912 Dice on KiTS19), demand significantly higher memory (e.g., 16 GB + GPU vs. 4 GB for 2D) and longer training times (hours vs. minutes per epoch), which exceed our current infrastructure. The 2D approach thus prioritizes efficiency over full spatial fidelity. 5. Conclusions This study introduces a new 12-step fuzzy-based image enhancement pipeline that is combined with sophisticated neural network architectures to enhance the performance of medical image analysis tasks, such as pneumonia classification (Chest X-ray Pneumonia), brain tumor segmentation (BraTS2020), and kidney tumor segmentation (KiTS19). BRISQUE scores of 21.7 (KiTS19), 23.4 (BraTS2020), and 22.1 (Chest X-ray) demonstrate that the proposed pipeline, which uses histogram-based, fuzzy entropy-based, and fuzzy standard deviation-based techniques, performs better than the original datasets (28.8, 30.3, 26.8) and CLAHE-filtered datasets (26.4, 27.6, 25.6). It also triples the dataset size and reduces image noise. On the KiTS19 dataset, the improved CNN model outperformed the best baseline (LinkNetB7) by 2.40%, achieving a Dice coefficient of 99.60%, precision of 98.70%, sensitivity of 99.30%, and recall of 98.60%. This showed remarkable accuracy in kidney tumor delineation. In a similar vein, the Concatenated CNN model, which was created to take advantage of the complementary information in the upgraded datasets, outperformed TransUNet by 2.30% on BraTS2020, recording a Dice coefficient of 91.50%, precision of 92.00%, sensitivity of 91.20%, and recall of 91.40%. With precision, recall, F1-score, and AUC-ROC metrics all outperforming alternatives by a substantial margin, the CCNN obtained an accuracy of 0.989 for the classification of chest X-ray pneumonia, which was a 7.2% improvement over CLAHE-filtered data. These outcomes highlight the adaptability of the pipeline and the CCNN’s capacity to improve feature detection in a variety of medical imaging modalities. The improved models have significant clinical advantages, including accurate tumor segmentation for radiation and surgical planning, low false positives and missed detections, and the potential to improve patient outcomes for the identification of pneumonia and kidney and brain tumors. Higher picture quality is indicated by the enhanced BRISQUE scores, which make it easier to see important details such as lung opacities and tumor boundaries. But there are still restrictions. The models’ performance is optimized for the enhanced datasets; additional validation is necessary to see whether they can be applied to other datasets or imaging modalities. Despite being lessened by the CCNN’s parallel architecture, the larger dataset size may provide computational difficulties. Furthermore, thorough comparisons are limited by the absence of sensitivity and recall data for some baseline techniques. Future research could look at a number of ways to improve this pipeline even more. First, the quality of enhancement could be further improved by including other fuzzy-based techniques, like adaptive membership functions that are suited to particular modalities. Second, expanding the assessment to encompass a wider range of datasets (such as PET or ultrasound scans) may confirm the generalizability of the pipeline. Third, combining the CCNN with more sophisticated deep-learning architectures, including transformer-based models, may result in even better segmentation and classification results. Fourth, benchmarking the proposed preprocessing pipeline across a broader set of backbone architectures (e.g., ResNet, EfficientNet, DenseNet) is a planned direction for future research. Also extend the pipeline and CCNN to 3D models, leveraging volumetric data to improve accuracy on datasets like KiTS19 and BraTS2020, with validation on external 3D datasets (e.g., KiTS21) and optimized hardware to address current limitations. Lastly, a thorough examination of the enhancement pipeline’s real-time applicability and computational efficiency will be helpful for its actual implementation in clinical situations. All things considered, our work offers a solid basis for developing medical image analysis using cutting-edge preprocessing methods, with encouraging ramifications for raising diagnostic precision and patient outcomes. Disclaimer/Publisher’s Note: Author Contributions Conceptualization, A.A., O.Y. and A.S.B.; methodology, A.S.B., O.Y. and D.S.; software, A.S.B., D.S. and S.B.; validation, K.J. and S.B.; formal analysis, A.A., A.S.B. and S.B.; investigation, A.S.B., O.Y. and K.J.; resources, D.S.; data curation, S.B. and K.J.; writing—original draft preparation, A.A. and A.S.B.; writing—review and editing, A.J.C. and O.Y.; visualization, A.A., K.J. and D.S.; supervision, A.J.C.; project administration, A.J.C.; funding acquisition, A.J.C. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement Data are contained within the article. Conflicts of Interest The authors declare no conflicts of interest. References 1. Shen D. Wu G. Suk H.I. Deep Learning in Medical Image Analysis Annu. Rev. Biomed. Eng. 2017 19 221 248 10.1146/annurev-bioeng-071516-044442 28301734 PMC5479722 2. Liu X. Song L. Liu S. Zhang Y. A Review of Deep-Learning-Based Medical Image Segmentation Methods Sustainability 2021 13 1224 10.3390/su13031224 3. Isensee F. Jens P. Andre K. nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation arXiv 2018 1809.10486 4. Buriboev A.S. Khashimov A. Abduvaitov A. Jeon H.S. CNN-Based Kidney Segmentation Using a Modified CLAHE Algorithm Sensors 2024 24 7703 10.3390/s24237703 39686240 PMC11644871 5. Musthafa N. Memon Q.A. Masud M.M. Advancing Brain Tumor Analysis: Current Trends, Key Challenges, and Perspectives in Deep Learning-Based Brain MRI Tumor Diagnosis Eng 2025 6 82 10.3390/eng6050082 6. Banafshe F. Abhilash H. Gregor K. Jacob L. Janet L. Improved-Mask R-CNN: Towards an accurate generic MSK MRI instance segmentation platform (data from the Osteoarthritis Initiative) Comput. Med. Imaging Graph. 2022 97 102056 10.1016/j.compmedimag.2022.102056 35364383 7. Buriboev A.S. Muhamediyeva D. Primova H. Sultanov D. Tashev K. Jeon H.S. Concatenated CNN-Based Pneumonia Detection Using a Fuzzy-Enhanced Dataset Sensors 2024 24 6750 10.3390/s24206750 39460230 PMC11510836 8. Liu C. Sui X. Kuang X. Liu Y. Gu G. Chen Q. Adaptive Contrast Enhancement for Infrared Images Based on the Neighborhood Conditional Histogram Remote Sens. 2019 11 1381 10.3390/rs11111381 9. Buriboev A. Muminov A. Computer State Evaluation Using Adaptive Neuro-Fuzzy Inference Systems Sensors 2022 22 9502 10.3390/s22239502 36502208 PMC9738543 10. Singh S.K. Abolghasemi V. Anisi M.H. Fuzzy Logic with Deep Learning for Detection of Skin Cancer Appl. Sci. 2023 13 8927 10.3390/app13158927 11. Gupta A. Mayank D. Vipul K. Attulya S. Atul D. Brain tumor segmentation from MRI images using deep learning techniques Proceedings of the International Advanced Computing Conference Hyderabad, India 16–17 December 2022 Springer Nature Cham, Switzerland 2022 10.1007/978-3-031-35641-4_36 12. Harriet Linda C. Wiselin Jiji G. Crack detection in X-ray images using fuzzy index measure Appl. Soft Comput. 2011 11 3571 3579 10.1016/j.asoc.2011.01.029 13. Zuiderveld K. Contrast Limited Adaptive Histogram Equalization Graphics Gems IV Academic Press Cambridge, MA, USA 1994 474 485 14. Abdusalomov A. Mirzakhalilov S. Umirzakova S. Shavkatovich Buriboev A. Meliboev A. Muminov B. Jeon H.S. Accessible AI Diagnostics and Lightweight Brain Tumor Detection on Medical Edge Devices Bioengineering 2025 12 62 10.3390/bioengineering12010062 39851336 PMC11759171 15. Heller N. Isensee F. Maier-Hein K.H. Hou X. Xie C. Li F. Nan Y. Mu G. Lin Z. Han M. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge Med. Image Anal. 2021 67 101821 10.1016/j.media.2020.101821 33049579 PMC7734203 16. Pizer S.M. Amburn E.P. Austin J.D. Cromartie R. Geselowitz A. Greer T. ter Haar Romeny B. Zimmerman J.B. Zuiderveld K. Adaptive Histogram Equalization and Its Variations Comput. Vis. Graph. Image Process. 1987 39 355 368 10.1016/S0734-189X(87)80186-X 17. Jain A.K. Fundamentals of Digital Image Processing. Prentice Hall Upper Saddle River, NJ, USA 1989 18. Gonzalez R.C. Woods R.E. Digital Image Processing 2nd ed. Prentice Hall Upper Saddle River, NJ, USA 2002 19. Polesel A. Ramponi G. Mathews V.J. Image Enhancement via Adaptive Unsharp Masking IEEE Trans. Image Process. 2000 9 505 510 10.1109/83.826787 18255421 20. Huang T. Yang G. Tang G. A Fast Two-Dimensional Median Filtering Algorithm IEEE Trans. Acoust. Speech Signal Process. 1979 27 13 18 10.1109/TASSP.1979.1163188 21. Kim K.-J. Cho S.-B. Tan S.C. Lim C.P. Fuzzy ARTMAP and hybrid evolutionary programming for pattern classification J. Intell. Fuzzy Syst. 2011 22 57 68 10.3233/ifs-2011-0476 22. Pal S.K. King R.A. Image Enhancement Using Fuzzy Sets Electron. Lett. 1980 16 376 378 10.1049/el:19800267 23. Tizhoosh H.R. Fuzzy Image Processing: An Overview Fuzzy Techniques in Image Processing Springer Berlin/Heidelberg, Germany 2005 1 20 10.1007/3-540-32367-8_1 24. Deng H. Deng W. Sun X. Ye C. Zhou X. Adaptive Intuitionistic Fuzzy Enhancement of Brain Tumor MR Images Sci. Rep. 2016 6 357602 10.1038/srep35760 PMC5082372 27786240 25. Balafar M.A. Abdul S. Iqbal M. Rozi M. New Multi-scale Medical Image Segmentation based on Fuzzy C-Mean (FCM) Proceedings of the 2008 IEEE Conference on Innovative Technologies in Intelligent Systems and Industrial Applications Cyberjaya, Malaysia 12–13 July 2008 10.1109/CITISIA.2008.4607337 26. Ananthi V.P. Balasubramaniam P. Image fusion using intuitionistic fuzzy sets Inf. Fusion 2014 20 21 30 10.1016/j.inffus.2013.10.011 27. Buriboev A.S. Rakhmanov K. Soqiyev T. Choi A.J. Improving Fire Detection Accuracy through Enhanced Convolutional Neural Networks and Contour Techniques Sensors 2024 24 5184 10.3390/s24165184 39204881 PMC11360108 28. Buriboev A.S. Abduvaitov A. Jeon H.S. Integrating Color and Contour Analysis with Deep Learning for Robust Fire and Smoke Detection Sensors 2025 25 2044 10.3390/s25072044 40218557 PMC11991653 29. Chen A. Lin D. Gao Q. Enhancing brain tumor detection in MRI images using YOLO-NeuroBoost model Front. Neurol. 2024 15 1445882 10.3389/fneur.2024.1445882 39239397 PMC11374633 30. Isola P. Zhu J.Y. Zhou T. Efros A.A. Image-to-Image Translation with Conditional Adversarial Networks Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Honolulu, HI, USA 21–26 July 2017 5967 5976 10.1109/CVPR.2017.632 31. Yang Q. Yan P. Zhang Y. Yu H. Shi Y. Mou X. Kalra M.K. Zhang Y. Sun L. Wang G. Low-Dose CT Image Denoising Using a Generative Adversarial Network with Wasserstein Distance and Perceptual Loss IEEE Trans. Med. Imaging 2018 37 1348 1357 10.1109/TMI.2018.2827462 29870364 PMC6021013 32. Goodfellow I. Pouget-Abadie J. Mirza M. Xu B. Warde-Farley D. Ozair S. Courville A. Bengio Y. Generative Adversarial Nets Advances in Neural Information Processing Systems MIT Press Cambridge, MA, USA 2014 2672 2680 33. Jiang Z. Ding C. Liu M. Tao D. Two-Stage Cascaded U-Net: 1st Place Solution to BraTS Challenge 2019 Segmentation Task Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries, Proceedings of the BrainLes 2019, Shenzhen, China, 17 October 2019 Lecture Notes in Computer, Science Crimi A. Bakas S. Springer Cham, Switzerland 2020 Volume 11992 239 250 10.1007/978-3-030-46640-4_22 34. Ronneberger O. Fischer P. Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation Medical Image Computing and Computer-Assisted Intervention (MICCAI), Proceedings of the 18th International Conference, Munich, Germany, 5–9 October 2015 Navab N. Hornegger J. Wells W. Frangi A. Springer Cham, Switzerland 2015 234 241 10.1007/978-3-319-24574-4_28 35. Da Cruz L.B. Araújo J.D.L. Ferreira J.L. Diniz J.O.B. Silva A.C. de Almeida J.D.S. de Paiva A.C. Gattass M. Kidney segmentation from computed tomography images using deep neural network Comput. Biol. Med. 2020 123 103906 10.1016/j.compbiomed.2020.103906 32768047 36. Lu C. Yu Y. Luo Q. Adeli X. Wang E. Lu Y. Yuille L. Zhou A. TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Montreal, BC, Canada 11–17 October 2020 10245 10254 37. Zhao W. Jiang D. Queralta J.P. Westerlund T. MSS U-Net: 3D segmentation of kidneys and tumors from CT images with a multi-scale supervised U-Net Inform. Med. Unlocked 2020 19 100357 10.1016/j.imu.2020.100357 38. Liu Z. Lin Y. Cao Y. Hu H. Wei Y. Zhang Z. Lin S. Guo B. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows arXiv 2021 10.48550/arXiv.2103.14030 2103.14030 39. Isensee F. Jaeger P.F. Kohl S.A.A. Petersen J. Maier-Hein K.H. nnU-Net: A Self-Configuring Method for Deep Learning-Based Biomedical Image Segmentation Nat. Methods 2020 18 203 211 10.1038/s41592-020-01008-z 33288961 40. Ibrahim A.U. Ozsoz M. Serte S. Al-Turjman F. Yakoi P.S. Pneumonia Classification Using Deep Learning from Chest X-ray Images During COVID-19 Cogn. Comput. 2024 16 1589 1601 10.1007/s12559-020-09787-5 PMC7781428 33425044 41. Kermany D.S. Goldbaum M. Cai W. Valentim C.C.S. Liang H. Baxter S.L. McKeown A. Yang G. Wu X. Yan F. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning Cell 2018 172 1122 1131.e9 10.1016/j.cell.2018.02.010 29474911 42. Rajpurkar P. Irvin J. Zhu K. Yang B. Mehta H. Duan T. Ding D. Bagul A. Langlotz C. Shpanskaya K. CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning arXiv 2017 1711.05225 43. Hsiao C.-H. Lin P.-C. Chung L.-A. Lin F.Y.-S. Yang F.-J. Yang S.-Y. Wu C.-H. Huang Y. Sun T.-L. A deep learning-based precision and automatic kidney segmentation system using efficient feature pyramid networks in computed tomography images Comput. Methods Programs Biomed. 2022 221 106854 10.1016/j.cmpb.2022.106854 35567864 44. Li D. Xiao C. Liu Y. Chen Z. Hassan H. Su L. Liu J. Li H. Xie W. Zhong W. Deep Segmentation Networks for Segmenting Kidneys and Detecting Kidney Stones in Unenhanced Abdominal CT Images Diagnostics 2022 12 1788 10.3390/diagnostics12081788 35892498 PMC9330428 45. Haghighi M. Warfield S.K. Kurugol S. Automatic renal segmentation in DCE-MRI using convolutional neural networks Proceedings of the 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018) Washington, DC, USA 4–7 April 2018 1 9 10.1109/ISBI.2018.8363865 PMC6248325 30473744 46. Zettler N. Mastmeyer A. Comparison of 2d vs. 3d U-Net Organ Segmentation in abdominal 3d CT images arXiv 2021 10.48550/arXiv.2107.04062 2107.04062 47. Akyel C. Kidney Segmentation with LinkNetB7 J. Adv. Res. Nat. Appl. Sci. 2023 9 844 853 10.28979/jarnas.1228740 48. Nagarajan S. Ramprasath M. Ensemble Transfer Learning-Based Convolutional Neural Network for Kidney Segmentation Int. J. Eng. Trends Technol. 2024 72 446 457 10.14445/22315381/ijett-v72i9p142 49. Myronenko A. 3D MRI Brain Tumor Segmentation Using Autoencoder Regularization Proceedings of the International MICCAI Brainlesion Workshop Granada, Spain 16 September 2018 Springer Cham, Switzerland 2019 311 320 10.1007/978-3-030-11726-9_28 50. Wang H. Xie S. Lin L. Iwamoto Y. Han X.-H. Chen Y.-W. Tong R. Mixed Transformer U-Net for Medical Image Segmentation Proceedings of the ICASSP 2022—2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) Singapore 23–27 May 2022 2390 2394 10.1109/ICASSP43922.2022.9746172 51. Zhang Y. Liu H. Hu Q. ResU-Net: Combining Residual Learning with U-Net for Brain Tumor Segmentation Proceedings of the Medical Image Computing and Computer-Assisted Intervention (MICCAI) Strasbourg, France 27 September–1 October 2021 Springer Cham, Switzerland 2021 245 254 10.1007/978-3-030-87193-2_23 52. Chen L. Papandreou G. Schroff F. Adam H. Rethinking Atrous Convolution for Semantic Image Segmentation arXiv 2017 10.48550/arXiv.1706.05587 1706.05587 53. Rahman T. Chowdhury M.E.H. Khandakar A. Islam K.R. Islam K.F. Mahbub Z.B. Kadir M.A. Kashem S. Transfer Learning with Deep Convolutional Neural Network (CNN) for Pneumonia Detection Using Chest X-ray Appl. Sci. 2020 10 3233 10.3390/app10093233 54. Elshennawy N.M. Ibrahim D.M. Deep-Pneumonia Framework Using Deep Learning Models Based on Chest X-Ray Images Diagnostics 2020 10 649 10.3390/diagnostics10090649 32872384 PMC7554804 55. Talaat M. Si X. Xi J. Multi-Level Training and Testing of CNN Models in Diagnosing Multi-Center COVID-19 and Pneumonia X-ray Images Appl. Sci. 2023 13 10270 10.3390/app131810270 Figure 1 Dataset improvements. Figure 2 Image enhancement steps. Figure 3 Transformations of local contrasts based on histogram length functions: ( a b Figure 4 CNN architecture for a single dataset. Figure 5 Training and validation loss curves. Figure 6 The visual results of kidney segmentation. sensors-25-05931-t001_Table 1 Table 1 Numerical example of steps 1–3 for a 2 × 2 image. Pixel Position Original Intensity (0–255) Normalized Intensity (Step 1) Membership Value (Step 2) Refined Membership (Step 3) (1, 1) 100 0.392 0.253 0.184 (1, 2) 150 0.588 0.731 0.668 (2, 1) 200 0.784 0.947 0.933 (2, 2) 50 0.196 0.068 0.036 sensors-25-05931-t002_Table 2 Table 2 BRISQUE scores for each dataset. Dataset Original Histogram-Based Fuzzy Entropy-Based Fuzzy Standard Deviation-Based Chest X-ray 26.8 22.4 21.1 22.9 BraTS2020 (MRI) 30.3 24.9 23.1 23.5 KiTS19 (CT) 28.8 23.5 22.8 22.4 sensors-25-05931-t003_Table 3 Table 3 Information about the parameters of the proposed CCNN model. Layer Input 1 Input 2 Input 3 Convolutional Layer 1 64 filters, 3 × 3 kernel, ReLU activation 64 filters, 3 × 3 kernel, ReLU activation 64 filters, 3 × 3 kernel, ReLU activation Max Pooling Layer 1 2 × 2 pooling, stride = 2 2 × 2 pooling, stride = 2 2 × 2 pooling, stride = 2 Convolutional Layer 2 128 filters, 3 × 3 kernel, ReLU activation 128 filters, 3 × 3 kernel, ReLU activation 128 filters, 3 × 3 kernel, ReLU activation Max Pooling Layer 2 2 × 2 pooling 2 × 2 pooling 2 × 2 pooling Convolutional Layer 3 256 filters, 3 × 3 kernel, ReLU activation 256 filters, 3 × 3 kernel, ReLU activation 256 filters, 3 × 3 kernel, ReLU activation Max Pooling Layer 3 2 × 2 pooling, resulting in feature map size 64 × 64 × 256 2 × 2 pooling, resulting in feature map size 64 × 64 × 256 2 × 2 pooling, resulting in feature map size 64 × 64 × 256 Concatenation Layer Concatenate the feature maps from all three branches along the last axis. Final feature map size: 64 × 64 × 768 (256 from each branch). Convolutional Layer 4 512 filters, 3 × 3 kernel, ReLU activation Max Pooling Layer 4 2 × 2 pooling, reducing the feature map size to 32 × 32 × 512 Convolutional Layer 5 1024 filters, 3 × 3 kernel, ReLU activation Max Pooling Layer 5 2 × 2 pooling, reducing the feature map size to 16 × 16 × 1024 Flatten Layer Flatten the 3D feature map to a 1D vector Dense Layer 1 1024 neurons, ReLU activation Dropout Layer 1 Dropout rate = 0.5 Dense Layer 2 512 neurons, ReLU activation Dropout Layer 2 Dropout rate = 0.5 Output Layer Two neurons (pneumonia or normal for classification; background or tumor for segmentation), softmax activation sensors-25-05931-t004_Table 4 Table 4 Impact of image enhancement on segmentation accuracy for the KiTS19 dataset. Dataset BRISQUE Value Neural Network Accuracy Original KiTS19 28.8 CNN 0.921 KiTS19 filtered by CLAHE 26.4 CNN 0.959 KiTS19 filtered by our method 21.7 Concatenated CNN 0.983 sensors-25-05931-t005_Table 5 Table 5 KiTS19 single-type vs. three-type comparison. Dataset Type Model Accuracy BRISQUE Fuzzy entropy CNN 0.971 23.8 Three-Type Fusion CCNN 0.983 22.1 sensors-25-05931-t006_Table 6 Table 6 KiTS19 comparison with fuzzy baselines. Method Accuracy BRISQUE Fuzzy c-means (FCM) 0.905 27.0 Fuzzy Entropy 0.971 23.8 Three-Type Fusion (CCNN) 0.989 21.7 sensors-25-05931-t007_Table 7 Table 7 Performance of our CCNN model with alternatives. Reference Method Dice Coefficient (%) Precision (%) Sensitivity (%) Recall (%) Dataset Hsiao et al., 2022 [ 43 EfficientNetB5 96.90 97.47 - 96.45 KiTS19 Da Cruz et al., 2020 [ 35 UNet2d 96.33 - 95.32 - KiTS19 Zhao et al., 2020 [ 37 UNet3d 96.90 97.10 - 96.80 KiTS19 Li et al., 2022 [ 44 ResUnet 96.54 - 96.49 - Own Haghighi et al., 2018 [ 45 UNet3d 87.50 92.70 - - DCE-MRI UNet2d [ 46 UNet2d 96.50 96.55 95.90 96.20 KiTS19 UNet3d [ 46 UNet3d 96.80 96.85 96.10 96.25 KiTS19 Cihan Akyel, 2023 [ 47 LinkNet 96.62 96.58 96.97 96.18 KiTS19 Cihan Akyel, 2023 [ 47 LinkNetB7 97.20 97.30 97.00 97.00 KiTS19 Ensemble CNN [ 48 Ensemble CNN 85.00 91.00 - 87.00 KiTS19 Proposed Model CCNN 99.60 98.70 99.30 98.60 KiTS19 sensors-25-05931-t008_Table 8 Table 8 Impact of image enhancement on segmentation accuracy for the BraTS2020 dataset. Dataset BRISQUE Value Neural Network Accuracy Original BraTS2020 30.3 CNN 0.853 BraTS2020 filtered by CLAHE 27.6 CNN 0.879 BraTS2020 filtered by our method 23.4 Concatenated CNN 0.917 sensors-25-05931-t009_Table 9 Table 9 Performance of our CCNN model. Reference Method Dice Coefficient (%) Precision (%) Sensitivity (%) Recall (%) Dataset Isensee et al., 2020 [ 39 nnU-Net 88.90 89.50 88.30 88.70 BraTS2020 Myronenko, 2019 [ 49 3D Autoencoder 87.50 88.20 87.00 87.30 Jiang et al., 2020 [ 33 Two-Stage U-Net 86.70 87.40 86.20 86.50 Lu et al., 2020 [ 36 TransUNet 89.20 90.00 88.80 89.10 Wang et al., 2021 [ 50 3D U-Net++ 87.80 88.50 87.40 87.60 Isensee et al., 2020 [ 39 Swin Transformer 88.50 89.10 88.00 88.40 Zhang et al., 2021 [ 51 ResU-Net 86.90 87.60 86.50 86.80 Chen et al., 2017 [ 52 DeepLabV3+ 87.30 88.00 86.90 87.20 Proposed Model CCNN 91.50 92.00 91.20 91.40 sensors-25-05931-t010_Table 10 Table 10 Impact of image enhancement on classification accuracy for the chest X-ray dataset. Dataset BRISQUE Value Neural Network Accuracy Original Chest X-ray 26.8 Classic CNN 0.871 Chest X-ray filtered by CLAHE 25.6 Classic CNN 0.917 Chest X-ray filtered by our method 22.1 Concatenated CNN 0.989 sensors-25-05931-t011_Table 11 Table 11 Performance of our CCNN across models. Neural Network Model Accuracy Precision AUC F1-Score Recall Proposed CCNN 0.989 0.993 0.987 0.998 0.996 Rahman T. et al. [ 53 0.981 0.992 0.981 0.972 0.981 MobileNetV2 [ 54 0.964 0.994 0.975 0.956 0.970 CNN [ 54 0.922 0.920 0.937 0.955 0.969 LSTM-CNN [ 54 0.918 0.926 0.922 0.934 0.954 AlexNet [ 55 0.805 0.431 0.981 0.992 0.856 ResNet-50 [ 55 0.867 0.645 0.971 0.999 0.914 MobileNet [ 55 0.834 0.559 0.969 0.989 0.879 VGG-19 [ 55 0.821 0.568 0.941 0.987 0.837 ",
  "metadata": {
    "Title of this paper": "Multi-Level Training and Testing of CNN Models in Diagnosing Multi-Center COVID-19 and Pneumonia X-ray Images",
    "Journal it was published in:": "Sensors (Basel, Switzerland)",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12473765/"
  }
}
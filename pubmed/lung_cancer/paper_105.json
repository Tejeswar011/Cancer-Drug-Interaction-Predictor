{
  "title": "Paper_105",
  "abstract": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12489112 PMC12489112.1 12489112 12489112 41034242 10.1038/s41598-025-12141-0 12141 1 Article Graph neural network model using radiomics for lung CT image segmentation Faizi Mohammad Khalid khalidfaizi840@gmail.com 1 Qiang Yan 1 5 Shagar Md Masum Billa 1 Wei Yangyang 2 Qiao Ying 2 Zhao Juanjuan zhaojuanjuan@tyut.edu.cn 1 3 4 Urrehman Zia 1 1 https://ror.org/03kv08d37 grid.440656.5 0000 0000 9491 9632 Taiyuan University of Technology, College of Computer Science and Technology (College of Data Science), 2 https://ror.org/02vzqaq35 grid.452461.0 0000 0004 1762 8478 First Hospital of Shanxi Medical University, 3 https://ror.org/03kv08d37 grid.440656.5 0000 0000 9491 9632 School of Software, Taiyuan University of Technology, 4 https://ror.org/01gcwk603 0000 0005 0895 0905 Jinzhong College of Information, College of Information, 5 https://ror.org/047bp1713 grid.440581.c 0000 0001 0372 1100 North University of China, School of Software, 1 10 2025 2025 15 478255 34148 14 2 2025 15 7 2025 01 10 2025 03 10 2025 03 10 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access http://creativecommons.org/licenses/by-nc-nd/4.0/ Early detection of lung cancer is critical for improving treatment outcomes, and automatic lung image segmentation plays a key role in diagnosing lung-related diseases such as cancer, COVID-19, and respiratory disorders. Challenges include overlapping anatomical structures, complex pixel-level feature fusion, and intricate morphology of lung tissues all of which impede segmentation accuracy. To address these issues, this paper introduces GEANet, a novel framework for lung segmentation in CT images. GEANet utilizes an encoder-decoder architecture enriched with radiomics-derived features. Additionally, it incorporates Graph Neural Network (GNN) modules to effectively capture the complex heterogeneity of tumors. Additionally, a boundary refinement module is incorporated to improve image reconstruction and boundary delineation accuracy. The framework utilizes a hybrid loss function combining Focal Loss and IoU Loss to address class imbalance and enhance segmentation robustness. Experimental results on benchmark datasets demonstrate that GEANet outperforms eight state-of-the-art methods across various metrics, achieving superior segmentation accuracy while maintaining computational efficiency. Keywords Lung CT image Segmentation Radiomics Hybrid loss function Feature fusion Subject terms Non-small-cell lung cancer Small-cell lung cancer Computational biology and bioinformatics Machine learning National Natural Science Foundation of China (NSFC) 62376183 Central guidance for local scientific and technological development funds; the central government guides local science and technology development funding projects YDZJSX2022C004 National Natural Science Foundation of China U21A20469 Supported by the Special Fund for Science and Technology Innovation Teams of Shanxi Province 202304051001009 National Natural Science foundation of China (NSFC) 62476190 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © Springer Nature Limited 2025 Introduction Lung cancer represents the largest proportion of cancer cases globally, with non-small cell lung cancer (NSCLC) being the most common variant of this disease 1 Radiomics is a methodology that entails the examination of medical pictures to derive diverse tumor attributes 2 3 4 5 6 Graph Neural Networks (GNNs) offer a complementary approach to radiomics by leveraging graph-structured data to capture spatial and feature relationships. GNNs aggregate and propagate information hierarchically, enabling the modeling of complex tumor behavior and metastasis prediction 7 8 This study proposes GEANet, a novel framework that integrates radiomics and GNNs within a CT-based encoder-decoder architecture. By addressing challenges in feature extraction, segmentation accuracy, and model interpretability, GEANet aims to improve the diagnosis and management of lung cancer, fostering trust in clinical applications. Related work Deep learning approaches, particularly those based on convolutional neural networks (CNNs), have revolutionized segmentation by learning complex features from large datasets. U-Net 9 10 11 12 13 14 15 Recent advancements in feature fusion have aimed to overcome these limitations. Fusion modules integrate multi-scale features for enhanced segmentation, as seen in models like IU-Net 16 17 18 19 In parallel, hybrid CNN-Transformer architectures have emerged as powerful tools for integrating local and global context. For instance, EFFResNet-ViT 20 21 Despite these advances, challenges persist-particularly in handling semantic gaps between modalities, class imbalance, and complex boundary delineation. This study builds upon these directions by integrating radiomics features with Graph Neural Networks (GNNs), enabling structural relationship modeling and quantitative feature encoding. The proposed GEANet leverages this synergy to overcome limitations of conventional CNN-based models, particularly in capturing subtle lung tissue boundaries and spatial heterogeneity. Methods Our proposed methodology consists of five interconnected phases within the overall pipeline for lung segmentation, as depicted in Fig. 1 Fig. 1 Overall framework of GEANet with Radiomics and EncoderDecoder GNN module. Radiomics Radiomics features, including texture (e.g., gray-level co-occurrence matrix) and shape descriptors (e.g., perimeter and area), were incorporated to enrich node representations in the GNN. The motivation stems from the clinical relevance of radiomics in characterizing lung tissue properties, such as heterogeneity and morphological patterns, which are critical for accurate segmentation. Unlike raw pixel intensities, radiomics provide quantitative metrics that enhance the model’s ability to differentiate pathological from healthy tissue, improving robustness across diverse CT imaging conditions. Radiomics features are extracted using tools like PyRadiomics, enabling quantitative analysis of lung nodules in CT images. These features enhance the understanding of the region of interest (ROI) by capturing diverse image attributes. Key filters, including exponential, Local Binary Patterns (LBP), logarithm, square, and wavelet, are applied to highlight distinct image features. Extracted features include texture, first-order statistics, and shape descriptors, which provide insights into pixel relationships, statistical distributions, and nodule morphology, respectively. Incorporating radiomics features into the encoder module enhances its capacity to capture critical spatial and morphological information for segmentation tasks, as illustrated in Fig. 2 Fig. 2 Radiomics module. Encoder The GNN module, based on Graph Attention Networks (GAT), was motivated by the need to capture spatial and structural relationships in CT images. Lung nodules often exhibit irregular shapes and varying textures, which are difficult to model using traditional convolutional neural networks (CNNs) alone. The revamped encoder module features a GNN block that utilizes GAT in conjunction with ReLU activation, Batch Normalization (BN), and Dropout layers, applied iteratively over three layers. This is succeeded by mean pooling to efficiently aggregate features. This integration enhances the model’s capacity to collect both local and global structural information, essential for lung nodule segmentation tasks. The input CT image, denoted as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X \\in \\mathbb {R}^{H \\times W \\times C}$$\\end{document} H W C \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P = \\frac{H}{S} \\times \\frac{W}{S}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S \\times S$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G = (V, E)$$\\end{document} V P E \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i \\in \\mathbb {R}^d$$\\end{document} d \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F = [f_1, f_2, \\dots , f_P] \\in \\mathbb {R}^{P \\times d}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A \\in \\mathbb {R}^{P \\times P}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A_{ij} = 1$$\\end{document} i j \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A_{ij} = 0$$\\end{document} i j \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A_{ij} = 1$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_i$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_j$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta = 0.8$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A_{ij} = 1$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{f_i \\cdot f_j}{\\Vert f_i\\Vert \\Vert f_j\\Vert } > 0.8$$\\end{document} The graph representation is processed through a sequence of GAT layers within the GNN block 3 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _{ij}$$\\end{document} i j 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\alpha _{ij} = \\text {Softmax}_{j} (\\text {LeakyReLU}(a^{T}[W_{Q}f_{i}||W_{K}f_{j}])) \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_{Q}, W_{K} \\in \\mathbb {R}^{d' \\times d}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha$$\\end{document} 2 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} f'_{i} = \\sigma \\Bigl ( \\sum _{j \\in N(i)} \\alpha _{ij}W_{V}f_{i} \\Bigl ), \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_{V} \\in \\mathbb {R}^{d' \\times d}$$\\end{document} N i i \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$g \\in \\mathbb {R}^{d'}$$\\end{document} 3 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} g = \\frac{1}{P} \\sum _{i = 1}^{P} Dropout(BN(f'_{i}) \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f'_{i}$$\\end{document} i 3 g \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Z_{\\text {transformer}} \\in \\mathbb {R}^{d''}$$\\end{document} 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Z = \\text {Concat}(g, Z_{transformer}), \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$g \\in \\mathbb {R}^{d'}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Z_{\\text {transformer}} \\in \\mathbb {R}^{d''}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Z = \\text {Concat}(g, Z_{\\text {transformer}})$$\\end{document} g \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Z_{\\text {transformer}}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Z \\in \\mathbb {R}^{d' + d''}$$\\end{document} Fig. 3 GNN module. Decoder The decoder comprises four stages, as illustrated in Fig. 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3 \\times 3$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{H}{32} \\times \\frac{W}{32} \\times 8C$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{H}{16} \\times \\frac{W}{16} \\times 4C$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{H}{16} \\times \\frac{W}{16} \\times 4C$$\\end{document} Boundary refinement The decoder output struggles to accurately delineate boundaries between lung and non-lung regions due to their fine details and ambiguous nature. To address this limitation, we propose a boundary refinement module, shown in Fig. 4 The module employs a convolutional block (Conv block) consisting of a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3 \\times 3$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F \\in \\mathbb {R}^{H \\times W \\times C_1}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_1$$\\end{document} F \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E \\in \\mathbb {R}^{H \\times W \\times C_2}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_2 = 2$$\\end{document} 5 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} E = \\text {ReLU}(\\text {BN}(\\text {Conv}_{3 \\times 3}(F))) \\end{aligned}$$\\end{document} E 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} E' = \\text {ReLU}(\\text {BN}(E)) \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E'$$\\end{document} F \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_{\\text {fused}} \\in \\mathbb {R}^{H \\times W \\times (C_1 + C_2)}$$\\end{document} 7 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} F_{\\text {fused}} = \\text {Concat}(F, E') \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1 \\times 1$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M_{\\text {edge}} \\in \\mathbb {R}^{H \\times W \\times 1}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M_{\\text {binary}} \\in \\mathbb {R}^{H \\times W \\times 1}$$\\end{document} 8 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} M_{\\text {edge}} = \\text {Sigmoid}(\\text {Conv}_{1 \\times 1}(F_{\\text {fused}})), \\quad M_{\\text {binary}} = \\text {Sigmoid}(\\text {Conv}_{1 \\times 1}(F_{\\text {fused}})) \\end{aligned}$$\\end{document} 4 Fig. 4 Boundary Refinement EA Module, illustrating the extraction and fusion of edge features to produce edge and binary change maps. Hybrid loss We propose a hybrid loss function that integrates focal loss and IoU loss to improve regional segmentation and boundary accuracy 9 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Loss = \\lambda _{1} Loss_{focal} + \\lambda _{2} Loss_{focal} + \\lambda _{3} Loss_{iou} \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda$$\\end{document} 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda _{1} = 0.3$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda _{2} = 0.4$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda _{3} = 0.3$$\\end{document} 10 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Loss_{total} = Loss_{prim} + \\sum _{n=1}^{4} Loss_{sup}^{n} \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Loss_{prim}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Loss_{sup}$$\\end{document} Experiments Dataset The lung segmentation model was trained and evaluated using two publicly available datasets: the LIDC-IDRI dataset (Dataset Version 1) and the Chest CT Cancer Images Dataset from Kaggle (Dataset Version 2). The LIDC-IDRI dataset comprises 1018 thoracic CT scan cases from 1010 patients, stored in DICOM format at a resolution of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$512 \\times 512$$\\end{document} In total, 2500 labeled images (1800 from LIDC-IDRI and 700 from the Chest CT Cancer Images Dataset) were prepared for model development. The dataset was partitioned into 2350 images (94%) for training and 150 images (6%) for testing. This partitioning deviates from common deep learning practices, which typically adopt a 7:2:1 or 8:1:1 ratio for training, validation, and test sets, allocating 10–20% of data for testing. The choice of a 6% test set was motivated by the need to maximize training data to capture the diverse anatomical variations and pathological conditions present in the combined datasets, particularly given the labor-intensive manual labeling process. To ensure robust model evaluation despite the smaller test set, the 150 test images were carefully selected to be representative of the datasets’ diversity, including variations in cancer types, lung tissue characteristics, and imaging artifacts. Increasing training data was necessary to improve the model’s ability to generalize across varied anatomical structures and pathological patterns, especially when leveraging two heterogeneous datasets. To mitigate risks associated with a smaller test set and the absence of a fixed validation split, we employed 5-fold cross-validation within the training set. Each fold used 470 images (20%) for validation, enabling robust hyperparameter tuning and monitoring of generalization performance across diverse subsets of the data. This setup served as a rigorous internal validation mechanism, with consistent cross-validation performance (mean Dice Similarity Coefficient [DSC] = 0.90 ± 0.02) across folds. Furthermore, to ensure that the 6% test set was not biased or unrepresentative, we conducted a statistical analysis comparing the distribution of radiomics features (e.g., texture, shape) between the test and training sets. A Kolmogorov–Smirnov test confirmed that the distributions were statistically similar (p > 0.05), supporting the adequacy of the test set for generalization assessment. The model achieved a test set DSC of 0.91, consistent with cross-validation outcomes, reinforcing confidence in its generalization performance. These results suggest that the test set, though small, effectively reflects the complexity and variability present in the full dataset. Nonetheless, to align with standard practices and further validate robustness, future work will explore adopting a more conventional split (e.g., 7:2:1) should additional labeled data become available. Evaluation metrics We utilized various metrics to thoroughly assess the performance of our network. This set of metrics encompasses measures pertinent to semantic segmentation tasks. This section outlines the five metrics utilized for evaluation. Intersection over Union (IoU) is the principal metric used for evaluating semantic segmentation. Furthermore, we compute the subsequent standard metrics. We compare our GEANet network with other state-of-the-art approaches on our lung segmentation dataset, utilizing the same training configuration of 1600 training images. Segmentation models include Unet 22 23 24 12 25 26 17 27 28 29 30 31 Table 1 Performance comparison on Dataset Version 1 (LIDC-IDRI) test set, including IoU, F1-score, Precision, Recall, Accuracy, and GPU memory. Method IoU F1-score Precision Recall Accuracy GPU Memory (GB) U-Net 94.07 95.03 98.15 99.45 95.85 6.5 RU-Net 94.67 96.33 95.65 97.45 97.10 6.8 ResNet34-Unet 95.38 97.83 97.29 98.25 96.85 7.0 BCDU-Net 96.10 98.51 99.00 98.01 97.20 7.2 ResBCDUnet 97.15 98.03 99.09 97.58 97.06 7.4 NasNet 97.07 98.53 98.05 97.45 98.85 8.0 DABT-U-Net 96.65 97.69 98.21 98.15 98.35 7.6 ABANet 96.57 97.70 98.25 98.35 99.15 7.5 TransUNet 96.80 97.80 97.90 98.50 98.90 9.5 UKAN 97.20 98.10 98.30 98.70 99.20 7.8 GEANet 98.07 99.03 98.65 99.45 99.85 8.2 Significant values are in bold. Results The proposed GEANet model was evaluated on two versions of a lung segmentation dataset, comprising 2,500 labeled images Version 1 (1800 images from LIDC-IDRI, DICOM format, 512 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} 1 3 Tables 1 3 2 GEANet’s ability to delineate hazy edges and complex lung structures is evident in Fig. 5 4 Preprocessing techniques (intensity normalization, augmentation) improved IoU by 1.2%, ensuring robustness across CT imaging variations. GPU memory analysis highlights GEANet’s efficiency (8.2 GB) compared to TransUNet’s heavier 9.5 GB, despite similar performance, making GEANet suitable for clinical deployment. Additional ablation studies confirm the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$320 \\times 320$$\\end{document} 5 6 Table 2 Cross-validation results (5-fold) on the training set, showing mean and standard deviation for IoU and F1-score. Metric Mean Std. dev. IoU 97.95% 0.15% F1-score 98.90% 0.12%  Fig. 5 Comparison of lung dataset outcomes with state-of-the-art image segmentation techniques. The initial two rows exhibit the original images alongside the corresponding ground truth. Rows 3 to 8 display the segmentation outcomes obtained using the suggested technique, Unet, RU-Net, ResNet34-Unet, BCDU-Net, and ResBCDUnet, respectively. Table 3 Performance comparison on Dataset Version 2 (chest CT cancer images) test set. Method IoU F1-score Precision Recall Accuracy GPU Memory (GB) U-Net 94.07 95.03 95.65 96.45 98.85 6.5 RU-Net 95.23 95.64 96.35 96.41 97.25 6.8 ResNet34-Unet 93.07 94.03 96.35 97.45 98.85 7.0 BCDU-Net 95.07 97.86 98.78 98.45 97.85 7.2 ResBCDUnet 95.56 96.68 98.61 98.95 99.05 7.4 NasNet 95.08 96.03 98.34 97.76 98.00 8.0 DABT-U-Net 96.37 97.03 98.05 98.64 98.15 7.6 ABANet 97.09 98.03 98.34 98.25 98.06 7.5 TransUNet 96.80 97.80 97.90 98.50 98.90 9.5 U-KAN 97.20 98.10 98.30 98.70 99.20 7.8 GEANet  98.07  99.03  98.83  99.45  99.85 8.2 Significant values are in bold.  Fig. 6 Comparison of lung dataset outcomes with state-of-the-art image segmentation techniques. The initial two rows exhibit the original images alongside the corresponding ground truth. In summary, although advanced techniques frequently encounter challenges such as over-segmentation or under-segmentation in complex scenes, our method demonstrates a remarkable ability to accurately maintain lung boundaries and structures. Our network demonstrates a superior ability to delineate boundaries and edges when compared to current methodologies. Ablation studies This section presents ablation studies conducted to assess the effectiveness of GEANet, utilizing the proposed dataset as demonstrations. To evaluate the effectiveness of GEANet’s components, we conducted comprehensive ablation studies on the lung segmentation dataset. We assessed five variants: Baseline + GNN, Baseline + Radiomics, Baseline + GNN + Radiomics, Baseline + GNN + Radiomics + Concatenation, and the full model (Baseline + GNN + Radiomics + Concatenation + Boundary Refinement). Experiments were standardized (batch size, learning rate) and evaluated using IoU, F1-score, Precision, Recall, Accuracy, and GPU memory consumption (in GB, measured on an NVIDIA RTX 3090). The GNN module, motivated by the need to capture spatial and structural relationships in lung nodules, uses Graph Attention Networks (GAT) to model patch-based graphs with edges based on 4-neighbor connectivity and cosine similarity (>0.8). Radiomics features (e.g., texture, shape descriptors) were included to quantify tissue properties, enhancing differentiation of pathological regions. The integration of radiomics features with the GNN module significantly enhances segmentation accuracy by combining quantitative tissue characterization with relational spatial modeling. Radiomics captures handcrafted descriptors-such as texture, shape, and intensity heterogeneity-which are particularly useful for identifying pathological lung regions that may not be easily distinguishable through standard CNN feature maps. However, when used alone, radiomics lacks the spatial context needed to fully delineate complex anatomical structures. The GNN addresses this limitation by modeling patch-level relationships using graph attention, allowing GEANet to propagate information across structurally similar or spatially adjacent regions. This fusion enables the network to better distinguish lung tissue from background or artifacts, even under compression noise (Version 2) or subtle tissue variation (Version 1). In contrast, traditional CNN-based models process local neighborhoods through fixed-size convolutional kernels and often fail to capture global structural context or utilize domain-specific descriptors. As shown in the ablation study (Table 4 Feature concatenation fuses GNN and transformer outputs, preserving complementary structural and semantic features, as validated previously. The boundary refinement module, addressing ambiguous lung boundaries, refines edge predictions via convolutional feature fusion. These components collectively tackle challenges like hazy edges and complex lung structures, improving segmentation accuracy. Table 4 Boundary refinement provides the final 0.42% IoU increase, sharpening edges. The boundary refinement module plays a critical role in enhancing the segmentation of hazy, low-contrast, or complex lung boundaries, which are common in thoracic CT images. It operates by fusing multiscale convolutional features to better distinguish edge pixels from background, reducing over- or under-segmentation at boundary regions. This is especially useful where conventional semantic segmentation models fail to capture subtle anatomical transitions. The module’s effectiveness is quantitatively evaluated in the ablation study (Table 4 6 Fig. 6 We also evaluated GEANet’s adaptability to input resolutions (160 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} 5 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} 6 Table 4 Ablation study results on the test set, evaluating the contribution of each component using IoU, F1-score, Precision, Recall, and Accuracy. Method IoU F1-score Precision Recall Accuracy Baseline 95.52 96.63 97.46 95.82 97.15 Baseline + GNN 95.64 96.25 94.46 94.82 94.95 Baseline + Radiomics 95.21 96.25 94.46 94.82 94.95 Baseline + GNN + Radiomics 97.15 98.05 98.46 98.82 99.05 Baseline + GNN + Radiomics + Concatenation 97.65 98.53 98.65 99.05 99.45 GEANet Model 98.07 99.03 98.65 99.45 99.85 Significant values are in bold. Table 5 Performance and GPU memory usage with various input resolutions. Resolution IoU GPU Memory (GB) 160 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} 97.85 6.0 320 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} 98.25 8.0 640 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} 97.70 9.5 Significant values are in bold. Table 6 Effect of loss functions on GEANet performance. Loss IoU F1-score Accuracy Focal 93.25 92.25 96.95 IoU 94.85 95.25 95.95 Focal + IoU 95.85 98.25 97.95 Significant values are in bold. These ablation studies demonstrate that GNN, radiomics, concatenation, boundary refinement, and hybrid loss each address specific lung segmentation challenges, with their combination in GEANet yielding state-of-the-art performance, validated by consistent metrics and visualizations. Conclusion This paper presents a hybrid framework, GEANet, utilizing manually extracted features using radiomics and graph neural network based features. Radiomics features provide quantitative information about txture, shape and intensity of CT images, while GNN models excel at learning complex patterns and representations for classification tasks. The GEANet methodology in particular, has demonastrated success due to its ability to capture intricate features in images. The GEANet employs an encoder-decoder framework that integrates multiple modules for classification tasks in lung CT images, featuring a novel hybrid loss function. Following the selection of features, the incorporation of radiomics-based attributes enhances the GNN modules to enhance the quality of the reconstructed image, we introduce the boundary refinement module. The experiments conducted on the proposed datasets indicate that GEANet surpasses eight leading methods across various evaluation metrics. The advantages of GEANet encompass its capacity to capture intricate features in Radiomics, hence improving segmentation precision while preserving computing economy. The integration of a hybrid loss function, merging Focal Loss and IoU Loss, substantially enhances the model’s resilience in addressing class imbalance and refining boundary delineation. Supplementary Information  Supplementary Information. Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Mohammad Khalid Faizi, Yan Qiang, Md Masum Billa Shagar, Yangyang Wei, Ying Qiao and Zia Urrehman contributed equally to this work. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-025-12141-0. Author contributions Conceptualization: Mohammad Khalid Faizi; Writing-original draft preparation: Mohammad Khalid Faizi, Md Masum Billa Shagar; review and editing: Juanjuan Zhao, Yan Qiang , Yangyang Wei and Ying Qiao, Zia Urrehman; supervision: Juanjuan Zhao. All authors have read and agreed to the published version of the manuscript. All authors reviewed the manuscript. Data availability The datasets used and analyzed during the current study are available from the corresponding author upon reasonable request. Please contact Juanjuan Zhao at zhaojuanjuan@tyut.edu.cn for further information. The Luna16 Dataset publically available on : https://luna16.grand-challenge.org/ Declarations Competing interests The authors declare no competing interests. References 1. Meza, R., Meernik, C., Jeon, J. & Cote, M. L. Lung cancer incidence trends by gender, race and histology in the United States, 1973-2010. PLoS ONE 10 10.1371/journal.pone.0121323 PMC4379166 25822850 2. Kumar V Radiomics: The process and the challenges Magnet. Resonan. Imaging 2012 30 9 1234 48 10.1016/j.mri.2012.06.010 PMC3563280 22898692 Kumar, V. et al. Radiomics: The process and the challenges. Magnet. Resonan. Imaging 30 10.1016/j.mri.2012.06.010 PMC3563280 22898692 3. Eloyan A Yue MS Khachatryan D Tumor heterogeneity estimation for radiomics in cancer Stat. Med. 2020 39 4704 4723 10.1002/sim.8749 32964647 PMC8244619 Eloyan, A., Yue, M. S. & Khachatryan, D. Tumor heterogeneity estimation for radiomics in cancer. Stat. Med. 39 32964647 10.1002/sim.8749 PMC8244619 4. Roy MS Donington JS Management of locally advanced non small cell lung cancer from a surgical perspective Curr. Treat. Opt. Oncol. 2007 8 1 14 10.1007/s11864-007-0023-3 17634831 Roy, M. S. & Donington, J. S. Management of locally advanced non small cell lung cancer from a surgical perspective. Curr. Treat. Opt. Oncol. 8 10.1007/s11864-007-0023-3 17634831 5. Lambin P Radiomics: Extracting more information from medical images using advanced feature analysis Eur. J. Cancer 2012 48 4 441 6 10.1016/j.ejca.2011.11.036 22257792 PMC4533986 Lambin, P. et al. Radiomics: Extracting more information from medical images using advanced feature analysis. Eur. J. Cancer 48 22257792 10.1016/j.ejca.2011.11.036 PMC4533986 6. Bowen, S. R. et al. Tumor radiomic heterogeneity: Multiparametric functional imaging to characterize variability and predict response following cervical cancer radiation therapy. J. Magnet. Reson. Imaging 47 10.1002/jmri.25874 PMC5899626 29044908 7. Scarselli F Gori M Tsoi AC Hagenbuchner M Monfardini G The graph neural network model IEEE Trans. Neural Netw. 2009 20 61 80 10.1109/TNN.2008.2005605 19068426 Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M. & Monfardini, G. The graph neural network model. IEEE Trans. Neural Netw. 20 19068426 10.1109/TNN.2008.2005605 8. Veyrin-Forrer L Kamal A Duffner S Plantevit M Robardet C On gnn explainability with activation rules Data Min. Knowl. Discov. 2022 38 3227 3261 10.1007/s10618-022-00870-z Veyrin-Forrer, L., Kamal, A., Duffner, S., Plantevit, M. & Robardet, C. On gnn explainability with activation rules. Data Min. Knowl. Discov. 38 9. Yoo H-J Deep convolution neural networks in computer vision: A review IEIE Trans. Smart Process. Comput. 2015 4 35 43 10.5573/IEIESPC.2015.4.1.035 Yoo, H.-J. Deep convolution neural networks in computer vision: A review. IEIE Trans. Smart Process. Comput. 4 10. Liu, Y., Yao, S., Wang, X., Chen, J. & Li, X. MD-UNET: A medical image segmentation network based on mixed depthwise convolution. Med. Biol. Eng. Comput. 10.1007/s11517-023-03005-8 38158549 11. Kumar, A., Agrawal, R. K. & Joseph, L. Itermiunet: A lightweight architecture for automatic blood vessel segmentation. Multimed. Tools Appl. 12. Azad, R., Asadi-Aghbolaghi, M., Fathy, M. & Escalera, S. Bi-directional convlstm u-net with Densley connected convolutions. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) 13. Oktay, O. et al. Attention u-net: Learning where to look for the pancreas. arXiv fabs/1804.03999 (2018). 14. Yang, D., Zhao, H., Yu, K. & Geng, L. Naunet: lightweight retinal vessel segmentation network with nested connections and efficient attention. Multimed. Tools Appl. 15. Chen R Yan X Wang S Xiao G Da-net: Dual-attention network for multivariate time series classification Inf. Sci. 2022 610 472 487 10.1016/j.ins.2022.07.178 Chen, R., Yan, X., Wang, S. & Xiao, G. Da-net: Dual-attention network for multivariate time series classification. Inf. Sci. 610 16. Jiang, Y. et al. IU-NET: A hybrid structured network with a novel feature fusion approach for medical image segmentation. BioData Min. 16 10.1186/s13040-023-00320-6 PMC9942350 36805687 17. Zhang, Y. et al. Automatic head overcoat thickness measure with NASNET-large-decoder net. arXiv abs/2106.12054 (2021). 18. Xu X Lei M Liu D Wang M Lu L Lung segmentation in chest X-ray image using multi-interaction feature fusion network IET Image Process. 2023 17 4129 4141 10.1049/ipr2.12923 Xu, X., Lei, M., Liu, D., Wang, M. & Lu, L. Lung segmentation in chest X-ray image using multi-interaction feature fusion network. IET Image Process. 17 19. Zhang Y Chen X Wang H DCSSGA-UNET: Biomedical image segmentation with Densenet channel spatial and semantic guidance attention Knowl.-Based Syst. 2025 287 111574 Zhang, Y., Chen, X. & Wang, H. DCSSGA-UNET: Biomedical image segmentation with Densenet channel spatial and semantic guidance attention. Knowl.-Based Syst. 287 20. Ahmed S Javed A Rahman A Effresnet-vit: A fusion-based convolutional and vision transformer model for explainable medical image classification IEEE Access 2024 12 12345 12359 Ahmed, S. et al. Effresnet-vit: A fusion-based convolutional and vision transformer model for explainable medical image classification. IEEE Access 12 21. Rahman MM An integrated approach using yolov8 and resnet, seresnet & vision transformer (VIT) algorithms based on ROI fracture prediction in x-ray images of the elbow Curr. Med. Imaging Rev. 2024 20 153 167 10.2174/0115734056309890240912054616 39360542 Rahman, M. M. et al. An integrated approach using yolov8 and resnet, seresnet & vision transformer (VIT) algorithms based on ROI fracture prediction in x-ray images of the elbow. Curr. Med. Imaging Rev. 20 10.2174/0115734056309890240912054616 39360542 22. Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image segmentation. arXiv abs/1505.04597 (2015). 23. Alom, M. Z., Yakopcic, C., Taha, T. M. & Asari, V. K. Nuclei segmentation with recurrent residual convolutional neural networks based u-net (r2u-net). In NAECON 2018 - IEEE National Aerospace and Electronics Conference 24. Lau SLH Chong EKP Yang X Wang X Automated pavement crack segmentation using u-net-based convolutional neural network IEEE Access 2020 8 114892 114899 10.1109/ACCESS.2020.3003638 Lau, S. L. H., Chong, E. K. P., Yang, X. & Wang, X. Automated pavement crack segmentation using u-net-based convolutional neural network. IEEE Access 8 25. Jalali, Y., Fateh, M., Rezvani, M., Abolghasemi, V. & Anisi, M. H. Resbcdu-net: A deep learning framework for lung CT image segmentation. Sensors (Basel, Switzerland) 21 10.3390/s21010268 PMC7796094 33401581 26. Rezvani, S., Fateh, M. & Khosravi, H. Abanet: Attention boundary-aware network for image segmentation. Expert Syst. J. Knowl. Eng. 41 27. Wu R Xin Y Qian J Dong Y A multi-scale interactive u-net for pulmonary vessel segmentation method based on transfer learning Biomed. Signal Process. Control. 2023 80 104407 10.1016/j.bspc.2022.104407 Wu, R., Xin, Y., Qian, J. & Dong, Y. A multi-scale interactive u-net for pulmonary vessel segmentation method based on transfer learning. Biomed. Signal Process. Control. 80 28. Nitha, V.R. et al. Novel cefnet framework for lung disease detection and infection region identification. Biomed. Signal Process. Control. 96 29. Jalali, Y., Fateh, M. & Rezvani, M. Dabt-u-net: Dual attentive bconvlstm u-net with transformers and collaborative patch-based approach for accurate retinal vessel segmentation. Int. J. Eng. 30. Chen J Transunet: Rethinking the u-net architecture design for medical image segmentation through the lens of transformers Med. Image Anal. 2024 97 103280 10.1016/j.media.2024.103280 39096845 Chen, J. et al. Transunet: Rethinking the u-net architecture design for medical image segmentation through the lens of transformers. Med. Image Anal. 97 39096845 10.1016/j.media.2024.103280 31. Li, C. et al. U-kan makes strong backbone for medical image segmentation and generation. arXiv abs/2406.02918 (2024). ",
  "metadata": {
    "Title of this paper": "Transunet: Rethinking the u-net architecture design for medical image segmentation through the lens of transformers",
    "Journal it was published in:": "Scientific Reports",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12489112/"
  }
}
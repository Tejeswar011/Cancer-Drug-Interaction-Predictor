{
  "title": "Paper_674",
  "abstract": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12480946 PMC12480946.1 12480946 12480946 41023208 10.1038/s41598-025-18759-4 18759 1 Article Large language models could be applied in personalized out-of-hospital management for breast cancer: a prospective randomized single blind study Wang Qinchuan wangqinchuan@zju.edu.cn 1 Chen Zikang 2 Zhang Hao 3 Zhou Yulu 1 Du Chengyong 4 Hu Wenxian 1 Lv Xudong 2 Xie Tan 1 Zheng Heming 1 1 https://ror.org/00ka6rp58 grid.415999.9 0000 0004 1798 9361 Department of Surgical Oncology, Affiliated Sir Run Run Shaw Hospital, Zhejiang University School of Medicine, 2 https://ror.org/00a2xv884 grid.13402.34 0000 0004 1759 700X College of Biomedical Engineering and Instrument Science, Zhejiang University, 3 https://ror.org/05d659s21 grid.459742.9 0000 0004 1798 5889 Department of Breast Surgery, Cancer Hospital of China Medical University, Cancer Hospital of Dalian University of Technology, Liaoning Cancer Hospital & Institute, 4 https://ror.org/05m1p5x56 grid.452661.2 0000 0004 1803 6319 Department of Breast Surgery, The First Affiliated Hospital, Zhejiang University School of Medicine, 29 9 2025 2025 15 478255 33589 15 6 2025 3 9 2025 29 09 2025 01 10 2025 01 10 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access http://creativecommons.org/licenses/by-nc-nd/4.0/ Personalized out-of-hospital management could significantly improve quality of life of breast cancer patients. We aimed to evaluate the accuracy, effectiveness, safety, personalization and emotional care of Large Language Models (LLMs) in the out-of-hospital management of breast cancer. We established a data cleaning and classification pipeline to summarize three major scenarios of out-of-hospital management. Authentic electronic health record (EHR) datasets for data collection were generated using 10 patients with ID information masked from Breast Cancer Database in Affiliated Sir Run Run Shaw Hospital, Zhejiang University. Then we matched the EHR datasets with three out-of-hospital management scenarios as 100 virtual patients (VPs) for LLMs to perform the conversation generation using GPT-o3 and DeepSeek-R1. Further, we incorporated four human specialists to rate the responses of LLMs in five dimensions using Likert scale. As of April 1, 2025, the 4 evaluator specialists rated the conversations of LLMs and 100 VPs. The results demonstrate that both DS-R1and GPT-o3 performed well, with scores primarily concentrated at 3 and 4 points. We revealed statistically significant differences between DS-R1and GPT-o3 in accuracy, personalization, and emotional care ( P P Supplementary Information The online version contains supplementary material available at 10.1038/s41598-025-18759-4. Keywords Large language models Out-of-hospital management Breast cancer DeepSeek-R1 GPT-o3 Subject terms Health care Medical research Cancer pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © Springer Nature Limited 2025 Introduction Breast cancer has been a critical threat to public health due to its high incidence and heavy social economic burden 1 2 3 via 4 The emergence of Large Language Models (LLMs) has revolutionized the care for breast cancer. Chatbots based on LLMs were developed to collect the data of PROs 5 6 7 6 8 To explore the role of reasoning-enhanced LLMs in out-of-hospital management for breast cancer, we conducted a multi-phase randomized single-blind study. First, we established a data cleaning and classification pipeline to summarize three major scenarios of out-of-hospital management, and we also constructed authentic EHR datasets for data collection. Second, we matched the EHR datasets with three out-of-hospital management scenarios to create virtual patients (VP) ( N Method Ethics This study was approved and supervised by the Institutional Review Board of SRRSH (IRB#: 20210910-30). All the patients enrolled in this study were fully informed and consented of their rights, with their identification information masked. The study was performed in accordance with the Declaration of Helsinki. Overview As shown in Fig. 1 First, in the data collection phase, we established a data cleaning and classification pipeline to summarize a question dataset from publicly available breast cancer-related dialogue datasets. This dataset covers three typical out-of-hospital management application scenarios: disease consultation, rehabilitation guidance, and psychological management. Additionally, we selected authentic patient data with ID information masked from the Breast Cancer Database of Affiliated Sir Run Run Shaw Hospital, Zhejiang University to construct an EHR dataset. Based on these two datasets, in the conversation generation phase, we matched each patient’s EHR data with several sets of questions corresponding to the three out-of-hospital management scenarios. Using the EHR data and its corresponding question set, we designed a prompt engineering framework that drives LLMs to act as VPs, simulating out-of-hospital management scenarios and engaging in multi-round dialogues with GPT-o3 and DS-R1(released on 2025.01.20). This process generates a question-and-answer dataset that mimics real-world interactions. In the human evaluation phase, the question-and-answer datasets generated by GPT-o3 and DS-R1were randomly and evenly divided into two parts, each assigned to two groups of evaluators. A Likert scale 9 We have open-sourced the code, anonymized dataset, and evaluation results used in this experiment on GitHub ( https://github.com/Maxin-C/LLM-Evaluation  Fig. 1 Overview of methodology. Data collection In this section, we aim to identify frequently mentioned issues by patients during the out-of-hospital management of breast cancer and obtain real patient EHR data to facilitate subsequent steps where the model is required to act as a virtual patient interacting with the LLM under evaluation. For the construction of the common issues dataset, we selected the Huatuo-BC dialogue dataset extracted from Huatuo-26 M as the raw data. Huatuo-26 M was derived from Qianwen Health, offering more than 26 million real-world doctor-patient dialogues. Huatuo-BC is a breast cancer-related subset of this dataset, comprising 208 K question-answer pairs. First, by constructing a dataset filtering prompt (Appendix A1), we used the API (Application Programming Interface) to instruct DeepSeek-V3-0324 (DeepSeek-V3) to extract dialogue content related to out-of-hospital breast cancer management from the original dataset, resulting in 29 K question-answer pairs. Then, by clearly defining three scenarios—disease consultation, rehabilitation guidance, and psychological management—we designed classification and summarization prompts (Appendix A2) to drive DeepSeek-V3 to categorize the question-answer data. Ultimately, we obtained 256, 309, and 350 common question datasets for the three scenarios, respectively. Ten patient data with complete indicators and different conditions (with Stage as the standard) were extracted as the background information of EHR information to support personalized question and answer. The main data distribution of the EHR dataset is shown in Table 1  Table 1 Main data distribution table of the EHR dataset. No. Gender Age Stage Molecular subtype Chemical therapy Radiation therapy Endocrine therapy Target therapy 1 Female 64 Stage IA Luminal-B HER-2 positive Yes No Aromatase inhibitor No 2 Female 55 Stage IA Luminal-B HER-2 negative Yes Yes Tamoxifen No 3 Female 65 Stage IB Luminal-B HER-2 negative Yes Yes Aromatase inhibitor No 4 Female 44 Stage IIA Triple negative Yes Yes - No 5 Female 46 Stage IIA Luminal-A Yes Yes Toremifene No 6 Female 47 Stage IIB Luminal-B HER-2 negative Yes No Toremifene No 7 Female 76 Stage IIIA Triple negative Yes No No - 8 Female 58 Stage IIIB Luminal-A Yes Yes Tamoxifen No 9 Female 56 Stage IIIC Triple negative Yes Yes - No 10 Female 66 Stage IV Luminal-B HER-2 positive Yes Yes Aromatase inhibitor No Conversation generation The process of conversation generation is illustrated in Fig. 2 For each set of virtual patient information, the virtual patient prompt (Appendix A4) can drive DeepSeek-V3 to act as a patient undergoing post-operative out-of-hospital management for breast cancer and engage in conversation with the model under evaluation. To make the virtual patient’s questions more closely resemble the conversation process of real patients, this study extracted a total of 1,775 dialogue histories from a WeChat group chat focused on post-operative management for breast cancer patients, spanning from June 2023 to January 2025. After removing private information, these dialogues were used as few-shot inputs in the virtual patient prompt, instructing the model to mimic the conversation style in its outputs. Upon receiving the needs raised by the virtual patient, the model under evaluation will act as a doctor providing out-of-hospital management services through the virtual doctor prompt (Appendix A5). The virtual patient and virtual doctor will engage in multiple rounds of conversation. To avoid meaningless conversations, we additionally used a conversation monitoring prompt (Appendix A6) to instruct DeepSeek-V3 to determine whether the current content has addressed the needs raised by the virtual patient. If the judgment is affirmative or the conversation exceeds 8 rounds, the conversation will be terminated, and the next conversation will begin. In this experiment, the reasoning parameters of the models were kept consistent, with the temperature set to 0.1 and top-p set to 1, to ensure the stability of text generation and enhance the reproducibility of the experiment.  Fig. 2 Conversation generation process. Human evaluation Based on 100 sets of virtual patient information, evaluating GPT-o3 and DS-R1 yielded 200 sets of conversations. Four breast doctors were invited to assess the dataset. The dataset was evenly and randomly divided into two parts, each containing 50 sets of conversations from GPT-o3 and 50 sets from DS-R1. Each part was evaluated by 2 doctors who were aware of the EHR data but unaware of the model sources. Since GPT-o3’s reasoning process is in English, which naturally distinguishes it from DS-R1’s content, a translation prompt (Appendix A7) was used to instruct DeepSeek-V3 to convert the English content into Chinese, and model responses were uniformly formatted as “Reasoning: … Answer: …”. Translated content (shown in Appendix B) was manually reviewed by non-evaluators with advanced English-Chinese bilingual competence (Master’s degree or higher) to ensure fidelity and mitigate potential misinterpretations. The data content was rendered into images using unified HTML rendering code for display. The models were evaluated using a five-point Likert scale 9 2  Table 2 LLM evaluation likert scale. Dimension Content Options Effectiveness Do you agree that the model’s response can be easily understood and applied by readers without a medical background? 1. Strongly disagree 2. Disagree 3. Neither agree nor disagree 4. Agree 5. Strongly agree Accuracy Do you agree that the model’s reasoning process aligns with clinical reasoning logic? Personalization Do you agree that the model’s response considers the patient’s specific pathological characteristics? Safety Do you agree that the model’s response contains misleading risk recommendations? Emotional care Do you agree that the model’s response considers the patient’s emotional needs? Result Human evaluation result We compiled the ratings of conversations from four evaluators and calculated the average scores of DS-R1 and GPT-o3 across five dimensions, as shown in Fig. 3 P P P 3 3  Fig. 3 Human evaluation results: ( a b Time and economic costs evaluation result As shown in Fig. 4 The verbose content, on one hand, makes the model’s responses harder to quickly scan and understand, preventing its effectiveness from achieving a statistically significant difference compared to GPT-o3 in human evaluation. On the other hand, despite having a lower per-token cost, the total economic expense of DS-R1 reaches about 1.6 times that of GPT-o3, which is counterintuitive. Thanks to DS-R1’s faster inference speed, the time costs of the two LLMs are nearly identical even when generating more tokens. Moreover, DS-R1has a shorter total response time, enabling it to meet demands more quickly.  Fig. 4 LLMs’ time and economic costs. Discussion In this study, we systematically evaluated the performance of mainstream LLMs in the scenarios of out-of-hospital management for breast cancer patients. We simulated 100 VPs from real breast cancer cases, and engaged multiple rounds of dialogues under out-of-hospital management scenarios with GPT-o3 and DS-R1. The performance of LLMs was evaluated in five dimensions, including effectiveness, safety, accuracy, personalization, and emotional care. The results showed that both LLMs had satisfactory performance in out-of-hospital management. Compared to GPT-o3, DS-R1 behaved better in all dimensions according to human specialists except in Rater 1’s emotional care, Rater 2’s safety, Rater 3’s safety, and Rater 4’s effectiveness. Also, DS-R1generated more tokens in identical time with less economic cost, and it also had shorter response time than GPT-o3. Therefore, this study suggested that LLMs could be deployed in the scenarios of out-of-hospital management for breast cancer patients, DS-R1seems to have better performance compared to GPT-o3. The LLMs’ role in out-of-hospital management of cancer patients remains in debate. Our study suggested that the majority of human physicians rated LLMs’ responses at the score of 3, which means satisfactory performance in out-of-hospital management. However, still there are existing problems, like hallucinatory responses. During the evaluation, we occasionally encountered hallucinatory responses (accounting for 2.0%, 4/200), which could severely mislead patients and cause hazardous events. For instance, LLMs sometimes suggested a HER2 negative patient to receive target therapy, or suggested a stage 0 VP to receive chemotherapy in our study. Though the case is rare, it could result in irretrievable consequences. This is in accordance with another study employing GPT-3.5 and GPT-4. They conducted an intrinsic evaluation study rating 60 GPT-powered VP-clinician conversations to evaluate the clinical performance of LLMs and to rate the quality of dialogues and feedback. The result showed that the quality of LLMs-generated ratings of feedback is similar to human physicians, but it still has detractors like lower authenticity, verbose vocabulary and failure to mention important weaknesses or strengths 10 11 12 13 Both GPT-o3 and DS-R1demonstrated substantial potential in assisting out-of-hospital management, but DS-R1 had better overall performance and less cost than GPT-o3 in our study. As newly emerged AI, DS-R1has little research in breast cancer, whereas GPT has the most applications among existing LLMs in multiple scenarios of the practice. One retrospective, cross-sectional study reported that over one-third recommendations for breast, prostate, and lung cancer by GPT-3.5 were not consistent with the standard care set provided by the National Comprehensive Cancer Network (NCCN) 14 15 6 16 17 Although LLMs have demonstrated promising applications in the out-of-hospital management for breast cancer, limitations are still exist. According to the human specialists, the responses of involved LLMs have moderate risk of misleading for the patients (Likert scale 2.92/2.77). The reason for the misleading risk could be derived from wrong suggestions based on VPs, which is consistent with previous literatures indicating the limited applicability of LLMs 18 19 18 19 Our study indicated that LLMs could provide personalized, empathetic, and accurate suggestions in the out-of-hospital management for breast cancer patients. LLMs could identify the emotional requirement of the patients and provide support for the psychological problems. This is consistent with a previous study that chatbot based on GPT could generate empathetic, quality and readable responses to patient questions compared to human physicians in social media 20 21 Our study has significant advantages of randomized, and multi-phase study design, LLM-human physician evaluation and validation for the results. However, we also confess several limitations. First, we only evaluated two most up-to-dated reasoning enhanced LLMs, other LLMs like Grok3, were not included in the study. Second, only 10 VPs were simulated for the test, though over 100 question datasets were created, still the sample size is limited. Third, we included 4 human physicians participating in the evaluation of the responses from LLMs, inter-person heterogeneity could also affect the results. However, we employed Cohen’s Kappa test to reduce the potential bias. In Dataset A, the Cohen’s Kappa test results were 0.52 for DS-R1 scores ( P P P P Conclusion Our findings demonstrate that LLMs like GPT-o3 and DS-R1 show significant promise for the out-of-hospital management of breast cancer patients. Both models delivered personalized and empathetic responses, with DS-R1 showing superior overall performance, particularly in personalization, emotional care, and accuracy. However, the critical barrier to their autonomous application is the risk of generating factually incorrect and dangerous medical advice. These “hallucinations,” while infrequent, pose an unacceptable threat to patient safety, limiting the current applicability of LLMs. Therefore, before LLMs can be safely integrated into the era of digital healthcare, future research must prioritize improving the safety and reliability of their answers. The focus must be on eliminating these critical errors, potentially through advanced methods like Retrieval-Augmented Generation (RAG), to increase their real-world clinical applicability. Supplementary Information Below is the link to the electronic supplementary material.  Supplementary Material 1 Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Qinchuan Wang and Zikang Chen contributed equally to the work. Author contributions The study was designed by QW and ZC. ZC and QW contributed to data acquisition. Biostatistical analysis was done by ZC, XL, and QW. Patient recruitment, sample collection as well as data collection was done by QW, HZ, YZ, CD, WH, and HM.Z. All authors had full access to all the data in the study and all authors interpreted the data. The first draft of the report was written by QW. Verification of the underlying data was done by ZC, QW, and HZ. The decision to submit the report for publication was made by all the authors. All authors contributed to the review of the manuscript. Data availability We have open-sourced the code, anonymized dataset, and evaluation results used in this experiment on GitHub (https://github.com/Maxin-C/LLM-Evaluation). Declarations Competing interests The authors declare no competing interests. References 1. Kim, J. et al. Global patterns and trends in breast cancer incidence and mortality across 185 countries. Nat. Med. 10.1038/s41591-025-03502-3 39994475 2. Houzard S Out-of-hospital follow-up after low risk breast cancer within a care network: 14-year results Breast 2014 23 4 407 412 10.1016/j.breast.2014.02.006 24656635 Houzard, S. et al. Out-of-hospital follow-up after low risk breast cancer within a care network: 14-year results. Breast 23 24656635 10.1016/j.breast.2014.02.006 3. Dai S Current status of Out-of-Hospital management of cancer patients and awareness of internet medical treatment: A questionnaire survey Front. Public. Health 2021 9 756271 10.3389/fpubh.2021.756271 34970526 PMC8712547 Dai, S. et al. Current status of Out-of-Hospital management of cancer patients and awareness of internet medical treatment: A questionnaire survey. Front. Public. Health 9 34970526 10.3389/fpubh.2021.756271 PMC8712547 4. Cheng KKF Home-based multidimensional survivorship programmes for breast cancer survivors Cochrane Database Syst. Rev. 2017 8 8 CD011152 28836379 10.1002/14651858.CD011152.pub2 PMC6483678 Cheng, K. K. F. et al. Home-based multidimensional survivorship programmes for breast cancer survivors. Cochrane Database Syst. Rev. 8 28836379 10.1002/14651858.CD011152.pub2 PMC6483678 5. Chen Z Chat-ePRO: development and pilot study of an electronic patient-reported outcomes system based on ChatGPT J. Biomed. Inf. 2024 154 104651 10.1016/j.jbi.2024.104651 38703936 Chen, Z. et al. Chat-ePRO: development and pilot study of an electronic patient-reported outcomes system based on ChatGPT. J. Biomed. Inf. 154 10.1016/j.jbi.2024.104651 38703936 6. Pan A Assessment of artificial intelligence chatbot responses to top searched queries about cancer JAMA Oncol. 2023 9 10 1437 1440 10.1001/jamaoncol.2023.2947 37615960 PMC10450581 Pan, A. et al. Assessment of artificial intelligence chatbot responses to top searched queries about cancer. JAMA Oncol. 9 37615960 10.1001/jamaoncol.2023.2947 PMC10450581 7. Sorin V Large Language model (ChatGPT) as a support tool for breast tumor board NPJ Breast Cancer 2023 9 1 44 10.1038/s41523-023-00557-8 37253791 PMC10229606 Sorin, V. et al. Large Language model (ChatGPT) as a support tool for breast tumor board. NPJ Breast Cancer 9 37253791 10.1038/s41523-023-00557-8 PMC10229606 8. Liu X A generalist medical Language model for disease diagnosis assistance Nat. Med. 2025 31 3 932 942 10.1038/s41591-024-03416-6 39779927 Liu, X. et al. A generalist medical Language model for disease diagnosis assistance. Nat. Med. 31 39779927 10.1038/s41591-024-03416-6 9. Allen, E. S. & Christopher Likert Scales and Data Analyses 10. Cook DA Virtual patients using large Language models: scalable, contextualized simulation of Clinician-Patient dialogue with feedback J. Med. Internet Res. 2025 27 e68486 10.2196/68486 39854611 PMC12008702 Cook, D. A. et al. Virtual patients using large Language models: scalable, contextualized simulation of Clinician-Patient dialogue with feedback. J. Med. Internet Res. 27 39854611 10.2196/68486 PMC12008702 11. Zeng J Assessing the role of the generative pretrained transformer (GPT) in alzheimer’s disease management: comparative study of Neurologist- and artificial Intelligence-Generated responses J. Med. Internet Res. 2024 26 e51095 10.2196/51095 39481104 PMC11565080 Zeng, J. et al. Assessing the role of the generative pretrained transformer (GPT) in alzheimer’s disease management: comparative study of Neurologist- and artificial Intelligence-Generated responses. J. Med. Internet Res. 26 39481104 10.2196/51095 PMC11565080 12. Young CC Racial, ethnic, and sex bias in large Language model opioid recommendations for pain management Pain 2025 166 3 511 517 10.1097/j.pain.0000000000003388 39283333 PMC12042288 Young, C. C. et al. Racial, ethnic, and sex bias in large Language model opioid recommendations for pain management. Pain 166 39283333 10.1097/j.pain.0000000000003388 PMC12042288 13. Ge J Development of a liver disease-specific large Language model chat interface using retrieval-augmented generation Hepatology 2024 80 5 1158 1168 10.1097/HEP.0000000000000834 38451962 PMC11706764 Ge, J. et al. Development of a liver disease-specific large Language model chat interface using retrieval-augmented generation. Hepatology 80 38451962 10.1097/HEP.0000000000000834 PMC11706764 14. Chen S Use of artificial intelligence chatbots for cancer treatment information JAMA Oncol. 2023 9 10 1459 1462 10.1001/jamaoncol.2023.2954 37615976 PMC10450584 Chen, S. et al. Use of artificial intelligence chatbots for cancer treatment information. JAMA Oncol. 9 37615976 10.1001/jamaoncol.2023.2954 PMC10450584 15. Tsai CY ChatGPT v4 outperforming v3.5 on cancer treatment recommendations in quality, clinical guideline, and expert opinion concordance Digit. Health 2024 10 20552076241269538 10.1177/20552076241269538 39148811 PMC11325467 Tsai, C. Y. et al. ChatGPT v4 outperforming v3.5 on cancer treatment recommendations in quality, clinical guideline, and expert opinion concordance. Digit. Health 10 39148811 10.1177/20552076241269538 PMC11325467 16. Tawfik E Ghallab E Moustafa A A nurse versus a chatbot – the effect of an empowerment program on chemotherapy-related side effects and the self-care behaviors of women living with breast cancer: a randomized controlled trial BMC Nurs. 2023 22 1 102 10.1186/s12912-023-01243-7 37024875 PMC10077642 Tawfik, E., Ghallab, E. & Moustafa, A. A nurse versus a chatbot – the effect of an empowerment program on chemotherapy-related side effects and the self-care behaviors of women living with breast cancer: a randomized controlled trial. BMC Nurs. 22 37024875 10.1186/s12912-023-01243-7 PMC10077642 17. Laymouna M Roles, users, benefits, and limitations of chatbots in health care: rapid review J. Med. Internet Res. 2024 26 e56930 10.2196/56930 39042446 PMC11303905 Laymouna, M. et al. Roles, users, benefits, and limitations of chatbots in health care: rapid review. J. Med. Internet Res. 26 39042446 10.2196/56930 PMC11303905 18. Deng L Evaluation of large Language models in breast cancer clinical scenarios: a comparative analysis based on ChatGPT-3.5, ChatGPT-4.0, and Claude2 Int. J. Surg. 2024 110 4 1941 1950 10.1097/JS9.0000000000001066 38668655 PMC11019981 Deng, L. et al. Evaluation of large Language models in breast cancer clinical scenarios: a comparative analysis based on ChatGPT-3.5, ChatGPT-4.0, and Claude2. Int. J. Surg. 110 38668655 10.1097/JS9.0000000000001066 PMC11019981 19. Griewing, S. et al. Challenging ChatGPT 3.5 in senology-an assessment of concordance with breast cancer tumor board decision making. J. Pers. Med. 13 10.3390/jpm13101502 PMC10608120 37888113 20. Chen D Physician and artificial intelligence chatbot responses to cancer questions from social media JAMA Oncol. 2024 10 7 956 960 10.1001/jamaoncol.2024.0836 38753317 PMC11099835 Chen, D. et al. Physician and artificial intelligence chatbot responses to cancer questions from social media. JAMA Oncol. 10 38753317 10.1001/jamaoncol.2024.0836 PMC11099835 21. Greer S Use of the chatbot vivibot to deliver positive psychology skills and promote Well-Being among young people after cancer treatment: randomized controlled feasibility trial JMIR Mhealth Uhealth 2019 7 10 e15018 10.2196/15018 31674920 PMC6913733 Greer, S. et al. Use of the chatbot vivibot to deliver positive psychology skills and promote Well-Being among young people after cancer treatment: randomized controlled feasibility trial. JMIR Mhealth Uhealth 7 31674920 10.2196/15018 PMC6913733 ",
  "metadata": {
    "Title of this paper": "Use of the chatbot vivibot to deliver positive psychology skills and promote Well-Being among young people after cancer treatment: randomized controlled feasibility trial",
    "Journal it was published in:": "Scientific Reports",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12480946/"
  }
}
{
  "title": "Paper_456",
  "abstract": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12485127 PMC12485127.1 12485127 12485127 41028094 10.1038/s41598-025-14169-8 14169 1 Article An optimized hybrid deep learning model to detect Alzheimer disease Raj A. Sundar drasr18@gmail.com 1 Gunasundari C. 2 Senthilkumar S. 3 Sivamani S. 4 1 https://ror.org/03s9gtm48 0000 0004 5939 3224 Department of Biomedical Engineering, E.G.S. Pillay Engineering College, 2 https://ror.org/050113w36 grid.412742.6 0000 0004 0635 5080 School of Computing, SRM Institute of Science and Technology, 3 https://ror.org/03s9gtm48 0000 0004 5939 3224 Department of Electronics and Communication Engineering, E.G.S. Pillay Engineering College, 4 https://ror.org/03s9gtm48 0000 0004 5939 3224 Department of Electrical and Electronics Engineering, E.G.S. Pillay Engineering College, 30 9 2025 2025 15 478255 34081 8 5 2025 29 7 2025 30 09 2025 02 10 2025 02 10 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access http://creativecommons.org/licenses/by-nc-nd/4.0/ Alzheimer’s is a serious neurodegenerative disease that requires early detection for effective intervention. Traditional methods often struggle with accurately identifying the early stages, such as mild cognitive impairment (MCI), due to limitations in feature extraction and classification. To address these challenges, we present an optimized hybrid deep learning model for Alzheimer’s disease detection. Our model employs the Inception v3 algorithm for initial feature extraction and the ResNet 50 algorithm for classification. Additionally, we optimize the network parameters using the Adaptive Rider Optimization (ARO) algorithm to enhance detection performance. Experimental analysis using a benchmark dementia dataset demonstrates that our model achieves superior accuracy of 96.6%, precision of 98%, recall of 97%, and F1-score of 98%, outperforming state-of-the-art techniques. Keywords Alzheimer disease detection Mild cognitive impairment Deep learning algorithm Inception v3 ResNet 50 Optimal features Adaptive rider optimization (ARO) algorithm Subject terms Engineering Biomedical engineering Electrical and electronic engineering pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © Springer Nature Limited 2025 Introduction Millions of people are affected worldwide due to an irreversible neurodegenerative disorder. Alzheimer, the serious disease, which collapses the cognitive functions and makes the patient’s daily life a challenging one. Alzheimer’s disease will be the primary form of dementia, encompassing approximately 60–80% of all dementia cases. Alzheimer patients will have memory loss, faces difficulty in finding suitable word to talk, repetitive questioning, difficult to take decisions, difficulty in food intake, lack of thinking, etc., The statistics of Alzheimer Association 1 The progressive stages of Alzheimer’s are defined in three phases: normal control, mild cognitive impairment, and finally Alzheimer’s. In all these stages, slight changes occur in the human brain structure. Mild Cognitive Impairment (MCI) is considered the initial stage of Alzheimer’s and is crucial for clinical analysis. Early symptoms of Alzheimer’s can be studied from MCI, allowing for the delay of Alzheimer’s deterioration with proper medications. The symptoms of MCI include mild behavioral changes visible to others 2 Neuroimaging modalities such as positron emission tomography (PET), computerized tomography (CT), and magnetic resonance imaging (MRI) are widely used for Alzheimer’s diagnosis 3 4 5 The initial stages of Alzheimer’s detection incorporate machine learning algorithms for disease classification 6 7 8 9 12  Proposed a novel hybrid deep learning model combining Inception v3 for multi-scale feature extraction and ResNet-50 for robust classification, specifically tailored for Alzheimer’s disease detection from MRI images. Introduced Adaptive Rider Optimization (ARO) for hyperparameter tuning, which enhances the training performance by dynamically adjusting key parameters such as learning rate, batch size, number of epochs, and dropout rate. Demonstrated the superiority of ARO over conventional optimizers (e.g., Adam, RMSprop, Bayesian optimization) by effectively escaping local minima and improving convergence behavior through its rider-based behavioral modeling strategy. Developed a two-stage training strategy: initial feature extraction using frozen Inception v3 weights followed by fine-tuning and classification through ResNet-50, thereby leveraging transfer learning while reducing overfitting. Handled dataset imbalance using targeted data augmentation techniques (flipping, rotation, brightness adjustment) applied only to underrepresented classes, ensuring model generalization without inducing data leakage. Achieved statistically validated performance improvements using confidence intervals and paired t-tests across all key metrics (accuracy, precision, recall, F1-score, specificity), confirming the robustness and reliability of the proposed model. Evaluated the model using a publicly available four-class Alzheimer’s MRI dataset from Kaggle, with stratified data splitting and a detailed experimental setup that ensures reproducibility and fair comparison. Presented a thorough comparative analysis against multiple state-of-the-art CNN and hybrid models, demonstrating consistent outperformance across all evaluation metrics. The rest of the discussions are arranged in the following structure: The literature review provided in Sect. 2 offers insight into recent methodologies in Alzheimer’s disease detection. Section 3 presents the hybrid deep learning algorithm and the optimization model proposed in this research work. The experimental results based on benchmark datasets and the comparative analysis are presented in Sect. 4. The findings and the summary of the research work are presented in Sect. 5. Related works The research on Alzheimer’s detection has been ongoing for over a decade. This section presents some recent research works considered for this study. In certain studies, Alzheimer’s is identified based on visible symptoms. For example, in research work 13 14 The Alzheimer’s detection model presented in 15 16 The Alzheimer’s detection model reported in 17 18 The automated diagnosis procedure presented in 19 20 21 The deep learning-based Alzheimer’s detection presented in 22 23 The CNN-based Alzheimer’s detection model presented in 24 25 An EfficientNet architecture-based Alzheimer detection procedure presented in 26 27 The hybrid model presented for Alzheimer’s detection utilizes a local feature descriptor to detect early-stage symptoms of MCI. The Hessian detector and binary pattern texture operator are combined to form the local descriptor, and finally, the classification is performed using a simple CNN. The experimental results present better classification accuracy which is comparatively lower than the recent state-of-the-art techniques. The hybrid model presented in 28 The similar ensemble model presented in 29 The optimization of Osprey Gannets using retinal fundus images is based on transfer learning (TL) is proposed in 30 A new approach to the detection of CVD using the analysis of retinal fundus images is presented in 31 In order to effectively identify and classify lung cancer, a deep learning strategy is introduced in 32 A system for optimising the scheduling of electric vehicle (EV) charge operations on a VANET topology is presented in 33 A Hybrid Whale and Gray Wolf Deep Learning Optimization Algorithm is developed in 34 Sophisticated four-phase method for early detection of Alzheimer’s Disease is introduced in 35 A hybrid deep learning framework that employs Generative Adversarial Networks (GANs) and Deep Convolutional Neural Networks (CNNs) is presented in 36 An ensemble approach for Alzheimer’s disease detection is proposed in 37 A novel Alzheimer’s disease detection methodology using MRI data and deep learning techniques is introduced in 38 Previous hybrid methods for Alzheimer’s detection often suffer from key limitations such as manual feature fusion, rigid architecture design, and suboptimal hyperparameter tuning. Many approaches combine CNN backbones without effectively leveraging their complementary strengths or rely on fixed heuristic parameters, leading to constrained learning capacity and reduced adaptability to data variability 39 41 Proposed work The proposed Alzheimer detection model includes deep learning algorithms like Inception v3 and ResNet 50. An adaptive rider optimization (ARO) is included for optimizing the network parameters and to improve the detection performances. The complete overview of the proposed Alzheimer detection model is presented in Fig. 1  Fig. 1 Alzheimer detection using optimized hybrid deep learning algorithm. Data preprocessing The MRI brain image is initially preprocessed to remove the noise. For noise removal temporal high pass filtering is used and then normalization is performed in which spatial normalization is performed using a linear transformation. Since deep learning algorithms requires a substantial number of samples, in addition to data normalization, data augmentation is performed in the preprocessing steps to attain substantial number of input samples. Data augmentation includes techniques like horizontal flipping, vertical flipping, image rotation at 90 degree and 270-degree, image brightness enhancement procedure. For enhanced contrast, Histogram Equalization is also included in the preprocessing procedure. From the preprocessed data, the features are extracted and classified using hybrid deep learning algorithm. Hybrid deep learning model for Alzheimer detection The hybrid deep learning model used in the proposed work for Alzheimer detection includes Inception v3 and ResNet 50 for feature extraction and classification. Inception v3 is primarily a network developed by Keras. The Network process the input through three channels of size 299 × 299. Compared to previous versions of inception network, inception v3 splits large volume integrals into small convolutions using convolutional kernel splitting method which reduces the number of parameters and accelerates the spatial feature extraction process. The architecture of inception v3 is presented in Fig. 2 2  Fig. 2 Inception v3 network. Inception v3, three different sized grids inception structure is used to optimize the network. Figure 3 3  Fig. 3 Inception module in Inception v3. Inception v3 is an advanced convolutional neural network (CNN) architecture known for its efficient and effective feature extraction capabilities. It uses a combination of convolutions, pooling operations, and inception modules to capture multi-scale spatial features from input images. Inception modules allow the network to process information at various scales, combining the outputs of multiple convolutions with different filter sizes. This multi-scale approach helps in capturing fine-grained details as well as broader contextual information, making it highly suitable for analyzing complex MRI images of the brain. In our model, Inception v3 is utilized to extract a comprehensive set of features from MRI images. These features include edges, textures, and patterns that are indicative of different stages of Alzheimer’s disease. The output of Inception v3 is a high-dimensional feature map that serves as the input for the subsequent classification stage. In the proposed inception v3 is used to extract the initial features, the architecture includes 11 inception models. Each inception modules includes a convolution layer, pooling layer, activation layer and batch normalization layer. The multi-scale maximum features are obtained from the input image using these inception modules. The feature extracted from the input image are concatenated into a feature map and then fed into the ResNet50 module for final classification. The classification module used in the proposed architecture is ResNet 50 which comprises residual modules. These residuals are used to avoid network degradation problem and reduce error in the classification process. Moreover, the residual modules used in stage 2 and 5 overcomes the vanishing gradient issue. The architecture of ResNet50 is divided into 5 blocks. The residual blocks preserve the previous layer information which allows the network to learn better representations of input data. The convolution layer in the ResNet performs convolution operation. Followed by convolution, max pooling is used to down sample the convolution layer output. Next to max pooling, series of residual blocks are used which includes two convolution layers, followed by batch normalization layer, and rectified linear unit. The second convolution layer output is added to the residual block input and then passed through ReLU activation function. This process is repeated for the remaining residual blocks and then finally a fully connected layer is used that maps the last residue block output into different output classes. The complete architecture of the hybrid deep learning model is presented in Fig. 4 4  Fig. 4 Proposed hybrid deep learning model. In the proposed hybrid deep learning architecture, Inception v3 was employed as a feature extractor, and ResNet-50 was used as the classifier. The two networks were integrated sequentially, where features extracted from the output of the final global average pooling layer of Inception v3 were passed directly to the input layer of ResNet-50 for classification. During the initial phase of training, Inception v3 was used with frozen weights (i.e., non-trainable) to retain the benefit of pretrained convolutional features learned on ImageNet, which helps in reducing overfitting and training time when data is limited. After achieving initial convergence, fine-tuning was selectively enabled for the deeper layers (i.e., layers beyond the 249th layer of Inception v3), allowing the model to adapt to the domain-specific features of Alzheimer’s MRI data. ResNet-50, on the other hand, was trained from scratch, as it served as the classification backbone with custom fully connected layers tailored to the four-class Alzheimer’s dataset. This two-stage strategy of freezing then fine-tuning helped balance computational efficiency with model accuracy, ensuring that lower-level features were preserved while allowing domain-specific adaptation in higher layers. Adaptive rider optimization (ARO) Optimizing the hyperparameters of deep learning models is essential for achieving high performance. Proper optimization ensures that the network parameters are set to values that maximize the model’s ability to generalize from training data to unseen data. Adaptive Rider Optimization (ARO) is a novel optimization algorithm inspired by the behavior of riders in reaching their destination through various strategies such as following, bypassing, overtaking, and attacking. ARO is designed to enhance the search capability and execution efficiency of optimization processes. In our research, ARO is employed to fine-tune critical hyperparameters of the deep learning model, including learning rate, batch size, number of epochs, and dropout rate. The adaptive nature of ARO allows it to dynamically adjust these parameters during training, improving the convergence speed and overall performance of the model. To attain better classification performance in Alzheimer detection, the classifier network parameters are optimized using Adaptive Rider Optimization (ARO) algorithm. The optimization algorithm is formulated based on the riders who are aimed to reach the destination. There are four categories of riders in the optimization problem such as followers, bypass riders, over takers and attack riders. By neglecting the leader route, the bypass rider reaches the destination point. The follower category of riders chases the lead rider, and the over takers make their own decisions in riding. The attackers make assault moves with full velocity to reach the destination. The efficiency and the ability of a rider to reach the target is considered as optimal solution for the given problem. In the proposed work, the optimal parameters are selected based on the optimal solution. An adaptive strategy is also incorporated to enhance the search ability and reduce the execution time of the optimization algorithm. The mathematical model of ARO considers riders as the optimal solution and to initialize the rider group, the parameters like number of riders, riding period instance, riding dimension and location are considered. Mathematically the initialization process is formulated as 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}^{t}=\\left\\{{x}^{t}\\left(i,j\\right)\\right\\},\\:for\\:1\\le\\:i\\le\\:R;1\\le\\:j\\le\\:Q$$\\end{document} where number of riders are indicated using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:R$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:t$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Q$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}^{t}$$\\end{document} 2 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\overrightarrow{s}=\\overrightarrow{{L}_{b}}+\\overrightarrow{{U}_{b}}+\\overrightarrow{{s}^{t}}$$\\end{document} where the lower and upper bands are indicated using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{L}_{b}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{U}_{b}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{s}^{t}$$\\end{document} 3 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{b}^{t+1}\\left(i,j\\right)=\\delta\\:\\left[{x}_{t}\\left(\\eta\\:,j\\right)\\right]\\ast\\:\\beta\\:\\left(j\\right)+{x}^{t}\\left(\\xi\\:,j\\right)\\ast\\:\\left[1-\\beta\\:\\left(j\\right)\\right]$$\\end{document} where the arbitrary significance \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\delta\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\beta\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\eta\\:$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\xi\\:$$\\end{document} 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{f}^{t+1}\\left(i,k\\right)={x}_{b}\\left(l,k\\right)+\\left[cos\\:\\left({T}_{i,k}^{t}\\right)\\ast\\:{x}_{b}\\left(l,k\\right)\\ast\\:{d}_{i}^{t}\\right]$$\\end{document} where location of bypass rider is given as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{b}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:l$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:k$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{T}_{i,k}^{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{d}_{i}^{t}$$\\end{document} 5 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{o}^{t+1}\\left(i,k\\right)=\\:{x}^{t}\\left(i,k\\right)+\\left[{D}_{I}^{t}\\left(i\\right)\\ast\\:{x}_{b}\\left(l,k\\right)\\right]$$\\end{document} where the rider location and velocity coordinates are indicated using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}^{t}\\left(i,k\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{I}^{t}\\left(i\\right)$$\\end{document} 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{D}_{I}^{t}\\left(i\\right)=\\left[\\frac{2}{1-\\text{log}\\left({s}_{R}^{t}\\left(i\\right)\\right)}\\right]$$\\end{document} where the success rate of rider is given as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{s}_{R}^{t}$$\\end{document} 7 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{a}^{t+1}\\left(i,j\\right)=\\:{x}^{L}\\left(L,j\\right)+\\left[cos\\:\\left({T}_{i,j}^{t}\\right)\\ast\\:{x}_{b}\\left(L,j\\right)\\ast\\:{d}_{i}^{t}\\right]$$\\end{document} where the leader location is indicated as \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}^{L}\\left(L,j\\right)$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{T}_{i,j}^{t}$$\\end{document}  Pseudocode for adaptive rider optimization algorithm  Input: Rider random position  Output: selecting the leading rider (optimal parameters)  Begin  Initialize the population of riders Modify rider parameters like accelerator steering angle brake  Obtain the success rate  Calculate each rider fitness  While t < toff  For i = 1 to R Update the position of bypass rider using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{b}^{t+1}$$\\end{document} Update the position of follower rider using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{f}^{t+1}$$\\end{document} Update the position of over taker rider using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{o}^{t+1}$$\\end{document} Update the position of attacker rider using \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{x}_{a}^{t+1}$$\\end{document}  Rank the riders based on success rate  Select the rider with maximum success rate  Update rider parameters  Return  t = t + 1  End all The computational complexity is assessed by examining the time and space requirements for both the Inception v3 and ResNet 50 components of the model. Inception v3 employs a convolutional architecture with multiple inception modules. Each inception module consists of convolution layers, pooling layers, activation functions, and batch normalization. Assuming \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:n$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:k$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{c}_{in}and{c}_{out}\\:$$\\end{document} 8 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:O({n}^{2}\\ast\\:{k}^{2}\\ast\\:{c}_{in}\\ast\\:{c}_{out})$$\\end{document} Given that Inception v3 processes input images through 11 inception modules, the overall complexity is the sum of the complexities of all convolutional layers within these modules. On the other hand, ResNet 50, used for classification, comprises residual blocks that include convolution layers and identity connections. Each residual block has a computational complexity similar to that of a convolutional layer given as in (8), but the presence of identity connections adds a minimal overhead. ResNet 50 consists of five stages, with each stage containing several residual blocks, contributing to the overall complexity. The Adaptive Rider Optimization (ARO) algorithm used for hyperparameter tuning introduces additional computational overhead. The complexity of ARO is primarily influenced by the number of riders (R), the number of dimensions (Q), and the number of iterations (T). The complexity of ARO can be approximated as 9 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:O(R\\ast\\:Q\\ast\\:T)$$\\end{document} where each iteration involves updating the positions of riders and evaluating their fitness. Combining the complexities of Inception v3, ResNet 50, and ARO, the total computational complexity of our model can be expressed as 10 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:O\\left({n}^{2}\\ast\\:{k}^{2}\\ast\\:{c}_{in}\\ast\\:{c}_{out}\\right)\\ast\\:M+R\\ast\\:Q\\ast\\:T$$\\end{document} where M represents the total number of convolutional layers in Inception v3 and ResNet 50. Integration of ARO into training pipeline The Adaptive Rider Optimization (ARO) algorithm was strategically integrated into the training pipeline to enhance the performance of the hybrid deep learning model by fine-tuning critical hyperparameters. Specifically, ARO was used to optimize four essential hyperparameters of the model:  Learning Rate (α) Batch Size (B) Number of Epochs (E) Dropout Rate (D) The objective function used by ARO was a composite performance metric, defined as: 11 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Fitness=\\:{\\omega\\:}_{1}.\\:accuracy+{\\omega\\:}_{2}.\\:F1\\:score-{\\omega\\:}_{3}.validation\\:loss$$\\end{document} In Eq. ( 11 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\omega\\:}_{1}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\omega\\:}_{2}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\omega\\:}_{3}$$\\end{document} Training Integration Workflow  Initialization Fitness Evaluation Rider Update Mechanism Convergence Check Final Training The final optimal hyperparameters identified by the ARO algorithm during training are summarized in Table 1  Table 1 Optimal hyperparameters tuned by ARO. Hyperparameter Optimized value Learning rate (α) 0.001 Batch size (B) 32 Number of epochs (E) 100 Dropout rate (D) 0.25 This integration of ARO ensured efficient exploration of the hyperparameter space, leading to improved generalization and reduced training time compared to manual tuning or grid/random search. The novelty of the proposed architecture lies in its sequential integration of Inception v3 and ResNet-50, strategically engineered to exploit their complementary strengths in multi-scale feature extraction and deep residual classification. Unlike conventional hybrid models that simply stack pre-trained networks, our design ensures modular feature transfer, where the output of Inception v3’s final global average pooling layer serves as the optimized input for ResNet-50, avoiding redundant feature maps and enabling a leaner yet more expressive representation. This two-stage training pipeline, which begins with frozen Inception v3 layers and fine-tunes only later stages, preserves generalizable low-level features while adapting high-level filters to Alzheimer-specific patterns. Furthermore, ResNet-50 is customized with modified fully connected layers tailored to the four-class Alzheimer dataset, ensuring better discrimination among closely related classes such as Mild and Very Mild Dementia. The proposed design surpasses earlier architectures by maintaining both computational efficiency and high classification accuracy through a tightly coupled but non-overlapping dual-stage learning strategy, thereby achieving a unique balance between generalization, specificity, and performance. Results and discussion The experimental setup for evaluating the proposed hybrid deep learning model for Alzheimer’s disease detection is detailed in this section. The dataset used in this study is publicly available from the Kaggle platform and can be accessed at the following URL: https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images.This The proposed hybrid deep learning model was implemented using Python with TensorFlow and Keras libraries on a high-performance computing system equipped with an NVIDIA Tesla V100 GPU, Intel Xeon processor, and 128 GB RAM. The model was trained using a categorical cross-entropy loss function, which is standard for multi-class classification tasks. For optimization, the Stochastic Gradient Descent (SGD) optimizer was employed with a momentum of 0.9 to accelerate convergence and avoid local minima. The initial learning rate was set to 0.001, and an adaptive learning rate scheduler was applied that reduced the learning rate by a factor of 0.1 if the validation loss did not improve for 10 consecutive epochs (ReduceLROnPlateau strategy). A dropout rate of 0.25 was incorporated to prevent overfitting, as determined through the ARO-based hyperparameter optimization process. The training was conducted over a maximum of 100 epochs with early stopping enabled, monitoring validation loss with a patience of 15 epochs to prevent overfitting. The dataset from the Kaggle repository was divided into 80% training and 20% testing, and within the training set, 20% was further allocated for validation. The data splitting was performed using a stratified sampling approach to preserve class distribution across training, validation, and test sets. This ensured a balanced representation of all four classes (Non-Demented, Very Mild Demented, Mild Demented, and Moderate Demented) throughout the training and evaluation process. Data augmentation techniques were applied only on the training data to synthetically enhance the sample size and improve generalization. The proposed model performance analysis is presented through simulation performed using python tool. The dataset utilized for experimentation is obtained from Kaggle repository. The four class Alzheimer’s Dataset 42 5 2  Fig. 5 Input images in Alzheimer’s dataset: ( a b c d  Table 2 Dataset description. S. no Class name Training Test Total 1 Mild demented 717 179 896 2 Moderate demented 52 12 64 3 Non-demented 2560 640 3200 4 Very mild demented 1792 448 2240 The data preprocessing step of proposed model includes data normalization and data augmentation. From Table 2 6 3  Fig. 6 Data augmentation ( a b c d e f  Table 3 Dataset description after augmentation. S. no Class name Training Test Total 1 Moderate demented 312 72 384 2 Very mild demented 10,752 2688 13,440 3 Non-demented 15,360 3840 19,200 4 Mild demented 4302 1074 5376 The metrics considered for performance evaluation are presented in Table 4  Table 4 Performance analysis of proposed model. S. no Metrics Training Test 1 Precision 96.26 95.65 2 Recall 96.89 96.48 3 Specificity 98.48 98.24 4 F1-Score 96.57 96.06 5 Accuracy 98.53 97.88 Further to validate the superior performance of the proposed model in Alzheimer detection, existing deep learning and hybrid deep learning models are considered for comparative analysis. Nagarathna.et.al 43 44 45 The confusion matrix analysis of proposed work is depicted in Fig. 7 7  Fig. 7 Confusion matrix analysis. Precision analysis The comparative analysis of the precision metric is presented in Fig. 8  Fig. 8 Precision comparative analysis. From the numerical values given in the Fig. 7 Recall analysis The recall metric presented in Fig. 9  Fig. 9 Recall comparative analysis. The proposed model’s higher performance can be observed from the comparative analysis. The difference in recall metric between the proposed model and custom CNN is 1.68% and for Inceptionv1, the difference is 6.35%. When compared to VGG-16 model the difference is 6.58%. For AlexNet, the difference is 9.98%. For ResNet-18, the difference is 7.88%. For the hybrid DenseNet121 and 201 models, the difference is 7.48% for both models. HCNN has a 5.24% difference in precision metrics, and the least performance is exhibited by the conventional CNN model, which is 31.48% less than the proposed model. Specificity analysis Figure 9 10  Fig. 10 Specificity comparative analysis. The performance difference in specificity metric between the proposed model and custom CNN is 4.64% and for Inceptionv1, the difference is 10.04%. When compared to VGG-16 model the difference is 7.84%. For AlexNet, the difference is 13.44%. For ResNet-18, the difference is 13.94%. For the hybrid DenseNet121 and 201 models, the difference is 2.24 and 1.24, respectively. HCNN has a 23.99% difference in specificity metrics, and the least performance is exhibited by the conventional CNN model, which is 43.92% less than the proposed model. F1-Score analysis The comparative analysis of the F1-score is presented in Fig. 11 10  Fig. 11 F1-Score comparative analysis. Accuracy analysis Figure 11 12  Fig. 12 Accuracy comparative analysis. The performance difference in precision metric between the proposed model and existing CNN and VGG-16 models is 1.68%. When compared to Inceptionv1, the difference is 9.28%. For AlexNet, the difference is 6.48%. For ResNet-18, the difference is 10.38%. For the hybrid DenseNet121 and 201 models, the difference is 7.99% and 6.13%, respectively. The HCNN has a 2.23% difference in accuracy metrics, and the least performance is exhibited by the conventional CNN model, which is 14.35% less than the proposed model. The overall performance comparative analysis with the existing methods is presented in Table 5  Table 5 Performance analysis with existing methods. S. no Authors Methods Metrics Precision Recall F1-Score Specificity Accuracy 1 Nagarathna et.al. 40 CNN 69.00 65.00 72.00 54.32 83.53 2 HCNN 91.25 91.24 92.00 74.25 95.52 3 Sharma et al. 41 Hybrid DenseNet121 92.00 89.00 90.00 96.00 89.89 4 Hybrid DenseNet201 93.00 89.00 90.00 97.00 91.75 5 Srivastava et al. 42 VGG-16 91.40 89.90 93.70 90.40 96.20 6 ResNet-18 85.40 88.60 86.50 84.30 87.50 7 AlexNet 88.50 86.50 85.80 84.80 91.40 8 Inception v1 89.30 90.30 90.10 88.20 88.60 9 Custom CNN 91.40 94.80 95.30 93.60 96.20 10 Proposed 95.65 96.48 96.06 98.24 97.88 Ablation study To assess the individual contributions of different components in the proposed hybrid deep learning model, we conducted an ablation study as shown in Table 6 5  Table 6 Ablation study. Configuration Accuracy (%) F1-Score (%) ARO + Inception v3 + ResNet-50 97.88 96.06 Adam + Inception v3 + ResNet-50 94.52 92.70 Grid Search + Inception v3 + ResNet-50 93.84 91.85 Bayesian Opt. + Inception v3 + ResNet-50 95.10 93.62 ARO + Inception v3 only 94.36 92.84 ARO + ResNet-50 only 93.29 91.47 Further, to evaluate the impact of each network component, we trained models with (i) only Inception v3, (ii) only ResNet-50, and (iii) the proposed hybrid architecture (Inception v3 + ResNet-50). When used independently, Inception v3 achieved 94.36% accuracy and 92.84% F1-score, while ResNet-50 achieved 93.29% accuracy and 91.47% F1-score. In contrast, the full hybrid model reached 97.88% accuracy and 96.06% F1-score. This improvement highlights the complementarity between Inception v3’s multi-scale feature extraction and ResNet-50’s deep residual learning for classification. Inception v3 captures detailed textural and spatial features, while ResNet-50 effectively classifies subtle inter-class variations. The sequential integration enhances both representation learning and discrimination capability, thereby offering a more robust framework for Alzheimer’s classification. Training and validation analysis Figure 13  Fig. 13 Training and Validation Loss performance. The graph illustrates that both the training loss (blue curve) and validation loss (orange curve) consistently decreased over the epochs. Importantly, the two curves converge as the training progresses, indicating that the model is learning effectively and is not overfitting to the training data. If the model were overfitting, we would observe a significant divergence between the training and validation loss curves, with the validation loss increasing while the training loss continues to decrease. In this graph, the validation loss closely follows the training loss, demonstrating that the model maintains its performance on unseen validation data. This behavior indicates that the model generalizes well and effectively captures the underlying patterns in the data without memorizing the training examples. Statistical analysis of the proposed work We have conducted a thorough comparative analysis to validate the performance of the proposed hybrid deep learning model. We have evaluated the model using standard performance metrics such as precision, recall, specificity, F1-score, and accuracy. These metrics were calculated for both the training and testing processes, and the results were compared against several existing models including CNN, VGG-16, ResNet-18, AlexNet, Inception v1, and hybrid models like DenseNet121 and DenseNet201. Precision Recall Specificity F1-Score Accuracy Comparative analysis 8 9 10 11 12 4 Experimental validation To further validate the robustness of the proposed model’s performance, we computed 95% confidence intervals for two key evaluation metrics: accuracy and F1-score. Based on the test set comprising 384 samples, the accuracy of 97.88% yields a 95% confidence interval of [96.44%, 99.32%], while the F1-score of 96.06% corresponds to a 95% confidence interval of [94.11%, 98.01%]. These intervals were calculated using a normal approximation to the binomial distribution, assuming independent Bernoulli trials for classification outcomes. The narrow bounds of the confidence intervals indicate the statistical reliability and consistency of the proposed model across varying samples, reinforcing its superiority over existing techniques. To establish the statistical significance of performance improvements, a paired t-test was conducted on the accuracy scores obtained over five independent runs of both the proposed model and the best-performing baseline model. The test yielded a t-statistic of 27.57 and a p-value of 1.03 × 10⁻⁵, indicating that the observed difference in accuracies is highly statistically significant ( p Conclusion An optimized hybrid deep learning model for Alzheimer detection is presented in this research work using Inception v3 and ResNet 50 algorithms. The hybrid model incorporates an Adaptive Rider Optimization (ARO) algorithm to fine tune the network parameters so that improved accuracy in Alzheimer detection was attained in the proposed research work. experimentations using benchmark dataset validates the proposed model performance with various metrics and compared with conventional deep learning and hybrid deep learning algorithms. The proposed model outperformed existing models CNN, HCNN, Hybrid DenseNet121, Hybrid DenseNet201, AlexNet, ResNet-18, VGG-16, Custom CNN, Inception v1 models with precision of 95.65%, recall of 96.48%, specificity of 98.24%, f1-score of 96.06% and accuracy of 97.88%. However, challenges such as high computational demands and data variability were encountered. Several critical areas of application are available as the future scope of this research work. Firstly, integrating multi-modal data such as genetic information, clinical test records, and other imaging modalities like PET and CT scans could provide a more comprehensive understanding of Alzheimer’s disease progression and improve the accuracy of early detection. Secondly, exploring the use of more advanced deep learning architectures, such as transformer-based models and attention mechanisms, could further enhance the model’s ability to capture complex patterns in the data. Additionally, implementing federated learning approaches could enable the use of distributed datasets while preserving patient privacy, thereby addressing data scarcity and heterogeneity issues. Another important direction is the development of real-time diagnostic tools and mobile applications that utilize the proposed model for practical and accessible Alzheimer’s detection in clinical settings. Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Author contributions Sundar Raj and S. Senthilkumar carried out this research work and draft the first copy. S. Sivamani supported in literature review and final drafting of the manuscript. C. Gunasundari contributed in revision of the manuscript. Funding There is no source of funding. Data availability The data’s used to support the findings of this study are included within this article. Declarations Ethics approval and consent to participate This study does not involve any direct interaction with human subjects. The MRI image data used was obtained from a publicly available dataset hosted on the Kaggle platform, which contains fully anonymized medical images released for research purposes. Therefore, ethical approval and informed consent were not required. Competing interests The authors declare no competing interests. References 1. https://www.alz.org/alzheimers-dementia/facts-figures 2. Petti U Baker S Korhonen A A systematic literature review of automatic Alzheimer’s disease detection from speech and language J. Am. Med. Inform. Assoc. 2020 27 11 1784 1797 10.1093/jamia/ocaa174 32929494 PMC7671617 Petti, U., Baker, S. & Korhonen, A.. systematic literature review of automatic Alzheimer’s disease detection from speech and language. J. Am. Med. Inform. Assoc. 27 32929494 10.1093/jamia/ocaa174 PMC7671617 3. Shaw LM Detection of Alzheimer disease pathology in patients using biochemical biomarkers: prospects and challenges for use in clinical practice J. Appl. Lab. Med. 2020 5 1 183 193 10.1373/jalm.2019.029587 31848218 PMC7246169 Shaw, L. M. et al. Detection of Alzheimer disease pathology in patients using biochemical biomarkers: prospects and challenges for use in clinical practice. J. Appl. Lab. Med. 5 31848218 10.1373/jalm.2019.029587 PMC7246169 4. Sharma, A. et al. Alzheimer’s patients detection using support vector machine (SVM) with quantitative analysis. Neurosci. Inf. 1 5. Shukla A Tiwari R Tiwari S Review on Alzheimer disease detection methods: automatic pipelines and machine learning techniques Sci 2023 5 1 1 24 10.3390/sci5010013 Shukla, A., Tiwari, R. & Tiwari, S. Review on Alzheimer disease detection methods: automatic pipelines and machine learning techniques. Sci 5 6. Ebrahimighahnavieh A Luo S Chiong R Deep learning to detect Alzheimer’s disease from neuroimaging: A systematic literature review Comput. Methods Programs Biomed. 2020 187 1 49 10.1016/j.cmpb.2019.105242 31837630 Ebrahimighahnavieh, A., Luo, S. & Chiong, R. Deep learning to detect Alzheimer’s disease from neuroimaging: A systematic literature review. Comput. Methods Programs Biomed. 187 10.1016/j.cmpb.2019.105242 31837630 7. Ramachandran L Mangaiyarkarasi SP Subramanian A Senthilkumar S Shrimp classification for white spot syndrome detection through enhanced gated recurrent unit-based wild geese migration optimization algorithm Virus Genes 2024 10.1007/s11262-023-02049-0 38253919 Ramachandran, L., Mangaiyarkarasi, S. P., Subramanian, A. & Senthilkumar, S. Shrimp classification for white spot syndrome detection through enhanced gated recurrent unit-based wild geese migration optimization algorithm. Virus Genes 38253919 10.1007/s11262-023-02049-0 8. Ramachandran L Mohan V Senthilkumar S Ganesh J Early detection and identification of white spot syndrome in shrimp using an improved deep convolutional neural network J. Intell. Fuzzy Syst. 2023 45 4 6429 6440 10.3233/JIFS-232687 Ramachandran, L., Mohan, V., Senthilkumar, S. & Ganesh, J. Early detection and identification of white spot syndrome in shrimp using an improved deep convolutional neural network. J. Intell. Fuzzy Syst. 45 9. Ganesh D Implementation of convolutional neural networks for detection of Alzheimer’s disease BioGecko J. New. Z. Herpetology 2023 12 01 71 82 Ganesh, D. et al. Implementation of convolutional neural networks for detection of Alzheimer’s disease. BioGecko J. New. Z. Herpetology 12 10. Francis A Francis IA Early detection of Alzheimer’s disease using local binary pattern and convolutional neural network Multimedia Tools Appl. 2021 80 496 29585 29600 10.1007/s11042-021-11161-y Francis, A. & Pandian, I. A. Early detection of Alzheimer’s disease using local binary pattern and convolutional neural network. Multimedia Tools Appl. 80 11. Vembarasi, K. et al. White spot syndrome detection in shrimp using neural network model. In Proceedings of the 18th INDIACom; INDIACom-; IEEE Conference ID: 57xxx, 2024 11th International Conference on Computing for Sustainable Global Development, 28th Feb-01st March 2024 12. Illakiya T Karthik R Automatic detection of Alzheimer’s disease using deep learning models and neuroimaging: current trends and future perspectives Neuroinformatics 2023 21 339 364 10.1007/s12021-023-09625-7 36884142 Illakiya, T. & Karthik, R. Automatic detection of Alzheimer’s disease using deep learning models and neuroimaging: current trends and future perspectives. Neuroinformatics 21 36884142 10.1007/s12021-023-09625-7 13. Ammar RB Ayed YB Language-related features for early detection of Alzheimer disease Procedia Comput. Sci. 2020 176 763 770 10.1016/j.procs.2020.09.071 Ammar, R. B. & Ayed, Y. B. Language-related features for early detection of Alzheimer disease. Procedia Comput. Sci. 176 14. Shankar K Alzheimer detection using group Grey Wolf Optimization based features with convolutional classifier Comput. Electr. Eng. 2019 77 230 243 10.1016/j.compeleceng.2019.06.001 Shankar, K. et al. Alzheimer detection using group Grey Wolf Optimization based features with convolutional classifier. Comput. Electr. Eng. 77 15. Varatharajan R Manogaran G Priyan MK Sundarasekar R Wearable sensor devices for early detection of Alzheimer disease using dynamic time warping algorithm Cluster Comput. 2018 21 681 690 10.1007/s10586-017-0977-2 Varatharajan, R., Manogaran, G., Priyan, M. K. & Sundarasekar, R. Wearable sensor devices for early detection of Alzheimer disease using dynamic time warping algorithm. Cluster Comput. 21 16. Chakravarthy A Panda BS Nayak SK Review and comparison for Alzheimer’s disease detection with machine learning techniques Int. Neurourol. J. 2023 27 4 403 409 Chakravarthy, A., Panda, B. S. & Nayak, S. K. Review and comparison for Alzheimer’s disease detection with machine learning techniques. Int. Neurourol. J. 27 17. Acharya UR Automated detection of Alzheimer’s disease using brain MRI images– A study with various feature extraction techniques J. Med. Syst. 2019 43 2 1 14 10.1007/s10916-019-1428-9 31396722 Acharya, U. R. et al. Automated detection of Alzheimer’s disease using brain MRI images– A study with various feature extraction techniques. J. Med. Syst. 43 10.1007/s10916-019-1428-9 31396722 18. Kishore, P., Kumari, C. U., Kumar, M. N. & Pavani, T. Detection and analysis of Alzheimer’s disease using various machine learning algorithms. Mater. Today Proc. 45 19. Sampath R Indumathi J Earlier detection of Alzheimer disease using N-fold cross validation approach J. Med. Syst. 2018 42 217 1 11 10.1007/s10916-018-1068-5 30280260 Sampath, R. & Indumathi, J. Earlier detection of Alzheimer disease using N-fold cross validation approach. J. Med. Syst. 42 10.1007/s10916-018-1068-5 30280260 20. Feng W Automated MRI-based deep learning model for detection of Alzheimer’s disease process Int. J. Neural Syst. 2020 30 06 1 14 10.1142/S012906572050032X 32498641 Feng, W. et al. Automated MRI-based deep learning model for detection of Alzheimer’s disease process. Int. J. Neural Syst. 30 10.1142/S012906572050032X 32498641 21. Helaly HA Badawy M Haikal AY Deep learning approach for early detection of Alzheimer’s disease Cogn. Comput. 2022 14 1711 1727 10.1007/s12559-021-09946-2 PMC8563360 34745371 Helaly, H. A., Badawy, M. & Haikal, A. Y. Deep learning approach for early detection of Alzheimer’s disease. Cogn. Comput. 14 10.1007/s12559-021-09946-2 PMC8563360 34745371 22. Venugopalan J Tong L Hassanzadeh HR Wang MD Multimodal deep learning models for early detection of Alzheimer’s disease stage Sci. Rep. 2021 11 3254 1 13 33547343 10.1038/s41598-020-74399-w PMC7864942 Venugopalan, J., Tong, L., Hassanzadeh, H. R. & Wang, M. D. Multimodal deep learning models for early detection of Alzheimer’s disease stage. Sci. Rep. 11 33547343 10.1038/s41598-020-74399-w PMC7864942 23. Jo T Nho K Risacher SL Saykin AJ Deep learning detection of informative features in tau PET for Alzheimer’s disease classification BMC Bioinform. 2020 21 496 1 13 10.1186/s12859-020-03848-0 PMC7768646 33371874 Jo, T., Nho, K., Risacher, S. L. & Saykin, A. J. Deep learning detection of informative features in tau PET for Alzheimer’s disease classification. BMC Bioinform. 21 10.1186/s12859-020-03848-0 PMC7768646 33371874 24. Chui KT Gupta BB Alhalabi W Alzahrani FS An MRI scans-based Alzheimer’s disease detection via convolutional neural network and transfer learning Diagnostics 2022 12 7 1 14 10.3390/diagnostics12071531 PMC9318866 35885437 Chui, K. T., Gupta, B. B., Alhalabi, W. & Alzahrani, F. S. An MRI scans-based Alzheimer’s disease detection via convolutional neural network and transfer learning. Diagnostics 12 10.3390/diagnostics12071531 PMC9318866 35885437 25. Ghazal, T. M. et al. Alzheimer disease detection empowered with transfer learning. Computers Mater. Continua 70 26. Lokesh K Early Alzheimer’s disease detection using deep learning EAI Endorsed Trans. Pervasive Health Technol. 2023 9 1 9 10.4108/eetpht.9.3966 Lokesh, K. et al. Early Alzheimer’s disease detection using deep learning. EAI Endorsed Trans. Pervasive Health Technol. 9 27. Miltiadous, A., Gionanidis, E., Tzimourta, K. D., Giannakeas, N. & Tzallas, A. T. DICE-Net: A novel convolution-transformer architecture for Alzheimer detection in EEG signals, IEEE Access 11 28. Pan D Early detection of Alzheimer’s disease using magnetic resonance imaging: A novel approach combining convolutional neural networks and ensemble learning Front. NeuroSci. 2020 14 1 19 10.3389/fnins.2020.00259 32477040 PMC7238823 Pan, D. et al. Early detection of Alzheimer’s disease using magnetic resonance imaging: A novel approach combining convolutional neural networks and ensemble learning. Front. NeuroSci. 14 32477040 10.3389/fnins.2020.00259 PMC7238823 29. Mujahid M An efficient ensemble approach for Alzheimer’s disease detection using an adaptive synthetic technique and deep learning Diagnostics 2023 13 15 1 20 10.3390/diagnostics13152489 PMC10417320 37568852 Mujahid, M. et al. An efficient ensemble approach for Alzheimer’s disease detection using an adaptive synthetic technique and deep learning. Diagnostics 13 10.3390/diagnostics13152489 PMC10417320 37568852 30. Balasubramaniam S Kadry S Kumar KS Osprey Gannet optimization enabled CNN based transfer learning for optic disc detection and cardiovascular risk prediction using retinal fundus images Biomed. Signal Process. Control 2024 93 1 106177 106188 10.1016/j.bspc.2024.106177 Balasubramaniam, S., Kadry, S. & Kumar, K. S. Osprey Gannet optimization enabled CNN based transfer learning for optic disc detection and cardiovascular risk prediction using retinal fundus images. Biomed. Signal Process. Control 93 31. Kadry, S., Dhanaraj, R. K. & Manthiramoorthy, C. Res-Unet based blood vessel segmentation and cardiovascular disease prediction using chronological chef-based optimization algorithm based deep residual network from retinal fundus images. Multimedia Tools Appl. 1 32. Choudhury A Balasubramaniam S Kumar AP Kumar SNP PSSO: political squirrel search optimizer-driven deep learning for severity level detection and classification of lung cancer Int. J. Inform. Technol. Decis. Mak. 2023 1 1 1 34 Choudhury, A., Balasubramaniam, S., Kumar, A. P. & Kumar, S. N. P. PSSO: political squirrel search optimizer-driven deep learning for severity level detection and classification of lung cancer. Int. J. Inform. Technol. Decis. Mak. 1 33. Balasubramaniam S Syed MH More NS Pole ally V Deep learning-based power prediction aware charge scheduling approach in cloud based electric vehicular network Eng. Appl. Artif. Intell. 2023 121 1 105869 105879 Balasubramaniam, S., Syed, M. H., More, N. S. & Pole ally, V. Deep learning-based power prediction aware charge scheduling approach in cloud based electric vehicular network. Eng. Appl. Artif. Intell. 121 34. Dakshinamoorthy C Hybrid Whale and Gray Wolf deep learning optimization algorithm for prediction of Alzheimer’s disease Mathematics 2023 11 5 1136 1146 10.3390/math11051136 Dakshinamoorthy, C. et al. Hybrid Whale and Gray Wolf deep learning optimization algorithm for prediction of Alzheimer’s disease. Mathematics 11 35. Rajasree RS Brintha Rajakumari S Ensemble-of-classifiers-based approach for early Alzheimer’s disease detection Multimedia Tools Appl. 2024 83 6 16067 16095 10.1007/s11042-023-16023-3 Rajasree, R. S. & Brintha Rajakumari, S. Ensemble-of-classifiers-based approach for early Alzheimer’s disease detection. Multimedia Tools Appl. 83 36. Sinha Roy R Sen A A hybrid deep learning framework to predict Alzheimer’s disease progression using generative adversarial networks and deep convolutional neural networks Arab. J. Sci. Eng. 2024 49 3 3267 3284 10.1007/s13369-023-07973-9 Sinha Roy, R. & Sen, A. A hybrid deep learning framework to predict Alzheimer’s disease progression using generative adversarial networks and deep convolutional neural networks. Arab. J. Sci. Eng. 49 37. Mujahid M An efficient ensemble approach for Alzheimer’s disease detection using an adaptive synthetic technique and deep learning Diagnostics 2023 13 15 2489 2498 10.3390/diagnostics13152489 37568852 PMC10417320 Mujahid, M. et al. An efficient ensemble approach for Alzheimer’s disease detection using an adaptive synthetic technique and deep learning. Diagnostics 13 37568852 10.3390/diagnostics13152489 PMC10417320 38. Sorour, S. E. et al. Classification of Alzheimer’s disease using MRI data based on deep learning techniques. J. King Saud Univ.-Comput. Inf. Sci. 36 39. Alnowaiser K Saber A Hassan E Awad WA An optimized model based on adaptive convolutional neural network and grey Wolf algorithm for breast cancer diagnosis PlosOne 2024 10.1371/journal.pone.0304868 PMC11332925 39159151 Alnowaiser, K., Saber, A., Hassan, E. & Awad, W. A. An optimized model based on adaptive convolutional neural network and grey Wolf algorithm for breast cancer diagnosis. PlosOne 10.1371/journal.pone.0304868 PMC11332925 39159151 40. Elbedwehy S Integrating neural networks with advanced optimization techniques for accurate kidney disease diagnosis Sci. Rep. 2024 14 21740 10.1038/s41598-024-71410-6 39289394 PMC11408592 Elbedwehy, S. et al. Integrating neural networks with advanced optimization techniques for accurate kidney disease diagnosis. Sci. Rep. 14 39289394 10.1038/s41598-024-71410-6 PMC11408592 41. Saber A An optimized ensemble model based on meta-heuristic algorithms for effective detection and classification of breast tumors Neural Comput. Applic 2025 37 4881 4894 10.1007/s00521-024-10719-9 Saber, A. et al. An optimized ensemble model based on meta-heuristic algorithms for effective detection and classification of breast tumors. Neural Comput. Applic 37 42. https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images 43. Nagarathna CR Kusuma M Automatic diagnosis of Alzheimer’s disease using hybrid model and CNN Int. J. Innovative Res. Sci. Eng. Technol. 2022 3 1 1 4 Nagarathna, C. R. & Kusuma, M. Automatic diagnosis of Alzheimer’s disease using hybrid model and CNN. Int. J. Innovative Res. Sci. Eng. Technol. 3 44. Sharma S HTLML: hybrid AI based model for detection of Alzheimer’s disease Diagnostics 2022 12 8 1 16 10.3390/diagnostics12081833 PMC9406825 36010183 Sharma, S. et al. HTLML: hybrid AI based model for detection of Alzheimer’s disease. Diagnostics 12 10.3390/diagnostics12081833 PMC9406825 36010183 45. Srivastava S Comparative analysis of deep learning image detection algorithms J. Big Data 2021 8 66 10.1186/s40537-021-00434-w Srivastava, S. et al. Comparative analysis of deep learning image detection algorithms. J. Big Data 8 ",
  "metadata": {
    "Title of this paper": "Comparative analysis of deep learning image detection algorithms",
    "Journal it was published in:": "Scientific Reports",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12485127/"
  }
}
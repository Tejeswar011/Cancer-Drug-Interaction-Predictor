{
  "title": "Paper_103",
  "abstract": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12489133 PMC12489133.1 12489133 12489133 41034364 10.1038/s41598-025-15702-5 15702 1 Article An interpretable hybrid deep learning framework for gastric cancer diagnosis using histopathological imaging Ren Tengfei 1 Govindarajan Vijay 2 Bourouis Sami 3 Wang Xiangkun 4 Ke Shanbao keshanbao88@163.com 5 1 https://ror.org/04ypx8c21 grid.207374.5 0000 0001 2189 3846 Department of Hemangioma, Henan Provincial People’s Hospital, Zhengzhou University People’s Hospital, Henan University People’s Hospital, 2 https://ror.org/01sh85g09 grid.497099.a 0000 0004 0399 5285 Distribution and Supply Technology, Expedia Group, 3 https://ror.org/014g1a453 grid.412895.3 0000 0004 0419 5255 Department of Information Technology, College of Computers and Information Technology, Taif University, 4 https://ror.org/035adwg89 grid.411634.5 0000 0004 0632 4559 Department of Breast and Thyroid Surgery, Juye County People’s Hospital, 5 https://ror.org/04ypx8c21 grid.207374.5 0000 0001 2189 3846 Department of Oncology, Henan Provincial People’s Hospital, Zhengzhou University People’s Hospital, Henan University People’s Hospital, 1 10 2025 2025 15 478255 34204 2 6 2025 11 8 2025 01 10 2025 03 10 2025 03 10 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access http://creativecommons.org/licenses/by-nc-nd/4.0/ The increasing incidence of gastric cancer and the complexity of histopathological image interpretation present significant challenges for accurate and timely diagnosis. Manual assessments are often subjective and time-intensive, leading to a growing demand for reliable, automated diagnostic tools in digital pathology. This study proposes a hybrid deep learning approach combining convolutional neural networks (CNNs) and Transformer-based architectures to classify gastric histopathological images with high precision. The model is designed to enhance feature representation and spatial contextual understanding, particularly across diverse tissue subtypes and staining variations. Three publicly available datasets—GasHisSDB, TCGA-STAD, and NCT-CRC-HE-100 K—were utilized to train and evaluate the model. Image patches were preprocessed through stain normalization, augmented using standard techniques, and fed into the hybrid model. The CNN backbone extracts local spatial features, while the Transformer encoder captures global context. Performance was assessed using fivefold cross-validation and evaluated through accuracy, F1-score, AUC, and Grad-CAM-based interpretability. The proposed model achieved a 99.2% accuracy on the GasHisSDB dataset, with a macro F1-score of 0.991 and AUC of 0.996. External validation on TCGA-STAD and NCT-CRC-HE-100 K further confirmed the model’s robustness. Grad-CAM visualizations highlighted biologically relevant regions, demonstrating interpretability and alignment with expert annotations. This hybrid deep learning framework offers a reliable, interpretable, and generalizable tool for gastric cancer diagnosis. Its superior performance and explainability highlight its clinical potential for deployment in digital pathology workflows. Keywords Gastric cancer classification Histopathological image analysis Deep learning CNN-transformer hybrid model Medical image interpretation Grad-CAM explainability Multi-dataset validation Subject terms Computational science Computer science pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © Springer Nature Limited 2025 Introduction Background and motivation Gastric cancer remains one of the leading causes of cancer-related deaths worldwide, with particularly high prevalence in East Asia and parts of Eastern Europe 1 2 3 4 5 6 Problem statement Although histopathological examination is the gold standard for diagnosing gastric cancer, manual analysis of whole slide images (WSIs) is tedious and affected by the subjectivity of different pathologists 7 8 9 10 11 Objectives  To design a hybrid deep learning architecture that integrates CNNs and Transformers for robust gastric cancer classification from histopathological image patches. To evaluate the proposed model across multiple public datasets (GasHisSDB, TCGA-STAD, NCT-CRC-HE-100 K) and ensure generalization across staining variations and tissue types. To improve model interpretability by incorporating Grad-CAM-based visual explanations aligned with clinically relevant histological features. To benchmark the proposed model against baseline methods using comprehensive metrics and cross-validation, validating its diagnostic reliability and clinical applicability. Contribution This paper presents a deep learning-based framework for gastric cancer classification using publicly available histopathological image datasets. The primary contributions of this work are as follows: Development of a robust classification pipeline based on state-of-the-art convolutional and transformer-based architectures, tailored for patch-level histopathological image analysis of gastric cancer. Integration of preprocessing techniques, including stain normalization and data augmentation, to enhance model generalization and reduce variability due to imaging conditions. Evaluation on a public dataset, ensuring reproducibility and facilitating comparison with future studies. We employ stratified cross-validation and report results using multiple performance metrics. Interpretability through visual explanations, such as Grad-CAM, which highlight the key image regions influencing the model’s decisions, thereby supporting potential clinical usability. Comprehensive experimental analysis, including ablation studies and comparisons with standard CNN baselines, to demonstrate the advantages of hybrid models in capturing both local and global features in histopathological images. These contributions aim to advance the development of scalable, interpretable, and clinically relevant deep learning tools for gastric cancer diagnosis. The remainder of this paper is organized as follows: Section \" Related work Methodology Results Discussion Conclusion and future work Related work In recent years, deep learning has become a powerful paradigm for cancer diagnosis, providing high performance in image classification, detection, and segmentation tasks over the past decade. In particular, convolutional neural networks (CNNs) have performed extremely well on cancer of various types, including breast, lung, prostate, and skin cancers 7 12 5 13 3 14 15 16 17 18 3 19 20 21 22 23 24 6 10 25 26 20 27 11 28 5 29 10 26 Cho et al. 30 31 32 33 1 Table 1 Summary of prior works on histopathological image classification. Study Dataset Cancer type Model used Key contributions Limitations Cireşan, et al. 20 MITOS Breast CNN Early deep learning for mitosis detection Not gastric; focused on small ROI Xu, et al. 27 TCGA (private subset) Lung CNN Applied DL to WSIs Dataset not publicly available Tellez, et al. 11 CAMELYON16 Breast ResNet + color normalization Stain normalization improved performance Limited to binary classification Chen, et al. 28 Private (China) Gastric VGG16 Gastric cancer subtype classification No code or data sharing; small cohort Esteva, et al. 5 ISIC Skin Inception CNN Dermatologist-level performance Non-histology; not generalizable Jang, et al. 29 TCGA-STAD Gastric ResNet-50 Gastric cancer detection from H&E No patch-based analysis Chen, et al. 10 Synapse Organ Segmentation TransUNet Introduced hybrid CNN-transformer Not trained/tested on histopathology Li et al. 26 NCT-CRC-HE-100K Colorectal ViT First ViT on colorectal histology High GPU requirement; long training time Wu, et al. 34 TCGA Breast/Prostate MIL (weakly supervised) WSIs analysis without patch labels Requires large-scale WSIs; weak interpretability Divate, et al. 35 Private multi-center Gastric DenseNet + attention Gastric subtype prediction with attention No external validation; limited dataset Cho, et al. 30 Internal dataset (Confocal LE) Gastric CNN + Real-time AI Pipeline First real-time AI-based gastric histologic diagnosis using confocal laser imaging Requires confocal endomicroscopy; non-standard modality Li, et al. 31 Multi-institutional gastric biopsy Gastric Deep CNN (multi-class) Classified 4 gastric lesion types, including early/advanced cancer and dysplasia Dataset details limited; lacks WSI context Maity, et al. 32 Digital histopathology (public) Gastric CNN with XAI Developed explainable AI for gastric cancer with quantifiable Grad-CAM insights Does not assess generalization to external datasets Mohammed, et al. 33 Review (various datasets) Multiple CNN, Transformer (Survey) Survey of CNN/Transformer use in histopathology; overview of strengths and pitfalls Not an empirical study; no new model or evaluation Methodology The proposed methodology is a five-stage pipeline that performs accurate and interpretable classification of gastric cancer from histopathological images. The system starts by extracting patches from whole slide images (WSI), followed by stain normalization to reduce color variability. A hybrid deep learning architecture is used, which takes input of augmented image patches and combines a CNN backbone (such as ResNet50 or EfficientNet) with a transformer encoder for global context modeling. To make the model robust, the model is trained with a cross-entropy loss function and stratified k-fold cross-validation. Finally, class-discriminating regions are interpreted using Grad-CAM for clinical validation. Figure 1 Fig. 1 Proposed hybrid deep learning framework for gastric cancer classification and interpretability. Datasets This study utilizes three publicly available histopathological image datasets to improve model generalization and performance across diverse gastric cancer tissues. These datasets include GasHisSDB, TCGA-STAD, and NCT-CRC-HE-100 K. Together, they provide various image formats, staining protocols, and cancer subtypes, essential for training a robust and transferable classification model. The first dataset, GasHisSDB 36 37 38 2 Table 2 Dataset characteristics. Dataset Tissue Type Samples Image Size Use Case GasHisSDB Gastric (Normal, Dysplasia, Cancer) 245,196 patches 160 × 160 px Main training/testing TCGA-STAD Gastric WSIs (Tumor vs Normal) ~ 500 slides WSIs (patches: 224 × 224) External validation + Patch extraction NCT-CRC-HE-100 K Colorectal tissues (9 types) 100,000 patches 224 × 224 px Pretraining (transfer learning) These datasets were preprocessed consistently using stain normalization and quality filtering. Stratified sampling was applied to maintain class balance across training, validation, and testing sets. Each dataset was processed to ensure image normalization, labeling format, and patch resolution consistency. The combined use of domain-relevant gastric cancer data and related colorectal histology enables robust model development and supports transfer learning strategies to improve classification performance in resource-constrained or small-sample scenarios. Preprocessing and augmentation A standardized preprocessing pipeline was applied to all histopathological images to ensure consistency across datasets and improve model robustness. The first step involved resizing all image patches to a uniform resolution of 224 × 224 pixels. While GasHisSDB and NCT-CRC-HE-100 K already provide fixed-size patches, patches extracted from TCGA-STAD whole slide images were cropped to the same size to maintain compatibility during model training. Stain variability is a well-known challenge in histopathology due to differences in scanner types, staining protocols, and laboratory conditions. To address this, all images were normalized using the Macenko stain normalization method, which aligns color distributions while preserving tissue morphology. Training with this technique will minimize domain shift across multiple datasets and generalize well to unseen slides. During training, there was extensive data augmentation to increase dataset diversity and avoid overfitting. Augmentation techniques were horizontal and vertical flipping, random rotations (up to ± 30 degrees), brightness and contrast adjustments, and Gaussian noise injection. At runtime, these transformations were applied probabilistically using the Augmentations library, and so the model saw different visual variations in each training epoch. Specifically, for the TCGA-STAD dataset, background or artifact regions that exceeded the threshold value were excluded with an Otsu threshold tissue detection algorithm. Retained were only patches with sufficient tissue content to avoid noisy inputs affecting the model performance. In addition, all image intensities were zero mean and unit variance per channel before feeding them into the neural network. We propose the use of this preprocessing strategy, which standardizes input data and makes the model’s capacity to learn robust and transferable features between gastric and colorectal tissue domains. Model architecture The architecture proposed is a fusion of a Convolutional Neural Network (CNN) backbone with a Transformer encoder to make use of both local texture patterns and global contextual information of histopathological image patches. This hybrid technique has extended capability to overcome the weaknesses of traditional CNNs in identifying long-range spatial dependency, which is essential in identifying distributed malignancies in gastric tissue. Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X\\in {\\mathbb{R}}^{H\\times W\\times C}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H\\times W$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\phi }_{CNN}(\\cdot )$$\\end{document} 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ F = \\phi_{CNN} \\left( X \\right), F \\in {\\mathbb{R}}^{h \\times w \\times d} $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h\\times w$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${f}_{i}\\in {\\mathbb{R}}^{p\\times p\\times d}$$\\end{document} 2 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ z_{i} = W_{p} \\cdot {\\text{flatten}}\\left( {f_{i} } \\right) + b_{p} , z_{i} \\in {\\mathbb{R}}^{D} $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${W}_{p}\\in {\\mathbb{R}}^{D\\times ({p}^{2}d)}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${PE}_{i}$$\\end{document} 3 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\tilde{z}_{i} = z_{i} + PE_{i} $$\\end{document} The full sequence \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Z=[{\\widetilde{z}}_{1},{\\widetilde{z}}_{2},\\dots ,{\\widetilde{z}}_{N}]$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N$$\\end{document} 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Attention\\left( {Q,K,V} \\right) = softmax\\left( {\\frac{{QK^{T} }}{{\\sqrt {d_{k} } }}} \\right)V $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q,K,V\\in {\\mathbb{R}}^{N\\times {d}_{k}}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Z$$\\end{document} 5 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ MHSA\\left( Z \\right) = Concat\\left( {head_{1} , \\ldots ,head_{h} } \\right)W^{O} $$\\end{document} 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Z^{\\prime }  = LayerNorm\\left( {Z + MHSA\\left( Z \\right)} \\right),Z^{{\\prime \\prime }}  = LayerNorm\\left( {Z^{\\prime }  + FFN\\left( {Z^{\\prime } } \\right)} \\right) $$\\end{document} The final output token \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${Z}_{cls}^{{\\prime}{\\prime}}$$\\end{document} 7 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\hat{y} = softmax\\left( {W_{c} Z_{cls}^{^{\\prime\\prime}} + b_{c} } \\right) $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Wc\\in {\\mathbb{R}}^{K\\times D}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\widehat{y}$$\\end{document} 8 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ L = - \\mathop \\sum \\limits_{i = 1}^{K} y_{i} {\\text{log}}\\left( {\\hat{y}_{i} } \\right) $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${y}_{i}$$\\end{document} 2 Fig. 2 Proposed hybrid CNN-Transformer architecture. The model was trained using a supervised learning framework. Input image patches were first passed through the hybrid CNN-Transformer architecture, which produced a probability distribution over the output classes. The model was trained end-to-end, with the CNN backbone initialized using pretrained weights from ImageNet and the Transformer layers initialized randomly. All images were resized to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$224\\times 224$$\\end{document} The dataset was split into training (70%), validation (15%), and testing (15%) subsets using stratified sampling to preserve class distribution. Data augmentation, including horizontal/vertical flipping, random rotation ( \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\pm {30}^{\\circ }$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$p=0.3$$\\end{document}  Algorithm 1 Loss function and optimizer We used the categorical cross-entropy loss function to optimize model parameters, suitable for multi-class classification. Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\widehat{y}}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${y}_{i}\\in [\\text{0,1}]$$\\end{document} 9 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ LCE = - \\mathop \\sum \\limits_{i = 1}^{K} y_{i} log\\left( {\\hat{y}_{i} } \\right) $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${w}_{i}$$\\end{document} 10 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ L_{WCE} = - \\mathop \\sum \\limits_{i = 1}^{K} w_{i} y_{i} log\\left( {\\hat{y}_{i} } \\right) $$\\end{document} For optimization, we employed the Adam optimizer, which combines momentum and adaptive learning rates. The update rule for parameter \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t$$\\end{document} 11 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\theta_{t} = \\theta_{t - 1} - \\alpha \\cdot \\frac{{\\hat{m}_{t} }}{{\\sqrt {\\hat{v}_{t} } + \\in }} $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\widehat{m}}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\widehat{v}}_{t}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\epsilon $$\\end{document} The initial learning rate was set to \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha =1{0}^{-4}$$\\end{document} Evaluation metrics To assess the performance of the proposed hybrid model, we employed several standard evaluation metrics for multi-class classification. These metrics capture various aspects of the model’s behavior, including correctness, sensitivity to minority classes, and overall discriminative ability. Accuracy is the proportion of correctly predicted samples over the total number of predictions: 12 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Accuracy = \\frac{TP + TN}{{TP + TN + FP + FN}} $$\\end{document} In the multi-class setting, this becomes: 13 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Accuracy = \\frac{1}{N}\\mathop \\sum \\limits_{i = 1}^{N} 1\\left( {\\hat{y}_{i} = y_{i} } \\right) $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\widehat{y}}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${y}_{i}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c$$\\end{document} 14 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Precisionc = \\frac{{TP_{c} }}{{TP_{c} + FP_{c} }} $$\\end{document} 15 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Recallc = \\frac{{TP_{c} }}{{TP_{c} + FN_{c} }} $$\\end{document} The F1-score for each class is the harmonic mean of precision and recall: 16 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ F1_{c} = 2 \\cdot \\frac{{Precision_{c} + Recall_{c} }}{{Precision_{c} + Recall_{c} }} $$\\end{document} To summarize model performance over all classes, we use the macro-averaged F1-score: 17 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ F1_{macro} = \\frac{1}{K}\\mathop \\sum \\limits_{c = 1}^{K} F1_{c} $$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C\\in {Z}^{K\\times K}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${C}_{i,j}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$j$$\\end{document} Results To evaluate the effectiveness of the proposed hybrid CNN-Transformer architecture for gastric cancer classification, we conducted extensive experiments using three public datasets: GasHisSDB, TCGA-STAD, and NCT-CRC-HE-100 K. Evaluation metrics included accuracy, F1-score, AUC, sensitivity, specificity, and precision, with mean and maximum values reported across multiple validation folds. The proposed model achieved a mean accuracy of 99.2% on GasHisSDB, outperforming all baseline models. The architecture also showed strong generalization across the other datasets, maintaining high sensitivity and AUC. All results are derived from stratified fivefold cross-validation. Table 3 Table 3 Comprehensive performance comparison of models across datasets. Model Dataset Accuracy (%) meanF1 maxF1 meanAUC maxAUC meanSen maxSen meanSpe maxSpe ResNet50 GasHisSDB 95.6 0.955 0.961 0.973 0.977 0.942 0.961 0.972 0.980 EfficientNet-B3 GasHisSDB 96.8 0.962 0.970 0.976 0.980 0.947 0.968 0.978 0.982 ViTs GasHisSDB 97.5 0.969 0.976 0.981 0.986 0.953 0.976 0.980 0.987 Proposed Model GasHisSDB 99.2 0.991 0.995 0.996 0.998 0.985 0.995 0.993 0.997 TCGA-STAD 96.1 0.960 0.969 0.982 0.989 0.941 0.978 0.982 0.991 NCT-CRC-HE 94.4 0.942 0.950 0.975 0.982 0.926 0.955 0.976 0.985 Table 4 Table 4 Class-wise metrics (GasHisSDB – proposed model, fold-averaged). Class Precision Recall (Sen) F1-Score Specificity AUC Normal 0.990 0.988 0.989 0.996 0.997 Dysplasia 0.992 0.987 0.990 0.995 0.996 Adenocarcinoma 0.993 0.980 0.986 0.989 0.996 Macro Avg 0.991 0.985 0.991 0.993 0.996 Table 5 Table 5 Fold Cross-validation results (GasHisSDB – proposed model). Fold Accuracy (%) F1-Score AUC Precision Sensitivity Specificity Fold 1 99.0 0.990 0.995 0.990 0.985 0.992 Fold 2 99.1 0.991 0.996 0.991 0.984 0.993 Fold 3 99.4 0.994 0.998 0.993 0.988 0.996 Fold 4 99.2 0.991 0.997 0.991 0.983 0.994 Fold 5 99.2 0.990 0.996 0.990 0.984 0.992 Mean ± SD 99.2 ± 0.14 0.991 ± 0.0014 0.996 ± 0.001 0.991 ± 0.001 0.985 ± 0.0018 0.993 ± 0.0016 Table 6 Table 6 ROC-AUC scores by class (GasHisSDB – proposed model). Class Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Mean ± SD Normal 0.996 0.996 0.998 0.997 0.996 0.996 ± 0.0008 Dysplasia 0.994 0.995 0.997 0.996 0.996 0.996 ± 0.0011 Adenocarcinoma 0.997 0.998 0.999 0.997 0.996 0.997 ± 0.0011 Macro Avg 0.996 0.996 0.998 0.997 0.996 0.996 ± 0.001 Table 7 Table 7 Misclassification Summary – GasHisSDB Test Set (Proposed Model). True class Misclassified As Misclassified count Total samples Error rate (%) Normal Dysplasia 21 8173 0.26 Dysplasia Adenocarcinoma 35 8173 0.43 Adenocarcinoma Dysplasia 39 8173 0.48 As Shown in Fig. 3 Fig. 3 Model performance comparison on GasHisSDB. Figure 4 Fig. 4 Receiver operating characteristic (ROC) curves. Figure 5 Fig. 5 Precision–recall (PR) curves. Figure 6 Fig. 6 Confusion matrix heatmap for the test set of GasHisSDB. Figure 7 Fig. 7 Comparing fold-wise accuracy. Figure 8 Fig. 8 Training vs. validation accuracy and loss curves across epochs. Figure 9 Fig. 9 Distribution of accuracy, AUC, and F1-score across 5 cross-validation. Figure 10 Fig. 10 Radar plot comparing the proposed model and baselines across six criteria. Comparative analysis To further validate the effectiveness of the proposed hybrid CNN–Transformer model, we conducted a comparative evaluation against multiple baseline architectures, including ResNet50, EfficientNet-B3, and ViT, as well as recent methods from the literature. The models were trained and evaluated using identical preprocessing pipelines, data splits, and augmentation strategies for fairness. Evaluation was performed on the same test sets using stratified fivefold cross-validation. Statistical analysis was applied using the Wilcoxon signed-rank test to assess whether observed performance differences were significant. Table 8 Table 8 Comparison with baseline and published models on GasHisSDB. Ref Model Accuracy (%) Macro F1 AUC (Macro) Params (M) Inference Time (ms) 39 ResNet50 95.6 0.955 0.973 23.5 34 40 EfficientNet-B3 96.8 0.962 0.976 13.0 29 6 ViT 97.5 0.969 0.981 85.2 110 41 Jang et al. (Cancers, 2021) 96.0 0.958 0.975 41.8 50 Proposed Model 99.2 0.991 0.996 40.1 42 Table 9 p Table 9 Wilcoxon Signed-Rank Test Results (Proposed vs Others, 5-Fold CV). Comparison p Significant at α = 0.05? Proposed vs ResNet50 0.002 Yes Proposed vs EfficientNet-B3 0.006 Yes Proposed vs ViTs 0.041 Yes Proposed vs Jang et al 0.003 Yes Figure 11 p p Fig. 11 Heatmap showing p-values from pairwise Wilcoxon signed-rank tests between all evaluated models. This comparative analysis confirms that the proposed hybrid architecture offers significant improvements in accuracy and robustness, while maintaining efficiency in model size and inference speed. Its balanced design allows it to outperform deeper or heavier alternatives (e.g., ViT-large) and less interpretable CNN-only models. Explainability via Grad-CAM While achieving high classification accuracy is crucial, model interpretability is equally vital in medical AI, especially in histopathology, where black-box predictions are unlikely to be trusted by pathologists. To address this, we applied Gradient-weighted Class Activation Mapping (Grad-CAM) to the output of the final convolutional layers in the CNN backbone of our hybrid model. Grad-CAM enables localization of discriminative regions that contribute most to the final class prediction, offering visual transparency into the model’s decision-making process. Grad-CAM heatmaps were generated for each test image to highlight the regions most relevant to the predicted class. The results were evaluated qualitatively (through visual inspection) and quantitatively (via overlap with expert-annotated regions). The Grad-CAM outputs reveal that the proposed model consistently focuses on morphologically significant areas such as atypical nuclei, glandular disorganization, and cellular crowding, characteristics that pathologists rely on during diagnosis. For normal patches, the attention is diffuse and well-distributed; for dysplasia, it concentrates on gland border irregularities; and for adenocarcinoma, heatmaps highlight hyperchromatic nuclei and irregular clusters. As reported in Table 10 Table 10 Average Attention Overlap. Class Mean IoU Max IoU Dice Coefficient Max Dice Normal 0.682 0.732 0.788 0.821 Dysplasia 0.705 0.749 0.802 0.841 Adenocarcinoma 0.724 0.769 0.816 0.859 Average 0.704 0.750 0.802 0.840 This quantitative evaluation demonstrates a high degree of alignment between Grad-CAM visualizations and pathologist-defined regions of interest, particularly for dysplasia and adenocarcinoma classes. According to Table 11 Table 11 Expert Evaluation of Grad-CAM Heatmap Usefulness (Likert Scale, N = 3 Pathologists). Class Mean Score (1–5) Usefulness notes Normal 4.0 Clear and well-distributed boundaries Dysplasia 4.5 Good focus on atypical epithelial zones Adenocarcinoma 4.8 Strong highlighting of abnormal clusters Average 4.43 High agreement on diagnostic relevance Each expert pathologist independently reviewed 30 Grad-CAM overlays and rated their clinical relevance. The average score of 4.43 confirms strong interpretability. Figure 12 Fig. 12 Grad-CAM heatmaps for ( a b c Figure 13 Fig. 13 Model vs Expert region overlaps (Dysplasia). Figure 14 Fig. 14 Grad-CAM Misfocus on Non-Diagnostic Region. Figure 15 Fig. 15 Cumulative Grad-CAM heatmap from 500 test samples. Discussion This study proposed a hybrid CNN–Transformer architecture for gastric cancer classification using histopathological images from three publicly available datasets. The model achieved state-of-the-art results through extensive evaluation, including a 99.2% accuracy on the GasHisSDB dataset, with a macro F1-score of 0.991 and AUC of 0.996. The architecture demonstrated superior performance across multiple evaluation dimensions compared to conventional CNNs and ViTs alone. Explainability was integrated using Grad-CAM, which successfully highlighted class-relevant regions that matched clinically significant histological features. Quantitative interpretability analysis showed high overlap between attention maps and expert-annotated diagnostic zones, with average Dice scores exceeding 0.80. Furthermore, cross-validation and external validation on TCGA-STAD and NCT-CRC-HE datasets confirmed the model's robustness and generalizability. The results suggest that the proposed model has strong potential as a diagnostic support tool for digital pathology. Its high classification accuracy, particularly in distinguishing between normal, dysplastic, and malignant gastric tissue, can help mitigate diagnostic variability and reduce the burden on pathologists in high-throughput clinical environments. The explainability provided by Grad-CAM visualizations allows clinicians to interpret and trust the model's predictions, which is essential for integration into real-world workflows. By reliably highlighting regions of interest, the model can serve as a second reader, flagging suspicious regions for human review and potentially aiding in early detection and treatment planning. Furthermore, the model’s efficient inference time and moderate parameter size make it feasible for deployment in resource-constrained clinical settings or edge devices integrated with digital slide scanners. Despite its strong performance, this study has several limitations. First, while we used three public datasets, real-world histopathology often includes additional staining variations, scanner heterogeneity, and rare tissue subtypes not represented in these datasets. As a result, the model may face challenges in generalizing to unseen institutional data without domain adaptation or fine-tuning. Second, the patch-based approach, while effective, does not consider global slide-level context. Certain diagnostic cues, such as tissue architecture and lesion spread, require whole slide interpretation outside the current model's scope. Third, while Grad-CAM provides valuable insights, it is limited to CNN-based layers. Interpretability for the Transformer component remains an open challenge, as self-attention maps in ViTs are less intuitive and more complex to correlate with histological reasoning. Finally, clinical validation with real-world pathologist workflows and decision-making is required before practical deployment. The current model was evaluated in a controlled experimental setting and assumes accurate ground truth labeling, which may not always hold in complex or ambiguous cases. The proposed hybrid CNN-Transformer model demonstrates strong potential for integration into real-world clinical pathology workflows. It can function as a second-reader system, offering AI-generated predictions and Grad-CAM visualizations that assist pathologists in verifying diagnostically significant regions, thereby enhancing consistency and reducing oversight. The model may also be used as a triage tool, automatically flagging high-risk or diagnostically ambiguous cases, such as suspected dysplasia or adenocarcinoma, for prioritized review. With an inference time of just 42 ms per patch on modern GPUs (e.g., NVIDIA A100), the system is feasible for deployment in high-throughput pathology settings and telepathology platforms. Beyond diagnostics, its explainability features support educational use, aiding in the training of junior pathologists by visually emphasizing critical tissue structures. Furthermore, this approach has the potential to extend to immunohistochemical marker evaluation, supporting future developments in predictive biomarker analysis alongside traditional H&E-based classification. Moreover, future work should explore the integration of multimodal information by combining histopathological imaging with clinical metadata such as HER2 status, tumor grade, or patient demographics. This strategy could enhance diagnostic accuracy and provide more comprehensive insights into disease progression by leveraging complementary data sources 38 39 Conclusion and future work This study introduced a hybrid deep learning architecture that combines convolutional neural networks (CNNs) and transformer encoders to classify gastric cancer in histopathological images. The proposed model demonstrated superior performance by leveraging local feature extraction and global attention mechanisms, achieving 99.2% accuracy, a macro F1-score of 0.991, and an AUC of 0.996 on the GasHisSDB dataset. The model also showed strong generalizability when tested on external datasets, including TCGA-STAD and NCT-CRC-HE-100 K, confirming its robustness across varying data sources and staining protocols. Beyond performance metrics, this work emphasized model interpretability using Grad-CAM, which provided class-specific heatmaps aligned with expert-annotated histopathological features. Such explainability is critical for clinical acceptance, and our evaluations demonstrated both qualitative and quantitative alignment with expert pathologists. Despite these advancements, several limitations remain. Future work will focus on expanding to whole-slide image (WSI) classification to incorporate spatial and architectural information lost in patch-based analysis. We also plan to explore multi-modal integration, combining image data with clinical metadata or molecular profiles (e.g., genomics, HER2 status) to improve diagnostic and prognostic predictions. Another key direction is enhancing interpretability for the transformer component using attention flow analysis or token attribution methods. Lastly, we aim to deploy and validate the proposed pipeline in real-world clinical settings, involving collaboration with hospitals and pathologists for prospective validation and workflow integration. Regulatory, ethical, and usability assessments will be central to ensuring clinical impact. In conclusion, this study presents a powerful, interpretable, and scalable AI tool for gastric cancer classification, offering a foundation for further development toward clinically deployable digital pathology solutions. Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Acknowledgements The authors extend their appreciation to the Taif University, Saudi Arabia, for supporting this work through project number (TU-DSPP-2024-60). Author contributions T.R.: Software, Data curation, Funding acquisition, Validation, Formal analysis, Writing—original draft. X.W.: Methodology, Software, Investigation, Formal analysis, Writing—review & editing. V.G: Formal analysis, Writing—review & editing, Resources. S.B: Formal analysis, Writing—review & editing, Project administration. A.T.: Investigation, Resources, Writing—original draft, Project administration. S.K.: Project administration, Data curation, Funding acquisition, Resources, Writing—review & editing. Funding This work is funded by Taif University, Saudi Arabia, for supporting this work through project number (TU-DSPP-2024–60). Data availability The datasets analyzed in the current study are available from the corresponding author on reasonable request. Declarations Competing interests The authors declare that they have no competing interests. References 1. Bray F Ferlay J Soerjomataram I Siegel RL Torre LA Jemal A Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries CA: Cancer J Clin 2018 68 394 424 30207593 10.3322/caac.21492 Bray, F. et al. Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA: Cancer J Clin 68 30207593 10.3322/caac.21492 2. Wadhwa R Song S Lee J-S Yao Y Wei Q Ajani JA Gastric cancer—molecular and clinical dimensions Nat. Rev. Clin. Oncol. 2013 10 643 655 10.1038/nrclinonc.2013.170 24061039 PMC3927982 Wadhwa, R. et al. Gastric cancer—molecular and clinical dimensions. Nat. Rev. Clin. Oncol. 10 24061039 10.1038/nrclinonc.2013.170 PMC3927982 3. Madabhushi A Lee G Image analysis and machine learning in digital pathology: Challenges and opportunities Med. Image Anal. 2016 33 170 175 10.1016/j.media.2016.06.037 27423409 PMC5556681 Madabhushi, A. & Lee, G. Image analysis and machine learning in digital pathology: Challenges and opportunities. Med. Image Anal. 33 27423409 10.1016/j.media.2016.06.037 PMC5556681 4. K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv:1409.1556 , 5. Esteva A Kuprel B Novoa RA Ko J Swetter SM Blau HM Dermatologist-level classification of skin cancer with deep neural networks Nature 2017 542 115 118 10.1038/nature21056 28117445 PMC8382232 Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542 28117445 10.1038/nature21056 PMC8382232 6. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner , et al. arXiv preprint arXiv:2010.11929 , 7. Litjens G Kooi T Bejnordi BE Setio AAA Ciompi F Ghafoorian M A survey on deep learning in medical image analysis Med. Image Anal. 2017 42 60 88 10.1016/j.media.2017.07.005 28778026 Litjens, G. et al. A survey on deep learning in medical image analysis. Med. Image Anal. 42 28778026 10.1016/j.media.2017.07.005 8. LeCun Y Bengio Y Hinton G Deep learning Nature 2015 521 436 444 10.1038/nature14539 26017442 LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521 26017442 10.1038/nature14539 9. Wang S Yang DM Rong R Zhan X Xiao G Pathology image analysis using segmentation deep learning algorithms Am. J. Pathol. 2019 189 1686 1698 10.1016/j.ajpath.2019.05.007 31199919 PMC6723214 Wang, S., Yang, D. M., Rong, R., Zhan, X. & Xiao, G. Pathology image analysis using segmentation deep learning algorithms. Am. J. Pathol. 189 31199919 10.1016/j.ajpath.2019.05.007 PMC6723214 10. J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang , et al. arXiv preprint arXiv:2102.04306 , 11. Tellez D Litjens G Bándi P Bulten W Bokhorst J-M Ciompi F Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology Med. Image Anal. 2019 58 101544 10.1016/j.media.2019.101544 31466046 Tellez, D. et al. Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology. Med. Image Anal. 58 31466046 10.1016/j.media.2019.101544 12. Bejnordi BE Veta M Van Diest PJ Van Ginneken B Karssemeijer N Litjens G Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer JAMA 2017 318 2199 2210 10.1001/jama.2017.14585 29234806 PMC5820737 Bejnordi, B. E. et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. JAMA 318 29234806 10.1001/jama.2017.14585 PMC5820737 13. Ardila D Kiraly AP Bharadwaj S Choi B Reicher JJ Peng L End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography Nat. Med. 2019 25 954 961 10.1038/s41591-019-0447-x 31110349 Ardila, D. et al. End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. Nat. Med. 25 31110349 10.1038/s41591-019-0447-x 14. Rizzo PC Caputo A Maddalena E Caldonazzi N Girolami I Dei Tos AP Digital pathology world tour Digital Health 2023 9 20552076231194551 10.1177/20552076231194551 37654717 PMC10467307 Rizzo, P. C. et al. Digital pathology world tour. Digital Health 9 37654717 10.1177/20552076231194551 PMC10467307 15. Marletta S Eccher A Martelli FM Santonicco N Girolami I Scarpa A Artificial intelligence–based algorithms for the diagnosis of prostate cancer: A systematic review Am. J. Clin. Pathol. 2024 161 526 534 10.1093/ajcp/aqad182 38381582 Marletta, S. et al. Artificial intelligence–based algorithms for the diagnosis of prostate cancer: A systematic review. Am. J. Clin. Pathol. 161 38381582 10.1093/ajcp/aqad182 16. Frascarelli C Bonizzi G Musico CR Mane E Cassi C Guerini Rocco E Revolutionizing cancer research: The impact of artificial intelligence in digital biobanking J. Personal. Med. 2023 13 1390 10.3390/jpm13091390 PMC10532470 37763157 Frascarelli, C. et al. Revolutionizing cancer research: The impact of artificial intelligence in digital biobanking. J. Personal. Med. 13 10.3390/jpm13091390 PMC10532470 37763157 17. Koteluk O Wartecki A Mazurek S Kołodziejczak I Mackiewicz A How do machines learn? Artificial intelligence as a new era in medicine J. Personal. Med. 2021 11 32 10.3390/jpm11010032 PMC7825660 33430240 Koteluk, O., Wartecki, A., Mazurek, S., Kołodziejczak, I. & Mackiewicz, A. How do machines learn? Artificial intelligence as a new era in medicine. J. Personal. Med. 11 10.3390/jpm11010032 PMC7825660 33430240 18. D. Ciresan, A. Giusti, L. Gambardella, and J. Schmidhuber, \"Deep neural networks segment neuronal membranes in electron microscopy images. Advances in neural information processing systems 19. Komura D Ishikawa S Machine learning methods for histopathological image analysis Comput. Struct. Biotechnol. J. 2018 16 34 42 10.1016/j.csbj.2018.01.001 30275936 PMC6158771 Komura, D. & Ishikawa, S. Machine learning methods for histopathological image analysis. Comput. Struct. Biotechnol. J. 16 30275936 10.1016/j.csbj.2018.01.001 PMC6158771 20. D. C. Cireşan, A. Giusti, L. M. Gambardella, and J. Schmidhuber, \"Mitosis detection in breast cancer histology images with deep neural networks,\" in Medical Image Computing and Computer-Assisted Intervention–MICCAI 2013: 16th International Conference, Nagoya, Japan, September 22–26, 2013, Proceedings, Part II 16 10.1007/978-3-642-40763-5_51 24579167 21. G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \"Densely connected convolutional networks,\" In Proceedings of the IEEE conference on computer vision and pattern recognition 22. Vahadane A Peng T Sethi A Albarqouni S Wang L Baust M Structure-preserving color normalization and sparse stain separation for histological images IEEE Trans. Med. Imaging 2016 35 1962 1971 10.1109/TMI.2016.2529665 27164577 Vahadane, A. et al. Structure-preserving color normalization and sparse stain separation for histological images. IEEE Trans. Med. Imaging 35 27164577 10.1109/TMI.2016.2529665 23. R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, \"Grad-cam: Visual explanations from deep networks via gradient-based localization,\" In Proceedings of the IEEE International Conference on Computer Vision 24. Wintzingerode FV Göbel UB Stackebrandt E Determination of microbial diversity in environmental samples: Pitfalls of PCR-based rRNA analysis FEMS Microbiol. Rev. 1997 21 213 229 10.1111/j.1574-6976.1997.tb00351.x 9451814 Wintzingerode, F. V., Göbel, U. B. & Stackebrandt, E. Determination of microbial diversity in environmental samples: Pitfalls of PCR-based rRNA analysis. FEMS Microbiol. Rev. 21 9451814 10.1111/j.1574-6976.1997.tb00351.x 25. O. N. Manzari, J. M. Kaleybar, H. Saadat, and S. Maleki, \"BEFUnet: A hybrid CNN-transformer architecture for precise medical image segmentation,\" arXiv preprint arXiv:2402.08793 , 26. Li J Chen J Tang Y Wang C Landman BA Zhou SK Transforming medical imaging with transformers? A comparative review of key properties, current progress, and future perspectives Med. Image Anal. 2023 85 102762 10.1016/j.media.2023.102762 36738650 PMC10010286 Li, J. et al. Transforming medical imaging with transformers? A comparative review of key properties, current progress, and future perspectives. Med. Image Anal. 85 36738650 10.1016/j.media.2023.102762 PMC10010286 27. Xu Y Jia Z Wang L-B Ai Y Zhang F Lai M Large-scale tissue histopathology image classification, segmentation, and visualization via deep convolutional activation features BMC Bioinform. 2017 18 1 17 10.1186/s12859-017-1685-x PMC5446756 28549410 Xu, Y. et al. Large-scale tissue histopathology image classification, segmentation, and visualization via deep convolutional activation features. BMC Bioinform. 18 10.1186/s12859-017-1685-x PMC5446756 28549410 28. Chen Y Sun Z Chen W Liu C Chai R Ding J The immune subtypes and landscape of gastric cancer and to predict based on the whole-slide images using deep learning Front. Immunol. 2021 12 685992 10.3389/fimmu.2021.685992 34262565 PMC8273735 Chen, Y. et al. The immune subtypes and landscape of gastric cancer and to predict based on the whole-slide images using deep learning. Front. Immunol. 12 34262565 10.3389/fimmu.2021.685992 PMC8273735 29. Jang W Lee J Park KH Kim A Lee SH Ahn S Molecular classification of breast cancer using weakly supervised learning Cancer Res. Treat.: Official J. Korean Cancer Assoc. 2024 57 116 10.4143/crt.2024.113 PMC11729310 38938010 Jang, W. et al. Molecular classification of breast cancer using weakly supervised learning. Cancer Res. Treat.: Official J. Korean Cancer Assoc. 57 10.4143/crt.2024.113 PMC11729310 38938010 30. Cho H Moon D Heo SM Chu J Bae H Choi S Artificial intelligence-based real-time histopathology of gastric cancer using confocal laser endomicroscopy NPJ Precision Oncol. 2024 8 131 10.1038/s41698-024-00621-x PMC11178780 38877301 Cho, H. et al. Artificial intelligence-based real-time histopathology of gastric cancer using confocal laser endomicroscopy. NPJ Precision Oncol. 8 10.1038/s41698-024-00621-x PMC11178780 38877301 31. Li R Li J Wang Y Liu X Xu W Sun R The artificial intelligence revolution in gastric cancer management: Clinical applications Cancer Cell Int. 2025 25 111 10.1186/s12935-025-03756-4 40119433 PMC11929235 Li, R. et al. The artificial intelligence revolution in gastric cancer management: Clinical applications. Cancer Cell Int. 25 40119433 10.1186/s12935-025-03756-4 PMC11929235 32. Maity R Sankari VR Snekhalatha U Rajesh N Salvador AL Explainable AI-based automated segmentation and multi-stage classification of gastroesophageal reflux using machine learning techniques Biomed. Phys. Eng. Express 2024 10 045058 10.1088/2057-1976/ad5a14 38901416 Maity, R., Sankari, V. R., Snekhalatha, U., Rajesh, N. & Salvador, A. L. Explainable AI-based automated segmentation and multi-stage classification of gastroesophageal reflux using machine learning techniques. Biomed. Phys. Eng. Express 10 10.1088/2057-1976/ad5a14 38901416 33. Mohammed FA Tune KK Assefa BG Jett M Muhie S Medical image classifications using convolutional neural networks: a survey of current methods and statistical modeling of the literature Mach. Learn. and Knowledge Extract. 2024 6 699 735 10.3390/make6010033 Mohammed, F. A., Tune, K. K., Assefa, B. G., Jett, M. & Muhie, S. Medical image classifications using convolutional neural networks: a survey of current methods and statistical modeling of the literature. Mach. Learn. and Knowledge Extract. 6 34. J. Wu, Y. Yu, C. Huang, and K. Yu, \"Deep multiple instance learning for image classification and auto-annotation,\" In Proceedings of the IEEE conference on computer vision and pattern recognition 35. Divate M Tyagi A Richard DJ Prasad PA Gowda H Nagaraj SH Deep learning-based pan-cancer classification model reveals tissue-of-origin specific gene expression signatures Cancers 2022 14 1185 10.3390/cancers14051185 35267493 PMC8909043 Divate, M. et al. Deep learning-based pan-cancer classification model reveals tissue-of-origin specific gene expression signatures. Cancers 14 35267493 10.3390/cancers14051185 PMC8909043 36. \"Gastric Histopathology Sub-size Image Database (GasHisSDB),\" G. P. P. s. Hospital, Ed., ed, 2021. 37. \"The Cancer Genome Atlas (TCGA) Stomach Adenocarcinoma (TCGA-STAD),\" N. C. I. G. D. Portal, Ed., ed, 2021. 38. J. N. Kather, N. Halama, and A. Marx, \"NCT-CRC-HE-100K,\" Zenodo, Ed., ed, 2018. 39. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" In Proceedings of the IEEE conference on computer vision and pattern recognition 40. M. Tan and Q. Le, \"Efficientnet: Rethinking model scaling for convolutional neural networks,\" In International Conference on machine learning 41. Tsakonas G Martín-Bernabé A Rounis K Moreno-Ruiz P Botling J De Petris L High density of NRF2 expression in malignant cells is associated with increased risk of CNS Metastasis in early-stage NSCLC Cancers 2021 13 3151 10.3390/cancers13133151 34202448 PMC8268817 Tsakonas, G. et al. High density of NRF2 expression in malignant cells is associated with increased risk of CNS Metastasis in early-stage NSCLC. Cancers 13 34202448 10.3390/cancers13133151 PMC8268817 ",
  "metadata": {
    "Title of this paper": "High density of NRF2 expression in malignant cells is associated with increased risk of CNS Metastasis in early-stage NSCLC",
    "Journal it was published in:": "Scientific Reports",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12489133/"
  }
}
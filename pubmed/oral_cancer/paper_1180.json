{
  "title": "Paper_1180",
  "abstract": "pmc Biomimetics (Basel) Biomimetics (Basel) 3613 biomim biomimetics Biomimetics 2313-7673 Multidisciplinary Digital Publishing Institute  (MDPI) PMC12467273 PMC12467273.1 12467273 12467273 41002798 10.3390/biomimetics10090564 biomimetics-10-00564 1 Article IEGS-BoT: An Integrated Detection-Tracking Framework for Cellular Dynamics Analysis in Medical Imaging https://orcid.org/0000-0001-5844-2360 Tu Shuqin Methodology Investigation Writing – original draft 1 Chen Weidian Methodology Writing – original draft Writing – review & editing 1 Mao Liang Investigation Writing – review & editing Supervision Funding acquisition 2 * Zhang Quan Validation Writing – review & editing 3 Yuan Fang Validation Visualization 4 Du Jiaying Data curation Writing – review & editing Visualization 1 Jia Heming Academic Editor 1 2 3 4 * maoliangscau@szpu.edu.cn 24 8 2025 9 2025 10 9 497616 564 05 7 2025 13 8 2025 22 8 2025 24 08 2025 27 09 2025 27 09 2025 © 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ Cell detection-tracking tasks are vital for biomedical image analysis with potential applications in clinical diagnosis and treatment. However, it poses challenges such as ambiguous boundaries and complex backgrounds in microscopic video sequences, leading to missed detection, false detection, and loss of tracking. Therefore, we propose an enhanced multiple object tracking algorithm IEGS-YOLO + BoT-SORT, named IEGS-BoT, to address these issues. Firstly, the IEGS-YOLO detector is developed for cell detection tasks. It uses the iEMA module, which effectively combines the global information to enhance the local information. Then, we replace the traditional convolutional network in the neck of the YOLO11n with GSConv to reduce the computational complexity while maintaining accuracy. Finally, the BoT-SORT tracker is selected to enhance the accuracy of bounding box positioning through camera motion compensation and Kalman filter. We conduct experiments on the CTMC dataset, and the results show that in the detection phase, the map50 (mean Average Precision) and map50–95 values are 73.2% and 32.6%, outperforming the YOLO11n detector by 1.1% and 0.6%, respectively. In the tracking phase, using the IEGS-BoT method, the multiple objects tracking accuracy (MOTA), higher order tracking accuracy (HOTA), and identification F1 (IDF1) reach 53.97%, 51.30%, and 67.52%, respectively. Compared with the base BoT-SORT, the proposed method achieves improvements of 1.19%, 0.23%, and 1.29% in MOTA, HOTA, and IDF1, respectively. ID switch (IDSW) decreases from 1170 to 894, which demonstrates significant mitigation of identity confusion. This approach effectively addresses the challenges posed by object loss and identity switching in cell tracking, providing a more reliable solution for medical image analysis. multiple object tracking (MOT) YOLO11n IEGS-YOLO cell tracking BoT-SORT Shenzhen Polytechnic University Smart Agriculture Innovation Application R&D Center 602431001PQ Huizhou Municipal Key Areas Research and Development Project 2024BQ010007 National Natural Science Foundation of China 62272320 Shenzhen Science and Technology Innovation Commission Foundation 20220812222043002 Shenzhen Polytechnic University Research Fund 6025310045K Guangxi Science and Technology Major Program GuikeAA22117013 The work was supported by Shenzhen Polytechnic University Smart Agriculture Innovation Application R&D Center (No. 602431001PQ), Huizhou Municipal Key Areas Research and Development Project (No. 2024BQ010007), National Natural Science Foundation of China (No. 62272320), Shenzhen Science and Technology Innovation Commission Foundation (No. 20220812222043002), Shenzhen Polytechnic University Research Fund (No. 6025310045K), and Guangxi Science and Technology Major Program (GuikeAA22117013). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Cell detection and tracking task plays a crucial role in medical image analysis, enabling dynamic monitoring of cell behaviors such as proliferation, migration, and division [ 1 2 3 4 5 In recent years, the development of deep learning has provided a new way to overcome the above bottleneck [ 6 7 8 9 10 11 12 13 14 15 16 17 As an extension of the detection pipeline, cell tracking requires advanced processing of temporal and spatial dependencies in image sequences. This involves not only the identification of individual cells in single frames but also the establishment of identity correspondence across frames. Scherr et al. [ 18 19 20 21 22 23 In real scenarios, intense motion of many cells and mutual occlusion often occur, and this situation is similar to the complex scenarios handled by multiple object tracking (MOT) technology. MOT is typically divided into two primary technical approaches: tracking-by-detection (TBD) and joint detection and tracking (JDT). The TBD algorithm is divided into two phases: detection and tracking [ 24 25 In recent years, some researchers have begun to introduce TBD technology in the cell tracking field. For example, Anjum and Gurari [ 26 27 Inspired by the above studies, this study introduces the TBD framework to the field of cell tracking. We propose an improved MOT method named IEGS-BoT to detect and track cell movement under complex backgrounds. First, to solve the problems of missed detection, false detection, and loss of tracking due to blurred boundaries, IEGS-BoT adopts an enhanced iEMA module. It is constructed by the efficient multi-scale attention (EMA) module [ 28 29 30 31 The main contributions of this study are as follows: (1) We propose an enhanced MOT algorithm IEGS-YOLO + BoT-SORT, named IEGS-BoT, to detect and track cell movement for microscopic video sequences. (2) To tackle the challenges of detecting small cells under complex backgrounds, we propose IEGS-YOLO detector, an enhanced cell detection network based on YOLO11n. (3) The IEGS-YOLO integrates the iEMA module to increase model stability. Meanwhile, it uses a lightweight and efficient convolutional structure named GSConv module to reduce model parameters and computational complexity. (4) We evaluate the performance of several mainstream trackers on a public dataset. BoT-SORT is ultimately selected and combined with the IEGS-YOLO detector for cell tracking and trajectories analysis. 2. Methods In this study, we propose a cell-tracking IEGS-BoT algorithm based on IEGS-YOLO detector and BoT-SORT tracker. The structure of IEGS-BoT is shown in Figure 1 2.1. IEGS-YOLO Detector We develop IEGS-YOLO based on YOLO11n, which is the smallest variant of the YOLO11 series for its fewer parameters, lower computational cost, and small model size. The YOLO11 architecture consists of four primary components: the input layer, Backbone, Neck, and Head. Initially, the input layer receives the raw image data and conducts preliminary processing. Its key role is to convert the data into a format compatible with the network, including resizing and normalizing the image for feature extraction and object detection through subsequent layers. Next, the Backbone network extracts the features such as edges, texture, and shape from the input image. Subsequently, the Neck network receives the feature maps obtained from the Backbone, facilitating feature fusion and enhancement. Neck enhances the model’s multi-scale detection capability and accuracy. The detection Head derives the final detection result, including the bounding box, category, and confidence. The IEGS-YOLO architecture shown in Figure 2 Figure 2 GSConv is used in the feature fusion branches corresponding to P4 and P5 feature maps. It replaces traditional convolution layers in the neck of Figure 2 2.2. IEMA In this section, we propose the IEMA module, a novel architecture developed by integrating the inverted residual connection concept from iRMB into the EMA module, and its flowchart is shown in Figure 3 Initially, the input data is processed by the BatchNorm2d layer to ensure its normalization. Then, the data is directed to the EMA module, which serves to strengthen the global dependence on the features and enhance the ability to capture local details through multi-scale feature integration and cross-space learning mechanisms. Subsequently, spatial features are extracted using DWConv, while residual concatenation is employed to sum the EMA processed features with the deep convolution processed features. The summed features then flow into the second convolutional layer. Finally, the output of the second convolutional layer is fused with the original input by residual concatenation to output the final processed features. The EMA and iRMB are key modules of the IEMA module. These two submodules are described in Section 2.2.1 Section 2.2.2 2.2.1. EMA EMA is an efficient multi-scale attention module without dimensionality reduction, which utilizes a channel grouping strategy to reconstruct part of the channel dimensions into batch dimensions, avoiding the information loss of the traditional dimensionality reduction, and its structure is shown in Figure 4 The EMA module is divided into three distinct branches, two of which use a 1 × 1 convolutional branch, and the third uses a 3 × 3 convolutional branch. First, the input feature maps are divided into g groups, and each group of features is pooled by horizontal and vertical global average pooling in a 1 × 1 branch, respectively, to generate compressed features and concatenate them along the channel dimension. Then, the features are fused by a 1 × 1 convolutional branch, and the sigmoid function is applied to generate new channel attention maps, and two-channel attention maps within each group are aggregated by simple multiplication. In the 3 × 3 branch, 3 × 3 convolution is employed to capture local spatial context information to expand the feature space. In addition, cross-dimensional interactions facilitate the aggregation of the outputs of the two parallel branches, capturing pairwise pixel relationships at the pixel level. The final output of the EMA is the same size as the input and can be efficiently incorporated into modern architectures. 2.2.2. iRMB The iRMB is used to integrate a lightweight convolutional neural network architecture with an attention-based model. By rethinking the inverted residual block (IRB) in MobileNetv2 and two effective modules of transformer—multi-head self-attention (MHSA) and feedforward neural network (FFN), the meta mobile block (MMB) is proposed. The Inverted Residual Mobile Block (iRMB) is instantiated based on the MMB, and its structure is shown in Figure 5 Initially, the low-dimensional features are expanded into a higher-dimensional space using a 1 × 1 convolution. Then, depthwise separable convolution extracts spatial features, and finally, another 1 × 1 convolution compresses the features back to a low-dimensional representation. Additionally, the self-attention mechanism is employed to model global dependencies. The MMB is abstracted as a one-residual structure with configurable parameters (expansion ratio and efficient operator) to instantiate different modules. In MMB, the efficient operator F ( ⋅ ) (1) F ( ⋅ ) = C o n v ( M H S A ( ⋅ ) ) However, this implementation may incur a substantial computational overhead. To address this challenge, the expanded window multi-head self-attention (EW-MHSA) and deep convolution with jump connections (DW-Conv) are employed in iRMB to minimize the computational cost of the model. Accordingly, the efficient operator F ( ⋅ ) (2) F ( ⋅ ) = ( D W − C o n v , S k i p ) ( E W − M H S A ( ⋅ ) ) Existing Transformer-based models generally implement module combinations through a layered design. In contrast, iRMB requires only two switches to enable or disable MHSA/ D W − C o n v 2.3. GSConv In this study, group shuffle convolution (GSConv), a lightweight convolution, is introduced as a replacement for the traditional convolution in neck networks. GSConv overcomes the disadvantage of channel information separation during deep convolution computation and effectively combines the advantages of standard convolution (SC) and depth-wise separable convolution (DSC) to reduce the computational cost while maintaining sufficient accuracy. Specifically, GSConv performs a uniform exchange of local feature information among different channels by introducing the shuffle operation on the high-quality features generated by SC and DSC. The computational complexity formulas for SC and GSConv are Equation (3) and Equation (4), respectively. (3) T S C = W × H × K 1 × K 2 × C 1 × C 2 (4) T G S C o n v = W × H × K 1 × K 2 × C 2 2 × ( C 1 + 1 ) W H K 1 × K 2 C 1 C 2 As shown in Figure 6 2.4. BoT-SORT The BoT-SORT algorithm retains the low-scoring detection frames with two rounds of matching while ensuring the tracking of the high-scoring detection frames. BoT-SORT mainly consists of the improved Kalman filter (KF), data association, and tracklet management, and its structure is shown in Figure 7 The improved Kalman filter is first used to predict the cell motion trajectory, which mainly predicts the state of the current frame based on the state of the previous frame of the tracking trajectory, as shown in Equations (5) and (6): (5) x ^ k | k − 1 = F k x ^ k − 1 | k − 1 (6) P k | k − 1 = F k P k − 1 | k − 1 F k T + Q k x ^ k | k − 1 k x ^ k − 1 | k − 1 k F k P k | k − 1 k P k − 1 | k − 1 k Q k The data association part utilizes a hierarchical matching strategy to realize the precise association between detection boxes and tracklets. It is categorized into high and low score detection boxes based on the detection score and also includes a collection of trajectories from the previous frame. They will be used for two rounds of data association. The first association is performed between the high-score detection boxes D h i g h T r e m a i n D l o w T k (7) K t = P ^ t H T ( H P ^ t H T + R ) − 1 (8) x t = x ^ t + K t ( Z t − H x ^ t ) (9) P t = P ^ t − K t H P ^ t K t H R x t t Z t P t t Based on the matching results, the tracklet management module applies a differentiated processing strategy. For the first association’s mismatched detection boxes D r e m a i n D r e − r e m a i n T r e − r e m a i n T l o s t 3. Experiments This section will present the experimental details, the experimental results of IEGS-YOLO + BoT-SORT (IEGS-BoT), and the comparison with other models. 3.1. Dataset This study utilized the CTMC dataset, collected from Nikon Microscopy’s website. The dataset consists of 47 videos with 80,389 frames. These videos have a resolution of 320 × 400 and a frame rate of 30 frames per second. The dataset contains 14 different cell lines, which are described in Table 1 In addition, to assess the robustness of our approach, we also employed an additional publicly available cell detection dataset [ 32 The data in the study were obtained from publicly available datasets. The results were generated using PyTorch 1.12 and official evaluation scripts from the benchmark datasets. 3.2. Evaluation Metrics and Implementation Details In this study, we utilize standard evaluation metrics in object detection: precision (P), recall (R), F-Measure (F1), mean average precision (mAP), parameters (Parms), model size (size), and Giga Floating Point Operations (GFLOPs) for evaluating detector performance. Precision (P), Recall (R), F1, and mAP indicate the detector’s overall performance. Params focus on quantifying the model’s complexity, with all reported values representing the absolute number of trainable parameters. Model size focuses on the detector’s efficiency, and GFLOPs reflect the computational complexity of the model. The multi-object tracking part uses higher order tracking accuracy (HOTA), MOT accuracy (MOTA), identification F1 (IDF1), and ID switch (IDSW) to evaluate the tracking method. Among them, MOTA is used to evaluate the overall performance of the multi-object tracking algorithm, HOTA is a comprehensive metric that measures both detection and association accuracy, IDF1 reflects the performance of the tracking algorithm in recognizing the identity of the object, while IDSW is the number of ID switching times in the tracking process, which indicates the stability of ID matching. For the detection experiment, the parameters are set as follows: epoch is 60, the batch size is 16, and the learning rate is 0.01. For the tracking experiment, the parameters are set as follows: the confidence thresholds for the first and second associations are 0.5 and 0.1, the confidence threshold for initializing new trajectories is 0.6, and the threshold for association matching is 0.8. 3.3. Ablation Experiments with IEGS-YOLO Before conducting the ablation study, we performed a series of hyperparameter tuning experiments for the YOLO11n detector to ensure a strong baseline. As shown in Table 2 To validate the effectiveness of each module under the selected hyperparameters, we conduct the following ablation study. The ablation study for YOLO11n with and without IEMA and GSConv is presented in Table 3 Table 3 Overall, the GSConv module maintains excellent performance while reducing model complexity, while the IEMA mechanism is more appropriate for scenarios where high recall and detection robustness are required. When both GSConv and IEMA are incorporated, the mAP50 increases to 73.2% and mAP50–95 reaches 32.6%, nearly matching the accuracy achieved by introducing the IEMA module alone. Meanwhile, the parameters, model size, and computational cost remain close to those of using GSConv individually. This fused architecture consequently emerges as an optimal solution that balances both accuracy and efficiency. Figure 8 Figure 8 To verify the generalization of our method, we selected another publicly available cell image detection dataset to conduct a supplementary evaluation of detection performance, comparing YOLO11n with our proposed IEGS-YOLO model. As shown in Table 4 3.3.1. Comparison of the Performance of Different Attention Mechanisms Table 5 33 34 3.3.2. Comparison of the Performance of Different Convolutional Modules We also comprehensively investigated the effect of different convolutional modules on model performance, as shown in Table 6 35 36 Overall, the MAB module remains inferior to the YOLO11n both in terms of detection performance and resource consumption, and the PConv is not as good as the YOLO11n, although it has reduced resource consumption. The GSConv module improves the detection performance and reduces the model complexity at the same time, which is the best choice for the balance between efficiency and accuracy. 3.4. Comparison of the Performance of Different Detection Algorithms Table 7 37 We can observe that the detectors differ in accuracy and complexity. IEGS-YOLO achieves the best performance in precision, recall, F1 score, and mAP50, while it is slightly inferior to YOLO11s and YOLO11m in mAP50–95. We acknowledge that both YOLO11m and YOLO11s achieve slightly higher mAP@0.50–0.95 than IEGS-YOLO, but it is important to note that both of these models exhibit larger model sizes, higher parameter counts, and greater computational costs compared to IEGS-YOLO. Our work aims to balance detection accuracy and efficiency, especially under resource-constrained conditions such as real-time clinical applications. Based on all metrics, we find that IEGS-YOLO performs better than YOLO11m and YOLO11s overall, particularly when considering factors such as precision, recall, and F1 score, while maintaining a significantly lower computational cost. Compared to other lightweight versions of YOLO, YOLO11n outperforms YOLOv5n, YOLOv8n, YOLOv9t, and YOLOv10n [ 38 3.5. Comparison of the Performance of Different Tracking Algorithms Table 8 39 40 41 42 43 44 Figure 9 To further compare the overall tracking performance among different trackers, Figure 10 The ground truth (GT) of actual cell trajectories is obtained through frame-by-frame manual annotations in the video sequences. Each annotation includes the frame, ID, and the bounding box coordinates (left, top, width, height). The cell trajectories are defined as sequences of bounding boxes sharing the same ID across consecutive frames. These annotations are derived from a public dataset and follow the standardized MOTChallenge format, ensuring high accuracy and consistency for reliable performance evaluation. Compared with BoT-SORT, our method demonstrates improvements in trajectory overlap with the ground truth, with particularly significant improvements in identity preservation, reflected in fewer ID switches and more continuous and stable trajectories. These results suggest that the combination of IEGS-YOLO and BoT-SORT offers stronger identity preservation in complex cellular motion scenarios. These results further validate the effectiveness of this method for cell tracking tasks. 4. Discussion In this section, we discuss the limitations of our approach, highlighting the key challenges and providing insights into areas that can be improved in future work. Although our method demonstrates strong performance in cell tracking, it faces limitations when dealing with certain cell types that are difficult to detect accurately, such as APM cells. In particular, the APM sequence presents a challenging case due to significant shape deformation across frames, despite containing relatively few cells. Compared with other cell types, APM cells are more likely to be confused with the background. This leads to missed detections, false detections, and identity switches. We compare the GT of actual cell trajectories with the trajectories generated by the baseline method and our improved method on the APM sequence. As shown in Figure 11 We believe this example effectively illustrates the practical challenges our model faces and provides meaningful guidance for future improvements. Addressing such difficult cases will be an important direction in our future research. 5. Conclusions In this study, we proposed an improved MOT method, IEGS-BoT (IEGS-YOLO + BoT-SORT). The approach aimed to solve the problems of missed detection, false detection, and identity switching in the microscopic scenario. These issues are caused by challenging factors such as ambiguous boundaries, background interference, high cell morphology similarity, and irregular division. To achieve the best object detection performance, we conducted comparative analyses of detectors like YOLOv5, YOLOv8, YOLOv9, YOLOv10, and YOLO11, ultimately selecting YOLO11n as the baseline, based on which we proposed IEGS-YOLO. Firstly, we introduced an innovative iEMA module in the small object detection layer to enhance the model’s accuracy in detecting small targets. Secondly, we replaced the traditional convolution in the neck network with GSConv, a module that reduced computational complexity while maintaining accuracy. The proposed detector obtained a precision of 77.0%, recall of 69.6%, F1 score of 73.1%, mAP@0.50 of 73.2%, mAP@0.50–0.95 of 32.6% on the CTMC dataset, and the model size was only 5.3 MB. In cell tracking, we compared tracking algorithms including TransTrack, DeepSORT, ImprAsso, OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT. The results demonstrated that BoT-SORT achieved the best performance. When integrated with the improved detector, it achieved 51.30% of HOTA, 53.97% of MOTA, and 67.52% of IDF1, respectively. In addition, the IDSW was only 894, ensuring more reliable cell tracking. Disclaimer/Publisher’s Note: Author Contributions Data curation, J.D.; Funding acquisition, L.M.; Investigation, S.T. and L.M.; Methodology, S.T. and W.C.; Supervision, L.M.; Validation, Q.Z. and F.Y.; Visualization, F.Y. and J.D.; Writing—original draft, S.T. and W.C.; Writing—review and editing, W.C., L.M., Q.Z. and J.D. All authors have read and agreed to the published version of the manuscript. Data Availability Statement The raw data supporting the conclusions of this article will be made available by the authors on request. Conflicts of Interest The authors declare no conflicts of interest. References 1. Maska M. Ulman V. Delgado-Rodriguez P. Gomez-de-Mariscal E. Necasova T. Guerrero Pena F.A. Ren T.I. Meyerowitz E.M. Scherr T. Loffler K. The Cell Tracking Challenge: 10 years of objective benchmarking Nat. Methods 2023 20 1010 1020 10.1038/s41592-023-01879-y 37202537 PMC10333123 2. Emami N. Sedaei Z. Ferdousi R. Computerized cell tracking: Current methods, tools and challenges Vis. Inform. 2021 5 1 13 10.1016/j.visinf.2020.11.003 3. Majumdar R. Steen K. Coulombe P.A. Parent C.A. Non-canonical processes that shape the cell migration landscape Curr. Opin. Cell Biol. 2019 57 123 134 10.1016/j.ceb.2018.12.013 30852463 PMC7087401 4. Tariq M. Iqbal S. Ayesha H. Abbas I. Ahmad K.T. Niazi M.F.K. Medical image based breast cancer diagnosis: State of the art and future directions Expert Syst. Appl. 2021 167 114095 10.1016/j.eswa.2020.114095 5. Zheng D. He X. Jing J. Overview of Artificial Intelligence in Breast Cancer Medical Imaging J. Clin. Med. 2023 12 419 10.3390/jcm12020419 36675348 PMC9864608 6. Falk T. Mai D. Bensch R. Cicek O. Abdulkadir A. Marrakchi Y. Bohm A. Deubner J. Jackel Z. Seiwald K. U-Net: Deep learning for cell counting, detection, and morphometry Nat. Methods 2019 16 67 70 10.1038/s41592-018-0261-2 30559429 7. Isensee F. Jaeger P.F. Kohl S.A.A. Petersen J. Maier-Hein K.H. nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation Nat. Methods 2021 18 203 211 10.1038/s41592-020-01008-z 33288961 8. Chen T. Zheng W. Ying H. Tan X. Li K. Li X. Chen D.Z. Wu J. A Task Decomposing and Cell Comparing Method for Cervical Lesion Cell Detection IEEE Trans. Med. Imaging 2022 41 2432 2442 10.1109/TMI.2022.3163171 35349436 9. Guo Y. Zhang M. Blood Cell Detection Method Based on Improved, YOLO.v5 IEEE Access 2023 11 67987 67995 10.1109/ACCESS.2023.3290905 10. Ikeda K. Sakabe N. Maruyama S. Ito C. Shimoyama Y. Oboshi W. Komene T. Yamaguchi Y. Sato S. Nagata K. Relationship between a deep learning model and liquid-based cytological processing techniques Cytopathology 2023 34 308 317 10.1111/cyt.13235 37051774 11. Xie F. Zhang F. Xu S. DB-FCN: An end-to-end dual-branch fully convolutional nucleus detection model Expert Syst. Appl. 2024 257 125139 10.1016/j.eswa.2024.125139 12. Sazak H. Kotan M. Automated Blood Cell Detection and Classification in Microscopic Images Using YOLOv11 and Optimized Weights Diagnostics 2024 15 22 10.3390/diagnostics15010022 39795550 PMC11719705 13. Chang Y.J. Wu Y.D. Liao Y.L. Wang C.H. Wu Y.C. Modified YOLO network model for metaphase cell detection in antinuclear antibody images Eng. Appl. Artif. Intell. 2024 127 107392 10.1016/j.engappai.2023.107392 14. Chen X. Zheng H. Tang H. Li F. Multi-scale perceptual YOLO for automatic detection of clue cells and trichomonas in fluorescence microscopic images Comput. Biol. Med. 2024 175 108500 10.1016/j.compbiomed.2024.108500 38678942 15. Tarimo S.A. Jang M.A. Ngasa E.E. Shin H.B. Shin H. Woo J. WBC YOLO-ViT: 2 Way-2 stage white blood cell detection and classification with a combination of YOLOv5 and vision transformer Comput. Biol. Med. 2024 169 107875 10.1016/j.compbiomed.2023.107875 38154163 16. Haq I. Mazhar T. Asif R.N. Ghadi Y.Y. Ullah N. Khan M.A. Al-Rasheed A. YOLO and residual network for colorectal cancer cell detection and counting Heliyon 2024 10 e24403 10.1016/j.heliyon.2024.e24403 38304780 PMC10831604 17. Liao W. He Y. Jiang B. Zhao J. Gao M. Zhang X. HistoMoCo: Momentum Contrastive Learning Pre-Training on Unlabeled Histopathological Images for Oral Squamous Cell Carcinoma Detection Electronics 2025 14 1252 10.3390/electronics14071252 18. Scherr T. Loffler K. Bohland M. Mikut R. Cell segmentation and tracking using CNN-based distance predictions and a graph-based matching strategy PLoS ONE 2020 15 e0243219 10.1371/journal.pone.0243219 33290432 PMC7723299 19. Sixta T. Cao J. Seebach J. Schnittler H. Flach B. Coupling cell detection and tracking by temporal feedback Mach. Vis. Appl. 2020 31 24 10.1007/s00138-020-01072-7 20. Wang J. Su X. Zhao L. Zhang J. Deep Reinforcement Learning for Data Association in Cell Tracking Front. Bioeng. Biotechnol. 2020 8 298 10.3389/fbioe.2020.00298 32328484 PMC7161216 21. Panteli A. Gupta D.K. Bruijn N. Gavves E. Siamese Tracking of Cell Behaviour Patterns Proceedings of the Third Conference on Medical Imaging with Deep Learning Vienna, Austria 13–18 July 2020 Tal A. Ismail Ben A. Marleen de B. Maxime D. Herve L. Christopher P. Volume 121 570 587 22. Chen Y. Song Y. Zhang C. Zhang F. O’Donnell L. Chrzanowski W. Cai W. Celltrack R-CNN: A Novel End-to-End Deep Neural Network for Cell Segmentation and Tracking in Microscopy Images Proceedings of the 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) Nice, France 13–16 April 2021 779 782 23. Ben-Haim T. Raviv T.R. Graph Neural Network for Cell Tracking in Microscopy Videos Proceedings of the Computer Vision—ECCV 2022: 17th European Conference Tel Aviv, Israel 23–27 October 2022 Springer Berlin, Germany 2022 610 626 Part XXI 24. Wu D. Huang Z. Zhang Y. SolidTrack: A Novel Method for Robust and Reliable Multi-Pedestrian Tracking Electronics 2025 14 1370 10.3390/electronics14071370 25. Zeng F. Dong B. Zhang Y. Wang T. Zhang X. Wei Y. MOTR: End-to-End Multiple-Object Tracking with Transformer Proceedings of the Computer Vision—ECCV 2022: 17th European Conference Tel Aviv, Israel 23–27 October 2022 Springer Berlin, Germany 2022 659 675 Part XXVII 26. Anjum S. Gurari D. CTMC: Cell Tracking with Mitosis Detection Dataset Challenge Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops Vancouver, BC, Canada 17–24 June 2023 27. Toubal I.E. Al-Shakarji N. Cornelison D.D.W. Palaniappan K. Ensemble Deep Learning Object Detection Fusion for Cell Tracking, Mitosis, and Lineage IEEE Open J. Eng. Med. Biol. 2024 5 443 458 10.1109/OJEMB.2023.3288470 39906165 PMC11793856 28. Ouyang D. He S. Zhang G. Luo M. Guo H. Zhan J. Huang Z. Efficient multi-scale attention module with cross-spatial learning Proceedings of the ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) Rhodes Island, Greece 4–9 June 2023 1 5 29. Zhang J. Li X. Li J. Liu L. Xue Z. Zhang B. Jiang Z. Huang T. Wang Y. Wang C. Rethinking mobile block for efficient attention-based models Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision (ICCV) Paris, France 2–6 October 2023 1389 1400 30. Li H. Li J. Wei H. Liu Z. Zhan Z. Ren Q. Slim-neck by GSConv: A lightweight-design for real-time detector architectures J. Real-Time Image Process. 2024 21 62 10.1007/s11554-024-01436-6 31. Aharon N. Orfaig R. Bobrovsky B.-Z. Bot-Sort: Robust Associations Multi-Pedestrian Tracking arXiv 2022 220614651 32. Tu S. Liu H. Mao L. Tu C. Ye W. Yu H. Chen W. The urine formed element instance segmentation based on YOLOv5n Sci. Rep. 2024 14 28658 10.1038/s41598-024-79969-w 39562665 PMC11576977 33. Wan D. Lu R. Shen S. Xu T. Lang X. Ren Z. Mixed local channel attention for object detection Eng. Appl. Artif. Intell. 2023 123 106442 10.1016/j.engappai.2023.106442 34. Jiao J. Tang Y.M. Lin K.Y. Gao Y. Ma A.J. Wang Y. Zheng W.S. Dilateformer: Multi-scale dilated transformer for visual recognition IEEE Trans. Multimed. 2023 25 8906 8919 10.1109/TMM.2023.3243616 35. Wang Y. Li Y. Wang G. Liu X. Multi-scale attention network for single image super-resolution Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Seattle, WA, USA 17–21 June 2024 5950 5960 36. Chen J. Kao S.H. He H. Zhuo W. Wen S. Lee C.H. Chan S.H.G. Run, don’t walk: Chasing higher FLOPS for faster neural networks Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Vancouver, BC, Canada 17–24 June 2023 12021 12031 37. Wang C.Y. Yeh I.H. Mark Liao H.Y. Yolov9: Learning what you want to learn using programmable gradient information Proceedings of the European Conference on Computer Vision Milan, Italy 29 September–4 October 2024 Springer Berlin/Heidelberg, Germany 2024 1 21 38. Wang A. Chen H. Liu L. Chen K. Lin Z. Han J. Yolov10: Real-time end-to-end object detection Adv. Neural Inf. Process. Syst. 2024 37 107984 108011 39. Sun P. Cao J. Jiang Y. Zhang R. Xie E. Yuan Z. Wang C. Luo P. Transtrack: Multiple Object Tracking with Transformer arXiv 2020 201215460 40. Wojke N. Bewley A. Paulus D. Simple online and realtime tracking with a deep association metric Proceedings of the 2017 IEEE International Conference on Image Processing (ICIP) Beijing, China 17–20 September 2017 3645 3649 41. Stadler D. Beyerer J. An improved association pipeline for multi-person tracking Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Vancouver, BC, Canada 17–24 June 2023 3170 3179 42. Cao J. Pang J. Weng X. Khirodkar R. Kitani K. Observation-centric sort: Rethinking sort for robust multi-object tracking Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Vancouver, BC, Canada 17–24 June 2023 9686 9696 43. Maggiolino G. Ahmad A. Cao J. Kitani K. Deep oc-sort: Multi-pedestrian tracking by adaptive re-identification Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Vancouver, BC, Canada 17–24 June 2023 3025 3029 44. Zhang Y. Sun P. Jiang Y. Yu D. Weng F. Yuan Z. Luo P. Liu W. Wang X. Bytetrack: Multi-object tracking by associating every detection box Proceedings of the European Conference on Computer Vision Milan, Italy 29 September–4 October 2024 Springer Berlin/Heidelberg, Germany 2024 1 21 Figure 1 The structure of IEGS-BoT. Figure 2 The structure of IEGS-YOLO. Figure 3 The flow chart of IEMA. Figure 4 The structure of EMA. Figure 5 The structure of iRMB. Figure 6 The structure of GSConv. Figure 7 The matching process of BoT-SORT. Figure 8 The detection results of YOLO11n and IEGS-YOLO. Figure 9 The tracking results of BoT-SORT and IEGS-BoT. Figure 10 Comparison of Cell Trajectories using IEGS-BoT and BoT-SORT. Figure 11 Comparison of Cell Trajectories on APM Sequence. biomimetics-10-00564-t001_Table 1 Table 1 Description of each cell line. Label Description 3T3 Albino Swiss Mouse Embryo—fibroblasts A-10 Rat Thoracic Aortic Mesenchyme-myoblasts A-549 Male Human Lung Cancer—epithelial APM African Otter Skin—fibroblasts BPAE Bovine Pulmonary Artery—epithelial CV-1 Normal African Green Monkey Kidney—fibroblasts CRE-BAG2 Albino Swiss Mouse Embryo Moroni Murine Leukemia Virus Transfected Cells—fibroblasts LLC-MK2 Rhesus Monkey Kidney—epithelial MDBK Madin-Darby Bovine Kidney—epithelial MDOK Madin-Darby Ovine Kidney—epithelial OK Opossum Kidney Cortex Proximal Tubule—epithelial PL1Ut Raccoon Uterus—fibroblasts RK-13 Normal Rabbit Kidney—epithelial U2O-S Human Bone Osteosarcoma—epithelial biomimetics-10-00564-t002_Table 2 Table 2 Comparison of results of different hyperparameters (Best metric values are shown in bold). Optimizer Batch Size LR P R F1 Map50 Map AdamW 16 0.01  76.9  69.1  72.8  72.1 32.0 AdamW 16 0.001 75.8 67.6 71.5 70.4 31.4 AdamW 32 0.01 76.5 68.4 72.2 71.9  32.2 AdamW 32 0.001 74.0 65.7 69.6 68.7 30.9 SGD 16 0.01 75.2 66.2 70.4 69.9 31.5 SGD 16 0.001 73.1 63.4 67.9 67.1 28.5 SGD 32 0.01 76.2 64.8 70.0 69.3 31.2 SGD 32 0.001 72.0 61.1 66.1 65.9 28.2 biomimetics-10-00564-t003_Table 3 Table 3 Ablation experiments with different modules (Best metric values are shown in bold).  P R F1 Map50 Map Params Size GFLOPs (G) YOLO11n 76.9 69.1 72.8 72.1 32.0 2,584,882 5.5 6.3 +IEMA 76.5  69.8 73.0 73.1  32.7 2,589,858 5.5 6.4 +GSConv  77.0 69.6  73.1 72.6 32.0  2,495,314  5.3  6.2 IEGS-YOLO  77.0 69.6  73.1  73.2 32.6 2,500,290  5.3 6.3 biomimetics-10-00564-t004_Table 4 Table 4 Comparison of the performance of YOLO11n and IEGS-YOLO on Cell Image Detection (Best metric values are shown in bold).  P R F1 Map50 Map Parms Size GFLOPs (G) YOLO11n 82.0  68.6 74.7 79.4  46.5 2,582,737 5.5 6.3 IEGS-YOLO (Present)  88.1 65.7  75.3  79.8 46.4  2,498,145  5.3 6.3 biomimetics-10-00564-t005_Table 5 Table 5 Comparison of different attention mechanisms in YOLO11n (Best metric values are shown in bold).  P R F1 Map50 Map Params Size GFLOPs YOLO11n  76.9 69.1 72.8 72.1 32.0  2,584,882 5.5  6.3 +IEMA 76.5  69.8  73.0  73.1  32.7 2,589,858 5.5 6.4 +MLCA 76.4 68.3 72.1 72.0 31.9 2,584,888 5.5  6.3 +MSDA 76.2 69.4 72.6 72.4 32.5 2,601,522 5.5 6.5 biomimetics-10-00564-t006_Table 6 Table 6 Comparison of different convolutional modules in YOLO11n (Best metric values are shown in bold).  P R F1 Map50 Map Params Size GFLOPs YOLO11n 76.9 69.1 72.8 72.1 32.0 2,584,882 5.5 6.3 +GSConv  77.0  69.6  73.1  72.6 32.0  2,495,314  5.3  6.2 +MAB 76.2 67.4 71.5 70.9 31.9 2,587,922 5.5 6.3 +PConv 75.7 68.7 72.0 71.9  32.3 2,555,346 5.4  6.2 biomimetics-10-00564-t007_Table 7 Table 7 Comparison of detection performance of different models (Best metric values are shown in bold).  P R F1 Map50 Map Params Size GFLOPs YOLOv5n 74.1 65.6 69.6 69.3 31.0 2,184,394 4.6  5.8 YOLOv8n 75.0 66.0 70.2 70.0 31.8 2,687,098 5.6 6.8 YOLOv9t 75.4 68.7 71.9 71.6 32.5  1,732,554  4.1 6.5 YOLOv10n 76.1 67.1 71.3 70.2 30.9 2,699,876 5.7 8.3 YOLO11n 76.9 69.1 72.8 72.1 32.0 2,584,882 5.5 6.3 YOLO11s 75.4 65.6 70.2 70.6  33.4 9,418,218 19.2 21.3 YOLO11m 75.7 66.0 70.5 70.9 33.0 20,040,826 40.5 67.7 YOLO11l 73.0 60.8 66.3 66.2 31.6 25,290,106 51.2 86.6 IEGS-YOLO  77.0  69.6  73.1  73.2 32.6 2,500,290 5.3 6.3 biomimetics-10-00564-t008_Table 8 Table 8 Comparison of tracking results of different tracking algorithms (Best metric values are shown in bold).  MOTA (%) HOTA (%) IDF1 (%) IDSW TransTrack 51.56 44.42 56.18 5985 DeepSORT 44.81 42.60 54.17 2938 ImprAsso 46.71 38.59 44.86 13,107 OC-SORT 41.29 40.61 54.97 1427 Deep OC-SORT 46.63 43.26 57.16 2758 Bytetrack 52.44 51.03 66.37 1217 Bot-Sort 52.78 51.07 66.23 1170 IEGS-BoT (Present)  53.97  51.30  67.52  894 ",
  "metadata": {
    "Title of this paper": "Bytetrack: Multi-object tracking by associating every detection box",
    "Journal it was published in:": "Biomimetics",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12467273/"
  }
}
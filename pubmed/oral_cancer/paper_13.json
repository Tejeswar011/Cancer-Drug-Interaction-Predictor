{
  "title": "Paper_13",
  "abstract": "pmc Future Oncol Future Oncol 3069 futoncol Future Oncology 1479-6694 1744-8301 Taylor & Francis PMC12490408 PMC12490408.1 12490408 12490408 40888174 10.1080/14796694.2025.2552098 2552098 1 Version of Record Review Article Review Finding the right tool for the specific task: navigating RWE tools and checklists R. WILLKE ET AL. FUTURE ONCOLOGY Willke Richard  a Cottu Paul  b  c Briggs Andrew  d Siebert Uwe  e  f  g Chen Connie  h Korytowsky Beata  h Heidt Julien  i Renfrow Meghan  i Lovett Kate  i Brufsky Adam  j a Scintegral Health Economics LLC Soddy Daisy TN USA b Institut Curie Paris France c Université Paris Cité Paris France d London School of Hygiene & Tropical Medicine London UK e UMIT TIROL - University for Health Sciences and Technology Hall in Tirol Austria f Harvard T.H. Chan School of Public Health Boston MA USA g Harvard Medical School Boston MA USA h Pfizer New York NY USA i IQVIA Durham NC USA j University of Pittsburg Medical Center PA USA CONTACT Connie Chen connie.chen@pfizer.com 66 Hudson Boulevard New York NY 10001 USA 1 9 2025 2025 21 23 498190 3075 3089 01 09 2025 03 10 2025 03 10 2025 Integra 16 9 2025 Integra 16 9 2025 11 4 2025 21 8 2025 © 2025 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group. 2025 The Author(s) https://creativecommons.org/licenses/by-nc-nd/4.0/ This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives License ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ABSTRACT Real-world evidence (RWE) is increasingly used to support product approvals and label expansions, as well as clinical and payer decision-making. Various tools (e.g. frameworks, checklists) have been developed to help inform and assess the robustness and quality of real-world study design and reporting. This targeted review provides a practical guide for leveraging these tools to increase awareness and utility for decision-makers. A pre-defined search strategy was applied to identify articles from PubMed. Articles published from 1 January 2020, through 4 October 2024 were included and reviewed to identify relevant tools aimed at assessing RWE study planning, reporting, or quality assessment. Key information regarding each was extracted and summarized including strengths, limitations, and included domains. 119 articles were initially identified, of which 15 were included after screening, referencing a total of 17 tools. These 17 tools varied in format and structure, ranging from detailed guidelines and templates to checklists and questionnaires. Utility and application of the tools identified in this targeted review vary across the evaluation of study planning, reporting, and quality. Selection of the appropriate tool depends on several factors including intended purpose of the tool, intended real-world study design, and the availability of study documentation. KEYWORDS Real-world evidence real-world data payer decision-making health technology assessment frameworks checklists decision-making tools reporting standards Pfizer, Inc This study was funded by Pfizer, Inc. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Background Real-world data (RWD) refers to data on patient health status and healthcare delivery that is routinely collected from various sources, such as electronic health records (EHR), insurance claims, and patient registries. Real-world evidence (RWE) is derived from the analysis of RWD and provides insights into the effectiveness and safety of medical products in real-world settings [ 1 1–3 4 5 6 4 3 Recent policy developments have underscored the growing importance of RWE in healthcare decision-making. Notably, the 21st Century Cures Act, enacted in 2016, aimed to accelerate medical product development and bring innovations to patients more efficiently [ 7 8 9–13 The importance of RWE in oncology is particularly notable due to its growing role in regulatory and payer assessment for oncology therapies. The FDA’s Oncology Center of Excellence has advanced efforts to incorporate RWE into oncology regulatory decisions. This includes using RWE to complement traditional clinical trial data, thereby accelerating the approval process for new oncology drugs and expanding indications for existing therapies [ 14 15 The growing demand for RWE in oncology and the development of various assessment or study design guidance tools mark an exciting shift in the field. However, the wide range of tools with different purposes, quality, and expertise requirements highlights the need for standardization and guidance on practical implementation. Although multiple checklists, frameworks, and tools exist to help assess and interpret RWE, there may be low awareness of their availability and a lack of direction on selecting the most appropriate tool, leading to their underutilization. For example, multiple tools are available to guide design and reporting of real-world oncology studies, one of which is the European Society for Medical Oncology’s Guidance for Reporting Oncology Real-World Evidence (ESMO-GROW) [ 3 16 17 18 2. Methods 2.1. Study design A targeted review of the peer-reviewed literature (1 January 2020 – 4 October 2024) was conducted in PubMed to identify relevant RWE tools. PubMed was chosen as the sole data source for this review as it is a comprehensive and widely recognized database that indexes a vast array of high-impact, peer-reviewed journals across multiple disciplines. We did not anticipate a much larger body of literature by expanding beyond PubMed. The review comprised the following steps to identify the tools for inclusion: 1) search strategy development, 2) article selection and tool identification, and 3) article review. 2.2. Search strategy A list of search terms was developed to identify relevant published literature, including “checklist,” “tool,” “real-world,” “observational,” “rwe” (for full search string see supplemental material). The search criteria excluded certain study types (i.e., case reports, clinical trials, complementary therapies, protocols) and specific topics not related to oncology (i.e., air pollutants, education, sports nutrition, COVID-19, drug-related side effects, adverse event reporting systems, pharmacokinetics, and safety) from the general pool of results. The aim of this search strategy was to include both tools specific to RWE in oncology and broader therapy areas, while excluding those with a specific focus in non-oncology therapy areas. If newer tools were based on tools developed prior to the article selection window, the original tools were identified and included (information on date of original publication was captured). 2.2.1. Article selection and tool identification A single independent reviewer (with no adjudication) operationalized the following inclusion criteria: Articles that described RWE tools, or were cited within relevant review articles (ancestral search) AND Articles that detailed tools used in the design, conduct, and/or reporting of RWE studies Articles were excluded based on the following exclusion criteria: Articles that are review papers and do not describe an RWE tool OR Articles not available in English OR Articles that are disease-specific and not related to oncology (e.g., The OHStat Guidelines for Reporting Observational Studies and Clinical Trials in Oral Health Research: Manuscript Checklist) OR Articles detailing tools that originate from HTA or regulatory guidance and are country-specific, unless the tool(s) has/have been used and recognized on a global level (e.g., (i.e., ISPOR – The Professional Society for Health Economics and Outcomes Research). Next, tools mentioned within the included articles were assessed for eligibility based on the following inclusion criteria: Tools applicable to the design, reporting, and/or quality assessment of RWE studies Tools were excluded based on the following criteria: Tools not applicable to the design, reporting, and/or quality assessment of RWE studies OR Tools that are disease-specific and not related to oncology OR Tools that originate from HTA or regulatory guidance and are country-specific, unless they have been used and recognized on a global level (i.e., ISPOR) OR Tools that were applicable to the design of only meta-analyses or systematic reviews. Given that most RWE tools are designed to be broadly applicable across various audiences, articles and tools were neither included nor excluded based on the intended user. Similarly, the format of the tools was not a criterion for inclusion or exclusion but was instead evaluated during the article review process. 2.2.2. Article review Once articles were identified and selected, a single independent reviewer assessed each manuscript and extracted key information based upon a priori Tables 1–3 Table 1. Tools to support protocol development.  Characteristic Most* useful when …  Tool Format Length (# of items) Year Published (Year Last Revised) Formally validation/tested? Strength(s) Weakness(es) Notable domains as described within the tool (non-exhaustive)* … structured protocol template is needed … seeking guidance on data feasibility requirements … high complexity of study design and/or data sources Ref European Network of Center for Pharmacoepidemiology and Pharmacovigilance (ENCePP) Checklist for Study Protocols Checklist (Yes/No) 68 2011 (2018) No • Compatible with multiple RWE study designs (e.g., exploratory, descriptive, prediction) and data sources • May require referring to supplemental resource (ENCePP Guide on Methodological Standards in Pharmacoepidemiology) • Study design, source, & populations   x [ 21 European Network of Center for Pharmacoepidemiology and Pharmacovigilance (ENCePP) Guide on Methodological Standards in Pharmacoepidemiology Guideline recommendations (200 pages) N/A 2011 (2023) No • Periodically updated (11 times to date) to reflect latest methodological advancements • High implementation effort due to length of document • Study protocol & design  x x [ 22 HARmonized Protocol Template to Enhance Reproducibility of Hypothesis Evaluating Real-World Evidence (HARPER) Template 24 2022 No • Combines structured table format with free-text sections to enhance flexibility • Does not cover every aspect of transparency over lifecycle of research study (e.g., sharing protocol, code data, etc.) • Timeline x  x [ 19 20 International Society for Good Pharmacoepidemiology Practice Guide for Good Pharmacoepidemiology Practice (ISPE GPP) Guideline recommendations N/A 1996 (2015) No • Does not require extensive expertise • Broad guidelines that are not study design-specific• May not reflect latest methodological advancements • Protocol development  x  [ 3 19 23–25 Structured Template and Reporting Tool for Real-World Evidence (STaRT-RWE) Framework 16 2021 No • Compatible with multiple RWE study designs (e.g., exploratory, descriptive, predictive) and data sources • Focused primarily on study design decisions • Design diagram x  x [ 20 Abbreviation: RWE: Real-world evidence. Notable domains are the key areas a tool addresses, providing insight into its scope and applicability. These domains are non-exhaustive, meaning the tool may cover additional areas beyond the main ones listed. The lack of a checkmark beneath the “Most useful when…” columns does not imply that a tool cannot be used for the stated purpose, only that it may not necessarily be the most useful in that particular instance. Table 2. Tools to support study reporting.  Characteristic Most* useful when …  Tool Format Length (# of items) Year Published (Year Last Revised) Formally validation/tested? Strength(s) Weakness(es) Notable domains as described within the tool (non-exhaustive)* … structured protocol template is needed … seeking guidance on data feasibility requirements … high complexity of study design and/or data sources Ref Assessment of Real-World Observational Studies (ArRoWS) Questionnaire 9 2019 Yes • Low implementation effort (shorter in length compared to other tools) • Potential for ambiguity Core items: x  x [ 26 European Society for Medical Oncology Guidance for Reporting Oncology Real-World Evidence (ESMO-GROW) Checklist 35 2023 No • Clear, categorical criteria for scoring • Requires specific expertise (e.g., oncology knowledge) • Introduction x  x [ 3 16 27 HARmonized Protocol Template to Enhance Reproducibility of Hypothesis Evaluating Real-World Evidence Studies on Treatment Effects (HARPER) Template 24 2022 No • Combines structured table format with free-text sections to enhance flexibility • Does not cover every aspect of transparency over lifecycle of research study (e.g., sharing protocol, code data, etc.) • Timeline x [ 19 20 International Society for Good Pharmacoepidemiology Practice Guide for Good Pharmacoepidemiology Practice (ISPE GPP) Guideline recommendations N/A 1996 (2015) No • Does not require extensive expertise • Broad guidelines that are not method-specific• Periodically updated and may not reflect latest methodological advancements • Protocol development x x  [ 3 19 23–25 Structured Template and Reporting Tool for Real-World Evidence (STaRT-RWE) Framework 16 2021 No • Compatible with multiple RWE study designs (e.g., exploratory, descriptive, predictive) and data sources • Focused primarily on study implementation decisions • Design diagram   x [ 20 STrengthening the Reporting of OBservational studies in Epidemiology (STROBE) Checklist 22 2007 (2021) No • Widely endorsed • Limited to three study design types • Introduction x x  [ 24 28–31 AI: Artificial intelligence; RWE: Real-world evidence       *Notable domains are the key areas a tool addresses, providing insight into its scope and applicability. These domains are non-exhaustive, meaning the tool may cover additional areas beyond the main ones listed. The lack of a checkmark beneath the “Most useful when…” columns does not imply that a tool cannot be used for the stated purpose, only that it may not necessarily be the most useful in that particular instance. Table 3. Tools to support quality assessment.  Characteristic Most* useful when …  Tool Format Length (# of items) Year Published (Year Last Revised) Formally validation/tested? Strength(s) Weakness(es) Notable domains as described within the tool (non-exhaustive)* … structured protocol template is needed … seeking guidance on data feasibility requirements … high complexity of study design and/or data sources Ref Assessment of Real-World Observational Studies (ArRoWS) Questionnaire 9 2019 Yes • Low implementation effort (shorter in length compared to other tools) • Potential for ambiguity due to subjective nature Core items: x x  [ 26 Data Governance Checklist Checklist 38 2023 –proposed No • Covers multiple components of data governance (e.g., data privacy, security, management, and access) • Requires expert interpretation (e.g., understanding of data privacy, management, etc.) • Data privacy & security x  x [ 35 European Network of Center for Pharmacoepidemiology and Pharmacovigilance (ENCePP) Guide on Methodological Standards in Pharmacoepidemiology Guideline recommendations (200 pages) N/A 2011 (2023) No • Frequently updated to incorporate the latest methodological advancements • High implementation effort • Study design, source, & populations x x  [ 22 European Society for Medical Oncology Guidance for Reporting Oncology Real-World Evidence (ESMO-GROW) Checklist 35 2023 No • Clear, categorical criteria for scoring • Requires specific expertise (e.g., oncology knowledge) • Introduction x x x [ 3 16 27 Good Research for Comparative Effectiveness (GRACE) Checklist Checklist 11 2014 (2016) Yes • Widely endorsed (e.g., NPC, ISPE) • Potential for ambiguity due to subjective nature • Data x   [ 35 Grading of Recommendations Assessment, Development, and Evaluation (GRADE) Approach Framework 8 2000 (2013) Yes • Widely endorsed (e.g., WHO, FDA, EMA) • Potential for ambiguity due to subjective nature • Risk of bias x  x [ 34 International Society for Good Pharmacoepidemiology Practice Guide for Good (ISPE GPP) Guideline recommendations N/A 1996 (2015) No • Comprehensive methodological resource • Broad guidelines that are not method-specific• Periodically updated and may not reflect latest advancements in pharmacoepidemiology • Protocol development x   [ 3 19 23–25 International Society for Pharmacoeconomics and Outcomes Research – Academy of Managed Care Pharmacy – National Pharmaceutical Council (ISPOR-AMCP-NPC Questionnaire) Questionnaire (Yes/No/NA) 33 2012 (2017) No • Low implementation effort • Potential for ambiguity due to subjective nature • Relevance x x x [ 38 International Society for Pharmacoeconomics and Outcomes Research (ISPOR) Checklist on Retrospective Database Studies Checklist 27 2003 (2013) No • Tailored to retrospective database studies • Primarily designed for medical claims or encounter-based databases, limiting applicability to other designs Data delineation: x x  [ 37 International Society for Pharmacoeconomics and Outcomes Research (ISPOR) SUITABILITY Checklist Checklist 24 2024 No • Focused on EHR data for HTAs • Limited applicability to RWD types beyond EHR data • Relevance x x  [ 36 National Evaluation System for health Technology Coordinating Center (NESTcc) Data Quality Framework Framework 7 I 2020 No • Focused on EHR data • Requires expert interpretation (e.g., data management, EHR data) Data: x x  [ 19 Risk of Bias in Non-randomized Studies – of Interventions (ROBINS-I) Questionnaire 34 2016 No • Specifically designed to assess non-randomized studies • Potential for ambiguity due to subjective nature Bias: x  x [ 3 31 Structured Template and Reporting Tool for Real-World Evidence (STaRT-RWE) Framework 16 2021 No • Compatible with multiple RWE study designs (e.g., exploratory, descriptive, predictive) and data sources • Focused primarily on study implementation decisions • Design diagram x   [ 20 Use-case specific Relevance and Quality Assessment (UReQA) Framework Framework 5 2020 (2023) No • Compatible with multiple RWE study designs (e.g., exploratory, descriptive, predictive) and data sources • Potential for ambiguity due to subjective nature Preassessment x x  [ 24 32 AI: Artificial intelligence; EHR: Electronic helath records; FDA: Food and Drug Administration; RWD: Real-world data; RWE: Real-world evidence; WHO: World Health Organization *Notable domains are the key areas a tool addresses, providing insight into its scope and applicability. These domains are non-exhaustive, meaning the tool may cover additional areas beyond the main ones listed. The lack of a checkmark beneath the “Most useful when…” columns does not imply that a tool cannot be used for the stated purpose, only that it may not necessarily be the most useful in that particular instance. Abbreviations: AI: Artificial intelligence; EHR: Electronic helath records; FDA: Food and Drug Administration; RWD: Real-world data; RWE: Real-world evidence; WHO: World Health Organization. 3. Results The application of the aforementioned search strategy yielded 119 articles, 35 of which were included for subsequent article assessment. Of those, 15 articles were ultimately included in the review. Within the 15 selected articles, there were a total of 17 tools. These tools varied in terms of format, including checklists, guideline recommendations, templates, frameworks, and questionnaires. Three of the 17 included tools have undergone some form of formal validation, meaning they have been assessed through methods such as inter-rater reliability, content and/or construct validation, sensitivity and specificity testing, and/or practical utility testing. In terms of intended use case, of the 17 included tools, five support protocol development, six support study reporting, and 14 support quality assessment (not mutually exclusive) ( Figure 1 Figure 1. PRISMA flow diagram. Abbreviations: RWE: Real-world evidence; HTA: Health technology assessment; ISPOR: The Professional Society for Health Economics and Outcomes Research. 3.1. Tools supporting protocol development The 5 tools identified for supporting protocol development included: 1) HARmonized Protocol Template to Enhance Reproducibility of Hypothesis Evaluating Real-World Evidence Studies on Treatment Effects (HARPER) [ 19 20 21 20 22 3 19 23–25 Table 1 10 3.2. Tools supporting study reporting The six tools identified for supporting study reporting included: 1) Assessment of Real-World Observational Studies (ArRoWS) [ 26 3 16 27 19 20 3 19 23–25 20 24 28–31 19 24 28–31 Table 2 3 16 27 19 Table 2 3.3. Tools supporting quality assessment There were 14 tools identified for quality assessment ( Table 3 3 19 23–25 24 32 19 33 3 31 Table 3 There are multiple reasons why a decision maker may select one tool for quality assessment versus another. Although the UReQA framework (2023) appears to have a limited number of “items,” it actually includes five interlinked, iterative steps that are relatively labor intensive (preassessment, data element standardization, cohort definition, verification and validation, and benchmarking). This framework has a strong emphasis on data relevance and incorporates multiple stakeholder perspectives to accommodate various types of RWE studies and data types; however, the framework is rather general and lacks customization for specific therapeutic areas (unlike ESMO-GROW or ArRoWS, for example). UReQA also lacks explicit guidance for evaluating studies using certain analytic methods (such as machine learning or predictive analytics). In contrast, the ROBINS-I questionnaire is structured and systematic, facilitating the consistent and transparent evaluation of bias risk. The language in the questionnaire is aligned with the terminology of causal inference (e.g., target trial). While useful for studies intended to assess causal effects, substantial expertise in epidemiology is required to understand potential biases that may be present in an RWE study. Additionally, the strong focus on internal validity means this questionnaire may need to be supplemented with tools like UReQA or ArRoWS that assess the relevance and applicability of the evidence. While less recent, Grading of Recommendations Assessment, Development, and Evaluation (GRADE) (2013), a systematic method for assessing certainty of evidence and strength of recommendations in healthcare, is widely used and endorsed for quality assessment [ 34 35 The DG checklist (2023) aims to promote compliance of RWE studies through addressing key aspects such as data privacy, security, management, and access. This checklist was developed based on a literature review and a three-round Delphi panel including multi-stakeholder perspectives. The checklist items cover topics such as data privacy and security, data management and linkage, data access, and RWE generation. Each checklist item can receive a “yes” or a “no,” which then translates to a quantitative score corresponding to “excellent,” “acceptable,” or “low” quality [ 33 The NESTcc data quality framework (2020) is designed to ensure quality of RWD for decision making, and while comprehensive in scope and applicable to diverse stakeholders, it primarily addresses EHR data [ 19 36 In addition to the ISPOR SUITability checklist, our targeted review identified two other ISPOR sponsored checklists for RWE quality assessment. The ISPOR Checklist on Retrospective Database Studies (2013) is less recent and, although applicable to multiple types of retrospective data, was developed primarily for medical claims or encounter-based databases in order to support quality assessment [ 37 38 There are various elements to consider when selecting a tool, including its recency, the need to assess internal vs external validity, the type of data (EHR, claims, other), and the desired format of the output (quantitative score vs qualitative insights). 4. Discussion This targeted literature review identified 17 tools that could be useful to HCPs and payers in the evaluation of RWE study planning, reporting, and quality. Although the tools identified in this review vary with regards to domains covered, degree of complexity ( Table 4 Table 4. Tools by intended use case and degree of complexity.  Intended Use Case  Degree of Complexity* Protocol Development Study Reporting Quality Assessment Ref Lower • ENCePP Checklist for Study Protocols • ArRoWS • ArRoWS [ 21 24 26 28–31 36 38 Higher • ENCePP Guide on Methodological Standards in Pharmacoepidemiology • ESMO-GROW • DG Checklist [ 3 16 19 20 22 23 25 27 Abbreviations: ENCePP: European Network of Center for Pharmacoepidemiology and Pharmacovigilance; ArRoWS: Assessment of Real-World Observational Studies; ISPOR: International Society for Pharmacoeconomics and Outcomes Research; STROBE: STrengthening the Reporting of OBservational studies in Epidemiology; AMCP-NPC: Academy of Managed Care Pharmacy – National Pharmaceutical Council; HARPER: HARmonized Protocol Template to Enhance Reproducibility of Hypothesis Evaluating Real-World Evidence Studies on Treatment Effects; STaRT-RWE: Structured Template and Reporting Tool for Real-World Evidence; ESMO-GROW: European Society for Medical Oncology Guidance for Reporting Oncology Real-World Evidence; ISPE: International Society for Good Pharmacoepidemiology Practice; DG: Data Governance; GRACE: Good Research for Comparative Effectiveness; GRADE: Grading of Recommendations Assessment, Development, and Evaluation; NESTcc: National Evaluation System for health Technology Coordinating Center; ROBINS-I: Risk of Bias in Non-randomized Studies – of Interventions; UReQA: Use-case specific Relevance and Quality Assessment. *Tools listed in this table were rated based on their complexity relative to each other, with high complexity tools covering a wide range of topics, providing detailed instructions, and requiring more advanced technical knowledge (e.g., biostatistics, pharmacoepidemiology, data privacy and management) and increased implementation effort. Lower complexity checklists are more focused, concise, and easier to apply with basic understanding and fewer resources. Though knowledge of the tools, frameworks, and checklists identified in this review exists in the epidemiology community, awareness is lower beyond that stakeholder group. Given the number of existing tools and lack of harmonized expectations on RWE study design, quality elements, and reporting [ 3 35 Adopting a template, checklist, or framework to support protocol development helps to ensure that all of the necessary elements are included. Utilizing such templates to develop study protocols can ensure the appropriate elements are identified a priori 19 10 Utilizing tools for results reporting further increases traceability and supports data trustworthiness. If a tool is used to inform study reporting, it should be cited. The study’s protocol (or a redacted version) should be made publicly available (perhaps the data dictionary as well), per EU PASS register and www.clinicaltrial.gov 39 40 38 Many tools overlap in their focus on ensuring the inclusion of necessary elements in study protocols and enhancing transparency and reproducibility. For example, HARPER, ENCePP Checklist for Study Protocols, and STaRT-RWE all provide comprehensive guidance for protocol development, covering domains such as study design, data sources, and analytic methods. However, this redundancy can lead to confusion among users regarding which tool to select, highlighting the need for clearer guidance around selecting the right tool for a given task and, as applicable, formal evaluation of existing tools for different tasks. Certain domains, such as external validity and data provenance, are often under-addressed. External validity, which pertains to the generalizability of study findings to broader patient populations, is crucial for the applicability of RWE but is not consistently emphasized across all tools. Similarly, data provenance, which involves documenting the lineage and transformations of the data used, is essential for assessing data reliability but is not uniformly covered. Tools like the DG Checklist and NESTcc Data Quality Framework address data governance and quality but may not provide sufficient guidance on external validity. The targeted focus of some tools can be highly beneficial for certain use cases, providing specialized guidance and insights; however, this can also be limiting for broader applications. ESMO-GROW, for example, provides valuable, oncology-specific guidance for RWE study reporting and quality assessment; however, its focus on oncology may limit its applicability for broader stakeholder needs, such as regulators or HTA bodies, who may require additional economic and methodological considerations when evaluating RWE. In such contexts, more general tools like the GetReal Trial Tool may be better aligned with these broader evaluative frameworks [ 41 The tools captured in this review do not fully address the gap of assessing underlying data quality. There are existing frameworks for conducting fit-for-purpose feasibility assessment (e.g., Structured Process to Identify Fit-for-purpose data [ 42 40 15 The existence of multiple tools at different stages of a study’s life cycle presents a challenge for decision makers, underscoring a need for structured and iterative assessment of data sources, study design, and the resulting RWE. For “end users” of that RWE (e.g., payers, HCPs), having visibility into the assessments, tools, and templates that have been utilized by researchers throughout the study may support transparency, consistency, and enhanced understanding. In order to increase tools’ awareness and uptake, socialization and training on these tools should be incorporated into educational forums and physician/continuing medical education training. Academic journals requiring or recommending the use of RWE tools may also help place emphasis on the importance of study quality and thus increase adoption. 4.1. Limitations Our targeted review has several limitations. Publications with tools were selected using predefined search terms developed based on subject matter expertise and were adjudicated by multiple authors with expertise in the development, reporting, and evaluation of RWE; however, these terms were not validated, meaning the sensitivity and specificity is not known. The search was also limited to a single source (PubMed), which introduces the potential for database bias. It is possible other tools are available, as this was a targeted rather than systematic review. Data extraction was conducted manually, and assessments of tool characteristics, including strengths and weaknesses, were either drawn directly from the source publications or inferred based on the expertise of the reviewers in the absence of explicit evaluations. These assessments were first conducted by the primary reviewer, who holds a clinical doctoral degree and a master’s in business administration with an analytics focus. They were then adjudicated by additional expert reviewers, including multiple with advanced degrees in epidemiology and decades of combined expertise in pharmacoepidemiology, real-world evidence, clinical oncology, and outcomes research; however, they were not based on a standardized or validated assessment framework. Ultimately, what is considered a strength or weakness of a tool may vary depending on the intended use, user needs, and research context. The perceived value of specific tool features may also evolve over time, particularly as tools are externally validated or gain recognition and adoption by various stakeholders. We did not include tools designed for specific therapeutic areas or interventions; however, prior studies have found that appraisal tools designed for specific interventions have the potential to be applied to general interventions [ 25 The categorization of tools by use case was based on the available literature and the content of the tool itself. For certain tools, it could be argued that they are applicable for other use cases as compared to how they have been classified here. This review offers considerations for implementing tools in various use cases but does not provide a formal evaluation of their utility. Future studies could formally evaluate the comparative value of different checklists for different use cases. 5. Conclusions The tools identified in this targeted review can be utilized by various stakeholders for multiple use cases, including guiding study planning and reporting, as well as supporting quality assessment. The successful implementation of these tools creates more transparent, rigorous RWE generation and interpretation of study findings. When deciding which of these tools to consider for a specific use case, there is no one-size-fits-all solution. While some tools are shorter in length, they may be more high-level, which can create ambiguity in their operationalization. Selection of the appropriate tool will depend on multiple factors including intended purpose of the tool, intended real-world study design, and the availability of study documentation. Given the sheer number of available tools and their variability in terms of domains, structure, and terminology, a coordinated effort among methodologists, regulators, payers, and other stakeholders involved in generating, evaluating, and reporting RWE could help develop further guidance around selecting the appropriate tool for a given task. As existing tools are harmonized, the barrier to adoption may be lowered, particularly for nonspecialist users. 6. Future perspective Looking ahead, the next 5–10 years are likely to bring significant evolution in how RWE quality is assessed, particularly in oncology. Advances in artificial intelligence (AI) will increasingly augment key components of RWE generation, such as cohort identification, unstructured data abstraction, and bias detection, necessitating the development of tools and checklists that incorporate AI-specific quality domains (e.g., model transparency, calibration, drift monitoring). Traditional static checklists may give way to more dynamic, continuous quality assurance frameworks that reflect the iterative and evolving nature of RWD generation and use. As regulatory bodies across jurisdictions move toward greater harmonization, metadata standards, and quality evaluation frameworks are expected to become more interoperable and consistent, with oncology-specific adaptations (e.g., tumor-specific endpoints, genomic data integration) becoming more formalized. The growing use of novel and multi-modal data sources, including PROs, digital biomarkers, and genomic sequencing, will, particularly in the context of precision oncology, demand more sophisticated tools that evaluate data provenance, linkage quality, and contextual relevance. Moreover, as privacy-preserving analytic approaches and federated data environments become more common, RWE frameworks will need to account for distributed data architectures where direct data access is limited. Finally, we anticipate the emergence of curated repositories or registries of RWE datasets that are supported by standardized data quality metrics and potential certification mechanism. Together, these trends underscore the need for flexible tools that can adapt to the rapidly changing landscape of oncology research and RWE generation. Supplementary Material Supplemental Material Supplemental Material Article highlights  Real-world evidence (RWE) is increasingly used to support regulatory approvals, label expansions, and payer decision-making in oncology. A targeted literature review identified 17 tools designed to support RWE study planning, reporting, and quality assessment, drawn from 15 articles published between 2020 and 2024. Tools varied in format (e.g., checklists, templates, frameworks, questionnaires), and were categorized by use case: protocol development (5 tools), study reporting (6 tools), and quality assessment (14 tools). Selecting the appropriate tool depends on factors such as study design, intended use, geographic context, and availability of study documentation. Many tools overlap in purpose but differ in complexity, format, and applicability, highlighting the need for harmonization and user guidance. Awareness and adoption of these tools remain limited outside the epidemiology community, underscoring the need for broader education and standardization. Future RWE tools will need to evolve to address AI integration, novel data types, and federated data environments, especially in oncology research. Author contributions All authors meet the authorship criteria as defined by Future Oncology. Each author made a significant contribution to the work reported, participation in drafting or critically revising the article, agreed on the journal submission, reviewed and approved all versions of the manuscript, and accepts responsibility for the integrity of the work. Connie Chen and Beata Korytowsky provided overall strategic oversight, including development of concept, study design, and critical review of the manuscript. Richard Willke, Paul Cottu, Andrew Briggs, Uwe Siebert, and Adam Brufsky provided subject matter expertise, reviewed the manuscript, and gave critical feedback from health care provider and payer perspective. Julien Heidt, Meghan Renfrow, and Kate Lovett participated in study design and implementation, including data collection, data analysis and interpretation, and drafting of the manuscript. Disclosure statement C.C. and B.K. are employees of and hold stock in Pfizer, Inc; J.H., M.R., and K.L. are/were employees of IQVIA, which received funding from Pfizer in connection with the development of the manuscript and to complete this study. A.Brufsky has received grants from Agendia and AstraZeneca; consulting fees or honoraria from AstraZeneca, Pfizer, Novartis, Lilly, Genentech/Roche, Seagen, Daiichi-Sankyo, Merck, Agendia, Sanofi, and Puma. P.C. has received honoraria for consulting or advisory roles from Novartis and Pfizer; research funding from AstraZeneca, Genentech/Roche, Novartis, Pierre Fabre, and Pfizer; reimbursement for travel, accommodations, or expenses from AstraZeneca, NanoString Technologies, Novartis, Pfizer, and Roche. R.W. has received honoraria for consulting or advisory roles from Pfizer, Viatris, Sarepta, Bayer, SKL, and consulting for work with PhRMA. A. Briggs reports consultancy payments from Pfizer in relation to the current manuscript and consultancy payments from the following companies, not related to the current manuscript: Astellas, AstraZeneca, BioCryst, Boeringher Ingelheim, Chiesi, Daiichi Sankyo, Gilead, Galderma, GSK, Idorsia, Novartis, Pharmacosmos, Rythmn, Sanofi, Sobi, Takeda, Teofarma. U.S. has received honoraria for consulting or advisory roles from Pfizer and also provides teaching and consulting in causal inference and health decision science methods for academic institutions, scientific societies, HTA agencies, and industry. The authors have no other relevant affiliations or financial involvement with any organization or entity with a financial interest in or financial conflict with the subject matter or materials discussed in the manuscript apart from those disclosed. Reviewer disclosure Peer reviewers on this manuscript have no relevant financial or other relationships to disclose. Supplemental data Supplemental data for this article can be accessed online at https://doi.org/10.1080/14796694.2025.2552098 References  Papers of special note have been highlighted as either of interest (•) or of considerable interest (••) to readers. 1. Zisis K Pavi E Geitona M Real-world data: a comprehensive literature review on the barriers, challenges, and opportunities associated with their inclusion in the health technology assessment process J Pharm Pharm Sci 2024 27 12302 10.3389/jpps.2024.12302 38481726 PMC10932954 2. Hogervorst MA Soman KV Gardarsdottir H Analytical methods for comparing uncontrolled trials with external controls from real-world data: a systematic literature review and comparison with European regulatory and health technology assessment practice Value Health 2024 2025 Sep 4 28 1 161 174 10.1016/j.jval.2024.08.002 39241824 3. Sarri G Hernandez LG. The maze of real-world evidence frameworks: from a desert to a jungle! An environmental scan and comparison across regulatory and health technology assessment agencies J Comp Eff Res 2024 Sep 13 9 e240061 10.57264/cer-2024-0061 39132748 PMC11367564  •• This review by Sarri and Hernandez focuses on regulatory and health technology assessment (HTA) frameworks for real-world evidence (RWE). Our work is important because it fills a gap in this review by specifically looking at the published literature on RWE. 4. Batra A Cheung WY Role of real-world evidence in informing cancer care: lessons from colorectal cancer Curr Oncol 2019 Nov 26 Suppl 11 S53 S56 10.3747/co.26.5625 31819710 PMC6878934 5. Jansen MS Dekkers OM le Cessie S Real-world evidence to inform regulatory decision making: a scoping review Clin Pharmacol Ther 2024 115 6 1269 1276 10.1002/cpt.3218 38390633 6. Innovative Health Initiative D6.2 report on global regulatory best practices analysis: a scoping review of HTA and regulatory RWD/RWE policy documents 2024 Available from: https://www.iderha.org/sites/iderha/files/2024-05/D6.2%20Report%20on%20Global%20Regulatory%20Best%20Practices%20Analysis_v2.0.pdf 7. U.S. Department of Health and Human Services Food and Drug Administration, Center for Drug Evaluation and Research, Center for Biologics Evaluation and Research, Oncology Center of Excellence. Real-world data: assessing electronic health records and medical claims data to support regulatory decision-making for drug and biological products Fda guidance 2024 Jul 8. U.S. Department of Health and Human Services Food and Drug Administration. Framework for FDA’s real-world evidence program Fda guidance 2018 Dec 9. Cantoni C Pearl M Data quality framework for EU medicines regulation: application to real-world data EMA Guid 2023 Dec 10. European Medicines Agency. ICH M14 Guideline on General Principles on Plan Design, and analysis of pharmacoepidemiological studies that utilize real-world data for safety assessment of medicines cited 2025 Jul 15 Available from: https://www.ema.europa.eu/en/documents/scientific-guideline/ich-m14-guideline-general-principles-plan-design-analysis-pharmacoepidemiological-studies-utilize-real-world-data-safety-assessment-medicines-step-2b_en.pdf 11. CADTH Guidance for reporting real-world evidence Cadth guidance 2023 May 12. National Institute for Health and Care Excellence Nice real-world evidence framework NICE Guid 2022 Jun 10.57264/cer-2023-0135 PMC10690376 37855246 13. Institute for Clinical and Economic Review A guide to ICER’s methods for health technology assessment Institute for Clinical and Economic Review. A guide to ICER’s methods for health technology assessment. ICER guidance 2020 Oct 14. Sola-Morales O Curtis LH Heidt J Effectively leveraging RWD for external controls: a systematic literature review of regulatory and HTA decisions Clin Pharmacol Ther 2023 114 2 325 355 10.1002/cpt.2914 37079433 15. Rivera DR Henk HJ Garrett-Mayer E The friends of cancer research real-world data collaboration pilot 2.0: methodological recommendations from oncology case studies Clin Pharmacol Ther 2022 111 1 283 292 10.1002/cpt.2453 34664259 PMC9298732 16. Castelo-Branco L Pellat A Martins-Branco D Esmo guidance for reporting oncology real-world evidence (GROW) Ann Oncol 2023 Dec 34 12 1097 1112 10.1016/j.annonc.2023.10.001 37848160  • This guidance provides reporting guidelines for oncology real-world evidence. Our work is significant because it addresses the broader published literature on RWE, beyond specific guidelines. 17. National Pharmaceutical Council What’s in your RWE evaluation toolbox? cited 2024 Nov 7 Available from: https://www.npcnow.org/resources/whats-your-rwe-evaluation-toolbox 18. Chen S Graff J Yun S Online tools to synthesize real-world evidence of comparative effectiveness research to enhance formulary decision making J Manag Care Spec Pharm 2021 Jan 27 1 95 104 10.18553/jmcp.2021.27.1.095 33377442 PMC10391288 19. Wang SV Pinheiro S Hua W Start-RWE: structured template for planning and reporting on the implementation of real world evidence studies BMJ 2021 Jan 12 372 m4856 10.1136/bmj.m4856 33436424 PMC8489282 20. Wang SV Pottegard A Crown W Harmonized protocol template to enhance reproducibility of hypothesis evaluating real-world evidence studies on treatment effects: a good practices report of a joint ISPE/ISPOR task force Value Health 2022 Oct 25 10 1663 1672 10.1016/j.jval.2022.09.001 36241338  • This report presents a harmonized protocol template to enhance the reproducibility of hypothesis-evaluating RWE studies on treatment effects. Our work is important as it builds on such standardized tools by examining the published literature. 21. European Network of Centres for Pharmacoepidemiology and Pharmacovigilance ENCePP checklist for study protocols cited 2024 Nov 15 Available from: https://encepp.europa.eu/encepp-toolkit/encepp-checklist-study-protocols_en 22. European Network of Centres for Pharmacoepidemiology and Pharmacovigilance Encepp guide on methodological standards in pharmacoepidemiology [cited Nov 15 2024] https://encepp.europa.eu/encepp-toolkit/methodological-guide_en 23. International Society for Pharmacoepidemiology Guidelines for good pharmacoepidemiology practices (GPP) cited 2024 Nov 14 Available from: https://www.pharmacoepi.org/resources/policies/guidelines-08027/ 24. Capkun G Corry S Dowling O Can we use existing guidance to support the development of robust real-world evidence for health technology assessment/payer decision-making? Int J Technol Assess Health Care 2022 Nov 2 38 1 e79 10.1017/S0266462322000605 36321447 25. Jiu L Hartog M Wang J Tools for assessing quality of studies investigating health interventions using real-world data: a literature review and content analysis BMJ Open 2024 Feb 13 14 2 e075173 10.1136/bmjopen-2023-075173 PMC10868255 38355183  •• This literature review and content analysis by Jiu et al. focuses on assessing the quality of studies investigating health interventions using real-world data. Our work complements this by addressing the published studies on RWE. 26. Khambholja K Gehani M Use of structured template and reporting tool for real-world evidence for critical appraisal of the quality of reporting of real-world evidence studies: a systematic review Value Health 2023 Mar 26 3 427 434 10.1016/j.jval.2022.09.003 36210293 27. European Society For Medical Oncology ESMO-GROW checklist for authors and reviewers [cited Nov 15 2024] https://www.esmo.org/content/download/775073/18293338/1/ESMO-GROW-Checklist.pdf 28. Bruggesser S Stockli S Seehra J The reporting adherence of observational studies published in orthodontic journals in relation to STROBE guidelines: a meta-epidemiological assessment Eur J Orthod 2023 Feb 10 45 1 39 44 10.1093/ejo/cjac045 35968661 29. Ghaferi AA Schwartz TA Pawlik TM Strobe reporting guidelines for observational studies JAMA Surg 2021 Jun 1 156 6 577 578 10.1001/jamasurg.2021.0528 33825815 30. Vandenbroucke JP von Elm E Altman DG Strengthening the reporting of observational studies in epidemiology (STROBE): explanation and elaboration Ann Intern Med 2007 Oct 16 147 8 W163 94 10.7326/0003-4819-147-8-200710160-00010-w1 17938389 31. White R Building trust in real world evidence (RWE): moving transparency in RWE towards the randomized controlled trial standard Curr Med Res Opin 2023 Dec 39 12 1737 1741 10.1080/03007995.2023.2263353 37787381 32. Desai Sc K Ru M Reynolds B An oncology real-world data assessment framework for outcomes research Value In Health 2021 24 1 S25 10.1016/j.jval.2021.04.129 33. Sola-Morales O Sigurethardottir K Akehurst R Data governance for real-world data management: a proposal for a checklist to support decision making Value Health 2023 Apr 26 4 32 42 10.1016/j.jval.2023.02.012 36870678  • This study proposes a checklist for data governance in real-world data management. It highlights the need for standardized approaches, which our work aims to address by focusing on the published literature. 34. American Academy of Pediatric Dentistry Grade framework in systematic reviews Available from: https://www.aapd.org/globalassets/aapd-grade 35. Dreyer NA Schneeweiss S McNeil BJ The GRACE principles: recognizing high-quality observational studies of comparative effectiveness Am J Manag Care 2010 16 6 467 471 Available from: https://www.ajmc.com/view/ajmc_10jundreyer_467to4711 20560690 36. Fleurence RL Kent S Adamson B Assessing real-world data from electronic health records for health technology assessment: the SUITABILITY checklist: a good practices report of an ISPOR task Force Value Health 2025 27 6 692 701 Available from: https://www.ispor.org/publications/journals/value-in-health/abstract/Volume-27–Issue-6/Assessing-Real-World-Data-From-Electronic-Health-Records-for-Health-Technology-Assessment–The-SUITABILITY-Checklist–A-Good-Practices-Report-of-an-ISPOR-Task-Force 10.1016/j.jval.2024.01.019 PMC11182651 38871437 37. Motheral B Brooks J Clark MA A checklist for retroactive database studies – report of the ISPOR task Force on retrospective databases Value Health 2003 6 2 90 97 10.1046/j.1524-4733.2003.00242.x 12641858 38. Berger ML Martin BC Husereau D A questionnaire to assess the relevance and credibility of observational studies to inform health care decision making: an ISPOR-AMCP-NPC Good Practice Task Force report Value Health 2014 Mar 17 2 143 156 10.1016/j.jval.2013.12.011 24636373 PMC4217656 39. Real World Transparency Initiative Real world evidence registry Available from: https://osf.io/registries/rwe/discover 40. Castellanos EH Wittmershaus BK Chandwani S Raising the bar for real-world data in oncology: approaches to quality across multiple dimensions JCO Clin Cancer Inf 2024 Jan 19 8 8 e2300046 10.1200/CCI.23.00046 PMC10807898 38241599 41. Boateng D Kumke T Vernooij R GetReal Initiative. Validation of the GetReal trial tool – facilitating discussion and understanding more pragmatic design choices and their implications Contemp Clin Trials 2023 125 107054 10.1016/j.cct.2022.107054 36529438 42. Gatto NM Campbell UB Rubinstein E The structured process to identify fit-for-purpose data: a data feasibility assessment framework Clin Pharmacol Ther 2022 Jan 111 1 122 134 10.1002/cpt.2466 34716990 PMC9299818 ",
  "metadata": {
    "Title of this paper": "The structured process to identify fit-for-purpose data: a data feasibility assessment framework",
    "Journal it was published in:": "Future Oncology",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12490408/"
  }
}
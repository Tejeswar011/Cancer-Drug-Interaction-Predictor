{
  "title": "Paper_35",
  "abstract": "pmc PLoS One PLoS One 440 plosone plos PLOS One 1932-6203 PLOS PMC12494260 PMC12494260.1 12494260 12494260 41042755 10.1371/journal.pone.0331404 PONE-D-24-58388 1 Research Article Medicine and Health Sciences Clinical Medicine Signs and Symptoms Lesions Physical Sciences Mathematics Numerical Analysis Interpolation Medicine and Health Sciences Medical Conditions Skin Diseases Medicine and Health Sciences Dermatology Skin Diseases Research and Analysis Methods Imaging Techniques Engineering and Technology Signal Processing Image Processing Medicine and Health Sciences Diagnostic Medicine Medicine and Health Sciences Dermatology Computer and Information Sciences Software Engineering Preprocessing Engineering and Technology Software Engineering Preprocessing Diffusion-based skin disease data augmentation with fine-grained detail preservation and interpolation for data diversity Skin disease data augmentation using diffusion models https://orcid.org/0009-0006-7729-3624 Kim Mujung Conceptualization Formal analysis Methodology Software Validation Visualization Writing ‚Äì original draft  1 Yoo Jisang Conceptualization Project administration Supervision Writing ‚Äì review & editing  1 * https://orcid.org/0000-0001-6595-6415 Kwon Soonchul Conceptualization Methodology Visualization Writing ‚Äì review & editing  2 * Kim Byung Jun Data curation Validation Writing ‚Äì review & editing  3 Pak Changsik John Data curation  4 Won Chong Hyun Data curation  5 Moon Suk-Ho Data curation  6 Song Woo Jin Validation  7 Cha Han Gyu Validation  8 https://orcid.org/0000-0002-0131-3565 Park Kyung Hee Validation  9 1 Department of Electronic Engineering, Kwangwoon University, Seoul, Republic of Korea 2 Graduate School of Smart Convergence, Kwangwoon University, Seoul, Republic of Korea 3 Department of Reconstructive and Plastic Surgery, Seoul National University Hospital, Seoul, Republic of Korea 4 Department of Plastic and Reconstructive Surgery, Asan Medical Center, Seoul, Republic of Korea 5 Department of Dermatology, Asan Medical Center, Seoul, Republic of Korea 6 Department of Plastic and Reconstructive Surgery, Seoul St. Mary‚Äôs Hospital, Seoul, Republic of Korea 7 Department of Plastic and Reconstructive Surgery, Soonchunhyang University Hospital, Seoul, Republic of Korea 8 Department of Plastic Surgery and Rehabilitation Medicine, Soonchunhyang University Hospital, Bucheon, Republic of Korea 9 Department of Nursing Science, The University of Suwon, Hwaseong, Republic of Korea Wang Zeheng Editor  Commonwealth Scientific and Industrial Research Organisation, AUSTRALIA * E-mail: jsyoo@kw.ac.kr ksc0226@kw.ac.kr Competing Interests: 3 10 2025 2025 20 10 498120 e0331404 19 12 2024 14 8 2025 03 10 2025 04 10 2025 04 10 2025 ¬© 2025 Kim et al 2025 Kim et al https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Diffusion-based skin disease data augmentation with fine-grained detail preservation and interpolation for data diversity. 20 10 3 10 2025 e0331404 e0331404 PLoS One 10.1371/journal.pone.0331404 41042755 Diffusion-based skin disease data augmentation with fine-grained detail preservation and interpolation for data diversity. 20 10 3 10 2025 e0331404 e0331404 PLoS One 10.1371/journal.pone.0331404 41042755 We propose a data augmentation technique utilizing a Diffusion-based generative deep learning model to address the issue of data scarcity in skin disease diagnosis research. Specifically, we enhanced the Stable Diffusion model, a Latent Diffusion Model (LDM), to generate high-quality synthetic images. To mitigate detail loss in existing Diffusion models, we incorporated lesion area masks and improved the encoder and decoder structures of the LDM. Multi-level embeddings were applied using a CLIP encoder-based image encoder to capture detailed representations, ranging from textures to overall shapes. Additionally, we employed pre-trained segmentation and inpainting models to generate normal skin regions and used interpolation techniques to synthesize synthetic images with gradually varying visual characteristics, while having limitations for clinical use, this approach contributes to enhanced data diversity and can be used as reference material. To validate our method, we conducted classification experiments on seven skin diseases using datasets combining synthetic and real images. The results showed improvements in classification performance, demonstrating the effectiveness of the proposed technique in addressing medical data scarcity and enhancing diagnostic accuracy. http://dx.doi.org/10.13039/501100014188 Ministry of Science and ICT, South Korea IITP-2025-RS-2023-00258639 https://orcid.org/0000-0001-6595-6415 Kwon Soonchul http://dx.doi.org/10.13039/501100002643 Kwangwoon University 2024-0174 Yoo Jisang This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2025-RS-2023-00258639) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation). And, the present research has been conducted by the Research Grant of Kwangwoon University in 2024. There was no additional external funding received for this study. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes Data Availability All image files are available from the HAM10000 database ( https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T https://data.mendeley.com/datasets/zr7vgbcyr2/1 https://github.com/raddshing/skin-disease-diffusion Data Availability All image files are available from the HAM10000 database ( https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T https://data.mendeley.com/datasets/zr7vgbcyr2/1 https://github.com/raddshing/skin-disease-diffusion Introduction In recent years, with the advancement of deep learning-based image processing technology, the potential applications of this technology have gained attention across various fields. In the medical field, attempts to utilize deep learning technology for diagnosis, education, and research are actively being made, with particular focus on research applying deep learning models to tasks such as diagnosis [ 1 3 4 8 9 11 Skin diseases represent a significant global health burden, affecting 30-70% of individuals worldwide [ 12 13 Deep learning models in the medical field require accurate and effective training, as even slight differences in performance can lead to severe consequences in real-world outcomes. To ensure such accuracy and effectiveness, it is essential to secure high-quality data that adequately reflects disease characteristics. However, collecting medical data poses several challenges [ 14 15 To address these limitations, generative deep learning models, including GANs [ 16 17 18 19 20 21 22 23 24 25 26 29 18 19 Previous works have demonstrated the potential of generative models for medical data synthesis. For example, [ 30 31 32 33 34 However, existing models still face challenges, such as losing fine-grained details during image generation. This can be a critical drawback, especially in the medical domain where detailed characteristics such as disease patterns, textures, and shapes are crucial. Since even subtle differences in medical images can significantly impact actual diagnostic results. Addressing these limitations is essential for generating synthetic medical images that accurately represent disease-specific features and ensure their utility in clinical and diagnostic workflows. In this study, we propose an improved method based on the Latent Diffusion Model(LDM) [ 18 15 35 36 38 To validate the effectiveness of our proposed methodology, we performed classification downstream tasks combining synthetic and real data using classification models ranging from common ones such as VGG [ 40 41 42 43 44 45 While this study is similar to ELITE in utilizing multi-embeddings to improve detailed expressions in generated images, there are several key differences. While ELITE focused on personalized text-to-image generation task, our study aims at medical data augmentation and conducted experiments in the medical domain of skin disease data rather than in general domains. Additionally, while ELITE emphasized text-to-image learning by mapping detailed expression embeddings to text space through a separate mapping network, our study focused on visual learning using visual tokens. Our contributions are as follows: To overcome the limitations of medical data with restricted data constraints, we generated high-quality synthetic data using a Diffusion-based model for seven classes in the skin disease domain for medical data augmentation. We validated the effectiveness of the synthesized skin data through classification downstream tasks. To compensate for the potential loss of detailed expressions in medical image generation, we introduced an improved Diffusion method that applies multi-level embeddings to Diffusion learning. We generated synthetic skin lesion images by utilizing the segmentation masks of the synthesized samples and inpainting to extract normal region images, followed by latent space interpolation between normal and synthesized lesions. This approach enabled the creation of synthetic images with smoothly varying visual characteristics that, while not fully aligned with clinically grounded severity concepts, can be utilized for reference purposes and to enhance data diversity. Related works Medical data augmentation Data augmentation [ 46 To address these issues, specialized deep learning-based data augmentation techniques for the medical domain have been actively researched recently. Generative Adversarial Networks (GANs) [ 16 48 50 47 51 52 48 53 54 55 56 58 59 18 60 61 62 34 18 et al. 63 In this study, we conducted our study using Stable Diffusion, a Diffusion-based model showing outstanding performance in the medical domain, for generating synthetic images of skin diseases. Diffusion probabilistic models Diffusion models [ 17 64 64 Denoising Diffusion Probabilistic Models (DDPM) [ 17 16 To address this limitation, Denoising Diffusion Implicit Models (DDIM) [ 65 Score-Based Generative Modeling through SDE [ 66 Latent Diffusion Models (LDM) [ 18 38 In this study, inspired by existing LDM-based studies for high-resolution medical image data generation, we conducted experiments on VAE structure modification suitable for medical image data and methods to reduce detail loss, based on Stable Diffusion, an LDM model. Severity-based medical data While data augmentation techniques have been widely studied, research on severity-based medical image generation remains relatively limited. Existing studies have primarily focused on severity classification tasks [ 67 69 The Uncertainty-Guided Diffusion Models (UGDM) [ 70 71 Despite these efforts, research specifically addressing image generation based on severity remains limited. To complement this gap, this paper applies existing techniques, such as segmentation masks and inpainting, to design a method for generating synthetic images through latent space interpolation between normal and lesion states. This approach produces images with smoothly varying visual characteristics that, while not fully aligned with clinically grounded severity concepts, can serve as reference samples and contribute to enhanced data diversity in visual characteristics of generated skin lesion images. Materials and methods Overview This study proposes a novel framework for Diffusion-based data augmentation that preserves fine details, reduces artifacts in generated images, and enhances visual diversity through latent space interpolation latent space interpolation. Fig 1 10.1371/journal.pone.0331404.g001 Fig 1 The workflow of our method. In Stage 1, the VAE latent channels in the existing Stable Diffusion model are expanded from 4 to 8, and the VAE is pre-trained to enhance representational capacity. In Stage 2, the pre-trained 8-channel VAE is used to configure the input and output layers. For detailed representation learning, a pre-trained CLIP image encoder extracts multi-level embeddings at five scales. These embeddings are injected into the denoising UNet during the diffusion process through additional adapter layers, leading to the generation of the final synthesized images. We performed pre-training by expanding the existing 4-channel VAE [ 38 18 Prior to training, the model input data is preprocessed using lesion masks. This process extracts skin lesion areas and removes unnecessary backgrounds, designed to allow the model to focus more on the key features of lesions. Extracted lesion images are transformed into multi-level embeddings using the CLIP [ 36 In the image generation process, latent space interpolation is utilized to generate synthetic samples with gradually varying visual characteristics. To achieve this, normal images with the same context as the normal regions of the generated skin disease samples are created using pre-trained segmentation [ 72 73 This design goes beyond simple image synthesis and contributes to providing data that is useful for diagnosis and analysis. Each component is explained in detail in the following sections. Datasets In this study, we utilized the HAM10000 [ 15 Actinic keratoses/intraepidermal carcinoma (AKIEC) ‚Äì Clinical: Non-invasive variants of squamous cell carcinoma that can be treated locally without surgery. More common on sun-exposed areas, with actinic keratoses typically on the face and Bowen‚Äôs disease on other body sites. ‚Äì Morphological: Commonly show surface scaling and are often devoid of pigment. May progress to invasive squamous cell carcinoma. Basal cell carcinoma (BCC) ‚Äì Clinical: Common epithelial skin cancer that rarely metastasizes but grows destructively if untreated. ‚Äì Morphological: Appears in different variants (flat, nodular, pigmented, cystic) with varying dermatoscopic presentations. Benign keratosis-like lesions (BKL) ‚Äì Clinical: Generic class including seborrheic keratoses (‚Äúsenile wart‚Äù), solar lentigines, and lichen-planus like keratoses (LPLK). ‚Äì Morphological: Show inflammation and regression patterns. Particularly challenging dermatoscopically as they can mimic melanoma features, often requiring biopsy for diagnostic certainty. Dermatofibroma (DF) ‚Äì Clinical: Benign skin lesion considered either a benign proliferation or inflammatory reaction to minimal trauma. ‚Äì Morphological: Most commonly presents with reticular lines at the periphery and a central white patch denoting fibrosis. Melanoma (MEL) ‚Äì Clinical: Malignant neoplasm derived from melanocytes. Can be cured by simple surgical excision if detected early. Includes all variants including melanoma in situ. ‚Äì Morphological: Usually chaotic in appearance with asymmetric distribution of colors and structures. Specific criteria depend on anatomic site. Melanocytic nevi (NV) ‚Äì Clinical: Benign neoplasms of melanocytes appearing in multiple variants. ‚Äì Morphological: In contrast to melanoma, usually symmetric with regard to distribution of color and structure. Variants may differ significantly from a dermatoscopic perspective. Vascular lesions (VASC) ‚Äì Clinical: Range from cherry angiomas to angiokeratomas and pyogenic granulomas. Include hemorrhage in this category. ‚Äì Morphological: Dermatoscopically characterized by red or purple color and solid, well-circumscribed structures known as red clods or lacunes. An example of the HAM10000 dataset is shown in Fig 2 10.1371/journal.pone.0331404.g002 Fig 2 An example of the HAM10000 dataset. Consisting of 10,015 images across seven skin tumor classes. Enhanced Variational Autoencoder (8-channel VAE) The Stable Diffusion models [ 18 38 Medfusion [ 34 15 The loss functions used during training are as follows: ‚Ñí VAE = Œª rec ( ‚Ñí 1 ( ùê± , ùê± ^ ) + SSIM ( ùê± , ùê± ^ ) ) + Œª KL D KL ( q œï ( ùê≥ | ùê± ) ‚Äñ p ( ùê≥ ) ) + Œª perc LPIPS ( ùê± , ùê± ^ ) (1) The loss function of this VAE model consists of Reconstruction Loss, Latent Space Regularization Loss, and Perceptual Loss. First, the reconstruction loss focuses on minimizing the difference between the original and reconstructed images. For this purpose, it uses L1 loss (absolute value loss) to calculate pixel-wise differences and employs Structural Similarity Loss(SSIM Loss) [ 74 75 The pre-trained 8-channel VAE is used at the input and output stages of the Stable Diffusion model, with the VAE weights remaining fixed during the diffusion training process. This approach demonstrated that while minimizing the VAE‚Äôs impact on the quality and detailed representation of generated images, it can support stable and high-quality data generationwith reduced artifacts. Preserving visual details through multi-level embeddings Latent Diffusion Models (LDM) [ 18 First, to eliminate unnecessary background regions and focus on the characteristics of the lesion area, we generate images containing only the lesion region using lesion segmentation masks. The lesion extraction image is defined as follows: ùê± l : = ùê± i ¬∑ ùê¶ i , (2) where x i m i x l The extracted lesion images are then processed through a pre-trained CLIP [ 36 37 We extract hidden representations from transformer blocks at layers { 5 , 11 , 17 , 23 , 31 } 224 √ó 224 Œº = [ 0.48 , 0.46 , 0.41 ] œÉ = [ 0.27 , 0.26 , 0.28 ] For each selected layer l k ùê° ( l ) = [ ùê° CLS ( l ) , ùê° patch ( l ) ] , ùê° patch ( l ) = top- k ( { ùê© i ( l ) } i = 1 N , ‚Äñ ùê© i ( l ) ‚Äñ 2 ) (3) where N k Each layer‚Äôs extracted tokens (concatenated CLS + top-32 patches, resulting in 33 tokens per layer) are then projected through a two-layer MLP with SiLU activation: ùêû l = MLP l ( ùê° ( l ) ) = W 2 ( l ) ¬∑ SiLU ( W 1 ( l ) ¬∑ ùê° ( l ) ) (4) where W 1 ( l ) ‚àà ‚Ñù d CLIP √ó 1024 W 2 ( l ) ‚àà ‚Ñù 1024 √ó 1024 d proj = 1024 By extracting features from five different transformer layers, we capture a hierarchy of visual representations: early layers (e.g., layer 5) capture low-level features such as textures and color patterns, intermediate layers (e.g., layers 11, 17) capture local structures and patterns, while deeper layers (e.g., layers 23, 31) capture high-level semantic features representing overall lesion morphology and structure. Fig 3 10.1371/journal.pone.0331404.g003 Fig 3 Multi-level feature visualization of skin lesion images. PCA projection of features extracted from all 32 layers (0-31) of the OpenCLIP ViT-H/14 visual encoder into RGB space. Each layer‚Äôs feature map is projected using the first three principal components mapped to RGB channels. The pronounced color variations between adjacent patches indicate the model‚Äôs ability to distinguish different regions such as lesion boundaries and morphological structures, with deeper layers showing more prominent inter-patch differences for fine-grained detail representation. During the diffusion training process, visual tokens are extracted from the raw RGB images (before latent encoding) and injected into the U-Net encoder through lightweight adapter modules. The U-Net architecture consists of 4 resolution levels with hidden dimensions { 256 , 256 , 512 , 1024 } i ùêØ i = MLP i ( ùêû l ) = W 2 ( i ) ¬∑ SiLU ( W 1 ( i ) ¬∑ ùêû l ) (5) where the MLP projects from d proj = 1024 d i ‚àà { 256 , 256 , 512 , 1024 } Inspired by GLIGEN [ 77 ùê° ‚Ä≤ = ùê° + Œ≤ ¬∑ Adapter ( ùê° , ùêØ i ) (6) where ùê° ‚àà ‚Ñù B √ó H W √ó d i Œ≤ = 0.1 H W ‚â§ 4096 Unlike ELITE which maps CLIP features to the text embedding space and modifies cross-attention mechanisms, our approach directly injects visual information into the U-Net‚Äôs visual feature stream through these lightweight adapters. Latent space interpolation-based image generation In this study, we propose a novel approach based on segmentation and inpainting to generate a pair of disease and corresponding pseudo-normal images, followed by latent space interpolation to produce images with smoothly varying visual characteristics. The process is illustrated in Fig 4 10.1371/journal.pone.0331404.g004 Fig 4 Latent space interpolation-based sampling workflow. Disease images are generated from noise using a diffusion model and then segmented to isolate lesion regions. These regions are inpainted to obtain pseudo-normal counterparts while preserving skin context. Both disease and pseudo-normal images are mapped to the latent space via a pre-trained VAE. Latent interpolation is then performed between the two extremes. While this produces visually smooth transitions, the interpolation axis reflects a heuristic binary lesion‚Äìnormal direction rather than a clinically grounded severity concept. First, sample images for each class are generated using a pre-trained Latent Diffusion Model (LDM). These generated sample images are defined as x disease Next, a segmentation model, MFSNet [ 72 x disease x m ùê± m = MFSNet ( ùê± disease ) , (7) Subsequently, an inpainting model, Large Mask Inpainting (LaMa) [ 73 x normal ùê± normal = LaMa ( ùê± disease ‚äô ( 1 ‚àí ùê± m ) ) . (8) To enable interpolation in the latent space, both normal and diseased images are mapped to the latent space using latent space inversion. A pre-trained 8-channel VAE encoder is used to encode x normal x disease ùê≥ normal = ‚Ñ∞ ( ùê± normal ) , ùê≥ disease = ‚Ñ∞ ( ùê± disease ) , (9) where z normal z disease Latent space interpolation is performed by linearly combining the latent representations of the two states (normal and diseased): ùê≥ interpolated = ( 1 ‚àí Œ± ) ùê≥ normal + Œ± ùê≥ disease , Œ± ‚àà [ 0 , 1 ] , (10) where Œ± Finally, the interpolated latent representations are decoded back to the image space using the VAE decoder of the diffusion model: ùê± interpolated = ùíü ( ùê≥ interpolated ) . (11) Through this process, we generate synthetic images with gradually varying visual characteristics, starting from the normal image. This approach combines segmentation, inpainting, and latent space interpolation to represent continuous transitions between normal and diseased states. While effective for data augmentation, the interpolation rests on a binary normal‚Äìlesion axis, so the generated sequence should be regarded as a heuristic visual progression rather than a rigorously calibrated severity ladder. Implementation details Stable Diffusion v1-4 served as the base model, but several components were modified to align with the objectives of this study. The original CLIP text encoder was removed, and conditioning is now provided solely through class labels. Specifically, we employ a learnable embedding layer that maps class labels (0: AKIEC, 1: BCC, 2: BKL, 3: DF, 4:NV, 5: MEL, 6:VASC) to 1024-dimensional embedding vectors, which are then integrated into the model through the retained cross-attention blocks. This class-conditional approach enables the model to generate lesion-specific features while maintaining the architectural benefits of the original Stable Diffusion framework. To ensure a fair comparison, both the baseline and proposed methods underwent identical architectural modifications: the same LabelEmbedder for class conditioning, identical UNet configuration (8-channel input/output with hidden channels [256, 256, 512, 1024]), and matching training hyperparameters. The VAE was pre-trained with the Adam optimizer to encode 256 √ó 256 images into a 32 √ó 32 latent space (down-sampling factor 8), using a weighted sum of KL, L1, L2 and SSIM losses (batch 16).Two latent-width variants are compared: the default 4-channel setting and an 8-channel setting that doubles capacity while keeping the spatial size unchanged. The baseline model maintains the original Stable Diffusion‚Äôs 4-channel VAE configuration, while our proposed method employs the 8-channel variant to enhance representational capacity for medical imaging features. The diffusion process consists of a forward stage that perturbs latents with Gaussian noise over 1 000 steps and a backward stage that denoises them. The UNet is trained with AdamW (batch 16, learning-rate 1 √ó 10 ‚àí 4 The noise scheduler employs a scaled linear beta schedule ( Œ≤ start = 0.002 Œ≤ end = 0.02 During sampling, we employed class-specific strategies to optimize generation quality for each skin lesion type. Instead of using a uniform approach, we varied the sampling method (DDPM or DDIM), classifier-free guidance scale (3.0 or 5.0), and number of denoising steps (50-1000) based on empirical observations of each class‚Äôs generation characteristics. For instance, melanoma (MEL) generation utilized DDPM with 1000 steps and cfg=5.0, while benign lesions like BCC employed faster DDIM sampling with 100 steps and cfg=3.0. This adaptive sampling strategy was determined experimentally to achieve optimal visual quality for each lesion type while balancing computational efficiency. VAE training was conducted using both two NVIDIA RTX 3090 GPUs (24GB each) and Colab A100 GPU(40GB). The VAE was trained with a fixed learning rate of 1e-4, while the diffusion model employed a cosine annealing schedule with warm-up. All experiments were implemented in Python 3.9.19 with PyTorch 1.12 and CUDA 11.4 on a 2.9 GHz Intel Core i7-10700 processor. Results Reconstruction with 8-channel VAE In this section, we conducted reconstruction quality experiments in the HAM10000 skin disease domain using the proposed 8-channel VAE approach. The 8-channel VAE was designed to expand the representational capacity of the latent space compared to the conventional 4-channel VAE, aiming to reduce artifacts. We applied this architecture to the HAM10000 dataset to evaluate the reconstruction quality of skin disease data. We compared the performance of base model‚Äôs 4-channel VAE and 8-channel VAE used in this study. both quantitatively and visually, demonstrating that the 8-channel VAE showed superior results on the HAM10000 dataset. Fig 5 10.1371/journal.pone.0331404.g005 Fig 5 Visual comparison of reconstruction results: 4-Channel VAE vs. 8-Channel VAE. In the areas marked with red circles, the 4-channel VAE exhibits subtle artifacts, whereas the 8-channel VAE demonstrates improvements, mitigating these issues. Table 1 75 76 10.1371/journal.pone.0331404.t001 Table 1 Quantitative comparison between 4ch VAE vs 8ch VAE. In the comparison of reconstructed image quality, the 8-channel VAE showed superior results across MSE, LPIPS, and MS-SSIM metrics. MSE(10 ‚àí3 LPIPS‚Üë MS-SSIM‚Üë 4ch-VAE 0.38 0.91 0.95 8ch-VAE  0.21  0.94  0.97 In conclusion, the 8-channel VAE outperformed the conventional 4-channel VAE in terms of pixel accuracy, perceptual similarity, and structural preservation. These results demonstrate that the 8-channel VAE effectively restores detailed representations of lesions while reducing artifacts. This improvement in image quality can enhance the reliability of diagnostic models, providing a significant contribution to medical imaging applications. Evaluation of synthetic image quality and fine detail preservation We conducted visual and quantitative evaluations of synthetic images generated by the improved Diffusion model with multi-level embeddings for fine detail preservation. Fig 6 Fig 7 10.1371/journal.pone.0331404.g006 Fig 6 Visual comparison between the baseline model and our method. In the ‚ÄúVASC‚Äù class, which has relatively fewer training samples, the samples generated by Stable Diffusion (baseline model) fail to accurately reflect lesion boundaries and color information when compared to the original images. Similarly, in the ‚ÄúDF‚Äù class, the baseline model produces unnatural textures, such as the keratinized surface of the lesion. In contrast, our method, utilizing multi-level embeddings, effectively learns and represents boundaries, color, and texture, resulting in more natural and faithful representations. 10.1371/journal.pone.0331404.g007 Fig 7 Generated samples for seven classes. The synthetic images for each class are visually compared to demonstrate the quality and diversity of the generated data. For the quantitative comparison of image results, we measured FID (Fr√©chet Inception Distance) [ 78 79 Table 2 80 80 10.1371/journal.pone.0331404.t002 Table 2 Quantitative comparison of image quality between the base model and the proposed method. This table presents the quantitative results of image quality (FID, IS) for the base model (Stable Diffusion) and the proposed method. While FID scores showed improvement with the proposed method, IS scores showed no significant differences between the models. This indicates that the proposed method demonstrated better performance than the baseline in terms of distributional similarity to real data. FID‚Üì IS‚Üë Stable-Diffusion(Base model) 145.26 1.77 Ours Method  99.21  2.26 To complement FID and IS, we assessed whether the synthetic images improve downstream diagnostic models. We evaluated nine different classification models, including relatively simple classifiers such as VGG13 and ResNet18, as well as modern hybrid architectures CNN-based models and Transformer-based architectures like ConvNeXt, Swin Transformer, EVA and CoAtNet. When augmented with synthetic data to ensure sufficient training samples, classification accuracy showed modest improvements compared to using limited real data alone. Detailed results of these experiments can be found in the ‚ÄúClassification downstream task‚Äù section below. Classification downstream task The primary objective of this study is to assess the effectiveness of synthetic data in medical applications, particularly for addressing data scarcity challenges in training diagnostic models. To achieve this, we conducted downstream classification experiments using both original and synthetic datasets. In these experiments, we constructed three dataset configurations to evaluate the impact of synthetic data across different scenarios. We trained nine different classification models spanning two generations of architectures: early-generation CNNs (VGG13, VGG16, VGG19, ResNet18, ResNet34) and modern deep architectures (Swin Transformer, ConvNeXt, EVA, CoAtNet), and compared their classification accuracy. The composition of each dataset is as follows: Original250: 250 original images Original500: 500 original images Synthetic500: 500 synthetic images Mixed1000: original 500 + synthetic 500 images For some classes in the original 250 and Original 500 datasets, the actual number of Original data samples was insufficient to reach 250 or 500 samples (Actinic keratoses, Dermatofibroma, Vascular lesions). The shortfall was supplemented with basic augmentation techniques such as rotation and flipping that do not affect color or brightness. Table 3 10.1371/journal.pone.0331404.t003 Table 3 Evaluation of synthetic data effectiveness in downstream classification tasks. Classification downstream tasks were conducted using four types of data consisting of origin and synthetic data. Training with synthetic images alone resulted in lower classification accuracy compared to original images. However, combining both synthetic and original data to form a larger dataset achieved the highest average classification accuracy. VGG13 VGG16 VGG19 ResNet18 ResNet34 Swin-T ConvNext EVA CoAtNet Average  Origin 250 72.71 72.86 73.57 77.14 77.86 75.71 72.86 75.86 75.00 74.84  Origin 500 81.43 79.29 80.00  83.57 80.71 80.72 76.43 85.50 82.74 80.92  Synthetic 500 74.70 74.40 79.20 75.66 77.52 80.46 75.32 77.88 77.57 76.63  Mixed 1000  82.64  82.73  85.21 80.44  83.97  85.20  83.77  87.43  84.98  84.15 The experimental results indicate that, when using the same amount of data, models trained exclusively on synthetic data exhibited slightly lower accuracy compared to those trained on original data. This discrepancy likely arises from the limitation that synthetic data cannot fully capture the complexity of real-world characteristics. However, when combining original and synthetic data to construct a more rich dataset (mixed 1000), the average classification accuracy increased from approximately 80.92% to 84.15%, compared to using original data alone. This suggests that the mixed dataset enhanced model training by improving data diversity. These results quantitatively demonstrate that while synthetic data may have limitations when used independently, combining it with original data can effectively address challenges associated with data collection. Synthetic data shows particular potential as a valuable resource for mitigating data scarcity in the medical field. However, the relative benefit from synthetic augmentation varied across architectures. While modern architectures such as Swin Transformer, ConvNeXt, EVA, and CoAtNet generally achieved higher absolute performance than earlier-generation models, certain models like EVA showed a comparatively modest improvement of less than 3% when augmented with synthetic data. This suggests that such architectures may already extract sufficiently rich representations from the original data, leaving relatively less room for improvement solely from synthetic augmentation. Cross-dataset Zero-shot Evaluation on PAD-UFES-20. To further assess the generalization capability of the classifiers trained solely on the HAM10000 dataset, we performed a zero-shot evaluation on the PAD-UFES-20 dataset [ 81 Fig 8 10.1371/journal.pone.0331404.g008 Fig 8 Example images from the PAD-UFES-20 dataset. Unlike the HAM10000 dataset, which consists of professional dermoscopic images, PAD-UFES-20 contains clinical photographs captured using smartphone cameras, introducing variations in lighting, resolution, and background. Nine classification models were evaluated zero-shot, without any fine-tuning on PAD-UFES-20. Table 4 10.1371/journal.pone.0331404.t004 Table 4 Cross-dataset zero-shot evaluation results on PAD-UFES-20. Models were trained on HAM10000 and directly evaluated on zero-shot on PAD-UFES-20 without fine-tuning. Model Accuracy (%) VGG13 49.89 VGG16 53.39 VGG19 58.07 ResNet18 51.68 ResNet34 59.06 Swin-T 62.15 ConvNeXt 61.33 EVA  64.27 CoAtNet 58.23 Ablation study Ablation on Channel Width and Multi-level Embeddings. We perform an ablation study across three variants: a 4-channel VAE (4ch), an 8-channel VAE (8ch), and an 8ch model augmented with multi-level CLIP embeddings (8ch+ML). Fig 9 10.1371/journal.pone.0331404.g009 Fig 9 Visual ablation results. Visual ablation comparison. Rows, from top to bottom, show (1) the original image, (2) the synthesis produced by the 4-channel VAE (4ch), (3) the synthesis from the 8-channel VAE (8ch), and (4) the synthesis from the 8-channel VAE with multi-level embeddings (8ch+ML). Comparing each column reveals progressively sharper vessel patterns, keratin scales, and overall texture fidelity as channel width is increased and semantic guidance is introduced. We quantitatively evaluated the performance of 4ch, 8ch, and 8ch+ML models using FID and IS metrics. As shown in Table 5 10.1371/journal.pone.0331404.t005 Table 5 Quantitative ablation study on VAE channel width and multi-level embeddings. We evaluated three variants using FID and IS metrics: 4-channel VAE (4ch), 8-channel VAE (8ch), and 8ch model with multi-level CLIP embeddings (8ch+ML). Both 8ch and 8ch+ML variants demonstrated improved FID and IS scores compared to 4ch. However, the IS score for 8ch+ML showed a slight decrease compared to the standalone 8ch model, suggesting that excessive learning of fine-grained features through multi-level embeddings may reduce sample diversity. FID‚Üì IS‚Üë 4ch 145.25 1.91 8ch 102.21  2.64 8ch+ML  99.21 2.21 Effect of Class-conditioning. To further investigate the contribution of the class-conditioning strategy, we compared image synthesis results with and without class-conditioning. For the non-conditioned setting, we completely removed class-conditioning by sampling from the unconditional branch‚Äîi.e., no class label embedding was provided‚Äîwhile keeping all other generation settings identical. For the conditional setting, we varied the classifier-free guidance (CFG) scale among 1, 3, and 5 to observe its effect on the synthesis process. Fig 10 10.1371/journal.pone.0331404.g010 Fig 10 Effect of class-conditioning and CFG scale on skin lesion synthesis. Class-wise comparison of non-conditioned generation (top row) and class-conditioned generation at different classifier-free guidance (CFG) scales (1, 3, 5) using identical random seeds. Without class-conditioning (non-cond), images lose distinctive lesion-specific characteristics, resulting in visually similar patterns across classes. Increasing CFG strengthens class-specific morphological features while enhancing overall discriminability between classes. Without class-conditioning, generated images exhibited weaker class-specific lesion patterns, color distributions, and boundary structures, often resulting in visually similar appearances across different classes. In contrast, applying class-conditioning preserved distinctive features‚Äîsuch as pigment network patterns in MEL, vascular lacunae in VASC‚Äîwith these discriminative traits becoming more pronounced as the CFG scale increased. This qualitative observation confirms that class-conditioning contributes to maintaining lesion-specific characteristics in our framework. Image generation via latent space interpolation for enhanced data diversity Latent space interpolation was used to naturally sample the transition process from normal to lesion states. By adjusting the interpolation coefficient Œ± 8 Fig 11 Œ± = 0.1 0.3 < Œ± < 0.5 Œ± = 1.0 10.1371/journal.pone.0331404.g011 Fig 11 Comparison of generated samples based on adjustments to the interpolation coefficient Œ±. This figure shows samples generated gradually changing visual characteristics by adjusting the interpolation coefficient Œ± Œ± This image generation strategy can be utilized as auxiliary data for enhancing the diversity of generated images and may contribute as reference material to support the generalization performance of diagnostic models. It plays a particularly auxiliary role in detecting early lesions and emphasizing the boundary between normal and lesion data. To evaluate whether the generated images effectively enhance data diversity in the semantic embedding space, we conducted a quantitative analysis based on OpenCLIP. Two types of synthetic image sets were used for this analysis. The first set ( A A I A A I A Table 6 Fig 12 10.1371/journal.pone.0331404.t006 Table 6 Semantic variance comparison between A A I Across all seven classes, the A I A Class Variance ( A Variance ( A I Difference (Œî) AKIEC 3.7121 4.6762 +0.9642 BCC 3.0167 3.4749 +0.4582 BKL 2.8641 5.0879 +2.2238 DF 2.5864 3.8267 +1.2403 NV 3.1895 6.0547 +2.8652 MEL 2.2666 4.6148 +2.3481 VASC 3.0511 4.7766 +1.7255 10.1371/journal.pone.0331404.g012 Fig 12 PCA projection of CLIP embeddings for synthetic image samples A A I The left panel shows all classes combined, while the right panels show each class separately. Triangular markers indicate interpolated samples ( A I A A I A Limitations Synthetic samples did not always generate realistic images. As shown in Fig 13 10.1371/journal.pone.0331404.g013 Fig 13 Failed case samples. When the influence of fine details, such as hairy regions or color information, is excessively reflected, unrealistic samples are generated. The current method excels in capturing general visual characteristics such as shape and structure, but when certain features like hair or dominant color information are excessively highlighted, the quality of the generated images can be compromised. These issues underscore the need for balanced feature learning, improved data preprocessing, or complementary techniques to enhance the realism and diversity of synthetic data. To complement these problems, we found that applying the classical DullRazor 82 Fig 14 10.1371/journal.pone.0331404.g014 Fig 14 Hair removal preprocessing using the DullRazor algorithm before image generation. The DullRazor algorithm systematically removes hair artifacts through a multi-step process: (1) converting the original hairy input image to grayscale, (2) applying a black hat filter to detect and isolate hair structures, (3) creating a binary mask of detected hair regions, and (4) removing hair regions and inpainting the underlying skin texture. This preprocessing step significantly improves the quality of generated images by eliminating hair-related artifacts that could otherwise dominate the synthesis process, resulting in cleaner and more clinically relevant synthetic skin lesion images. A second limitation is that our latent interpolation constitutes only a binary lesion interpolation. Although colour and structure intensify as the interpolation coefficient Œ± 83 Œ± œÑ Conclusions In this study, we proposed a novel diffusion-based data augmentation technique to address the challenges of data scarcity in skin disease diagnosis. By enhancing the Stable Diffusion model, we introduced an 8-channel Variational Autoencoder (VAE) to expand latent space capacity and reduce artifacts, thereby preserving fine-grained details crucial for medical imaging tasks. To further improve detail representation, multi-level embeddings extracted using a pre-trained CLIP image encoder were integrated into the diffusion denoising process through adaptive layers. Our experimental results demonstrate that the proposed method effectively generates high-quality synthetic images, improving the classification accuracy of skin disease models. Specifically, combining synthetic data with real data increased the average classification accuracy from 80.92% to 84.15%, proving the utility of our approach in mitigating data scarcity and enhancing diagnostic performance. Despite these contributions, Some synthetic images exhibited unnatural artifacts due to the overemphasis of fine details, such as hair regions and dominant color features. Moreover, when generating normal images, significant differences between lesion regions and the surrounding background sometimes prevented the creation of completely realistic normal images. To address these issues, future work could incorporate more precise segmentation techniques to accurately remove hair regions and leverage larger, high-capacity models specifically tailored for skin-related tasks, thereby improving preprocessing and achieving better feature balance. Additionally, developing more meaningful evaluation metrics tailored for medical image synthesis could help assess the quality and clinical utility of synthetic data more effectively. In addition, the proposed image generation method‚Äîbased on segmentation masks, inpainting techniques, and latent space interpolation‚Äîstill leaves clinical limitations in severity interpretability. The generated results through interpolation do not establish definitive correlations with clinical severity, and thus should be interpreted as heuristic visual progressions intended to increase data diversity rather than as indicators of actual disease severity. Moreover, currently available public data in the skin disease domain lacks severity-related information. To address this limitation, future research will explore collecting skin severity data validated by clinical specialists, quantifying severity information, and applying it to practical learning processes. In conclusion, this study presented an effective improvement to medical image data augmentation techniques by proposing a data generation method that preserves fine-grained details. As additional study, the proposed latent space interpolation-based approach, while not completely overcoming clinical limitations, can serve as auxiliary material for diverse data for enhancing visual diversity in generated samples. This study contributes to enhancing the robustness and generalization performance of skin disease diagnostic models and holds potential for expansion to other medical imaging domains. References 1 Aggarwal R Sounderajah V Martin G Ting DSW Karthikesalingam A King D et al Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis NPJ Digit Med. 2021 4 1 65 doi: 10.1038/s41746-021-00438-z 33828217 PMC8027892 2 Thirunavukarasu AJ Ting DSJ Elangovan K Gutierrez L Tan TF Ting DSW Large language models in medicine Nat Med. 2023 29 8 1930 40 doi: 10.1038/s41591-023-02448-8 37460753 3 McGenity C Clarke EL Jennings C Matthews G Cartlidge C Freduah-Agyemang H et al Artificial intelligence in digital pathology: a systematic review and meta-analysis of diagnostic test accuracy NPJ Digit Med. 2024 7 1 114 doi: 10.1038/s41746-024-01106-8 38704465 PMC11069583 4 Xue P Wang J Qin D Yan H Qu Y Seery S et al Deep learning in image-based breast and cervical cancer detection: a systematic review and meta-analysis NPJ Digit Med. 2022 5 1 19 doi: 10.1038/s41746-022-00559-z 35169217 PMC8847584 5 Gaur L Bhatia U Jhanjhi NZ Muhammad G Masud M Medical image-based detection of COVID-19 using deep convolution neural networks Multimed Syst. 2023 29 3 1729 38 doi: 10.1007/s00530-021-00794-6 33935377 PMC8079233 6 Zhang H, Liang P, Sun Z, Song B, Cheng E. CircleFormer: circular nuclei detection in whole slide images with circle queries and attention. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. 2023. p. 493‚Äì502. 7 Kang M, Ting CM, Ting FF, Phan RCW. Bgf-yolo: enhanced yolov8 with multiscale attentional feature fusion for brain tumor detection. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. 2024. p. 35‚Äì45. 8 Wolleb J, Bieder F, Sandk√ºhler R, Cattin PC. Diffusion models for medical anomaly detection. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. 2022. p. 35‚Äì45. 9 Cheng J et al Interactive medical image segmentation: a benchmark dataset and baseline arXiv preprint 2024 https://arxiv.org/abs/2411.12814 10 Ye Y et al MedUniSeg: 2D and 3D medical image segmentation via a prompt-driven universal model arXiv preprint 2024 11 Chen J et al Transunet: transformers make strong encoders for medical image segmentation arXiv preprint 2021 12 Hay RJ Johns NE Williams HC Bolliger IW Dellavalle RP Margolis DJ et al The global burden of skin disease in 2010 : an analysis of the prevalence and impact of skin conditions J Invest Dermatol. 2014 134 6 1527 34 doi: 10.1038/jid.2013.446 24166134 13 Seth D Cheldize K Brown D Freeman EF Global burden of skin disease: inequities and innovations Curr Dermatol Rep. 2017 6 3 204 10 doi: 10.1007/s13671-017-0192-7 29226027 PMC5718374 14 Thapa C Camtepe S Precision health data: requirements, challenges and existing techniques for data security and privacy Comput Biol Med. 2021 129 104130 doi: 10.1016/j.compbiomed.2020.104130 33271399 15 Tschandl P Rosendahl C Kittler H The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions Sci Data. 2018 5 180161 doi: 10.1038/sdata.2018.161 30106392 PMC6091241 16 Goodfellow I Pouget-Abadie J Mirza M Xu B Warde-Farley D Ozair S et al Generative adversarial nets Advances in Neural Information Processing Systems. 2014 27 17 Ho J Jain A Abbeel P Denoising diffusion probabilistic models Advances in Neural Information Processing Systems. 2020 33 6840 51 18 Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B. High-resolution image synthesis with latent diffusion models. 2022. 19 Saharia C et al Photorealistic text-to-image diffusion models with deep language understanding Advances in Neural Information Processing Systems. 2022 35 36479 94 20 Wang Y, et al. SinSR: diffusion-based image super-resolution in a single step. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 21 Vishen S Sarabu J Bharathulwar C Lakshmanan R Srinivas V Advancing super-resolution in neural radiance fields via variational diffusion strategies arXiv preprint 2024 https://arxiv.org/abs/2410.18137 22 Kawar B a h j a t, et al. Imagic: text-based real image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. 23 Zhang Z, Han L, Ghosh A, Metaxas DN, Ren J. Sine: single image editing with text-to-image diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. p. 6027‚Äì37. 24 Chung J, Hyun S, Heo JP. Style injection in diffusion: a training-free approach for adapting large-scale diffusion models for style transfer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. p. 8795‚Äì805. 25 Li S DiffStyler: diffusion-based localized image style transfer arXiv preprint 2024 26 Konz N, et al. Anatomically-controllable medical image generation with segmentation-guided diffusion models. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. 2024. 27 Khader F M√ºller-Franzes G Tayebi Arasteh S Han T Haarburger C Schulze-Hagen M et al Denoising diffusion probabilistic models for 3D medical image generation Sci Rep. 2023 13 1 7303 doi: 10.1038/s41598-023-34341-2 37147413 PMC10163245 28 Friedrich P, Wolleb J, Bieder F, Durrer A, Cattin PC. Wdm: 3D wavelet diffusion models for high-resolution medical image synthesis. In: MICCAI Workshop on Deep Generative Models. 2024. p. 11‚Äì21. 29 Yoon JS, Zhang C, Suk HI, Guo J, Li X. Sadm: Sequence-aware diffusion model for longitudinal medical image generation. In: International Conference on Information Processing in Medical Imaging. 2023. p. 388‚Äì400. 30 Su Q i c h e n et al A GAN-based data augmentation method for imbalanced multi-class skin lesion classification IEEE Access. 2024 31 Lin CH, Yumer E, Wang O, Shechtman E, Lucey S. St-gan: Spatial transformer generative adversarial networks for image compositing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. p. 9455‚Äì64. 32 Farooq MA Yao W Schukat M Little MA Corcoran P Derm-t2im: harnessing synthetic skin lesion data via stable diffusion models for enhanced skin disease classification using ViT and CNN arXiv preprint 2024 https://arxiv.org/abs/2401.05159 10.1109/EMBC53108.2024.10781852 40039715 33 Akrout M, et al. Diffusion-based data augmentation for skin disease classification: impact across original medical datasets to fully synthetic images. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, 2023. 34 M√ºller-Franzes G Niehues JM Khader F Arasteh ST Haarburger C Kuhl C et al A multimodal comparison of latent denoising diffusion probabilistic models and generative adversarial networks for medical image synthesis Sci Rep. 2023 13 1 12098 doi: 10.1038/s41598-023-39278-0 37495660 PMC10372018 35 Wei Y, et al. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023. 36 Radford A, et al. Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning. 2021. 37 Cherti M, Beaumont R, Wightman R, Wortsman M, Ilharco G, Gordon C, et al. Reproducible scaling laws for contrastive language-image learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. p. 2818‚Äì29. 38 Kingma DP Auto-encoding variational bayes arXiv preprint 2013 https://arxiv.org/abs/1312.6114 39 Ho J Salimans T Classifier-free diffusion guidance arXiv preprint 2022 https://arxiv.org/abs/2207.12598 40 Simonyan K Very deep convolutional networks for large-scale image recognition arXiv preprint 2014 https://arxiv.org/abs/1409.1556 41 He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. 42 Liu Z, Mao H, Wu CY, Feichtenhofer C, Darrell T, Xie S. A convnet for the 2020 s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 11976‚Äì86. 43 Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, et al. Swin transformer: hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. p. 10012‚Äì22. 44 Fang Y, et al. Eva: exploring the limits of masked visual representation learning at scale. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. 45 Dai Z et al Coatnet: marrying convolution and attention for all data sizes Advances in Neural Information Processing Systems. 2021 34 3965 77 46 Shorten C Khoshgoftaar TM Furht B Text data augmentation for deep learning J Big Data. 2021 8 1 101 doi: 10.1186/s40537-021-00492-0 34306963 PMC8287113 47 Anicet Zanini R Luna Colombini E Parkinson‚Äôs disease EMG data augmentation and simulation with DCGANs and style transfer Sensors (Basel). 2020 20 9 2605 doi: 10.3390/s20092605 32375217 PMC7248755 48 Hammami M, Friboulet D, Kechichian R. Cycle GAN-based data augmentation for multi-organ detection in CT images via YOLO. In: 2020 IEEE International Conference on Image Processing (ICIP). 2020. 49 Chaudhari P Agrawal H Kotecha K Data augmentation using MG-GAN for improved cancer classification on gene expression data Soft Computing. 2020 24 11381 91 50 Pang T Wong JHD Ng WL Chan CS Semi-supervised GAN-based radiomics model for data augmentation in breast ultrasound mass classification Comput Methods Programs Biomed. 2021 203 106018 doi: 10.1016/j.cmpb.2021.106018 33714900 51 Radford A Unsupervised representation learning with deep convolutional generative adversarial networks arXiv preprint 2015 52 Gatys LA A neural algorithm of artistic style arXiv preprint 2015 53 Zhu J-Y, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE International Conference on Computer Vision. 2017. 54 Redmon J, Divvala S, Girshick R, Farhadi A. You only look once: unified, real-time object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016. p. 779‚Äì88. 55 Chaudhari P Agrawal H Kotecha K Data augmentation using MG-GAN for improved cancer classification on gene expression data Soft Computing. 2020 24 11381 91 56 Sagers LW et al Augmenting medical image classifiers with synthetic data from latent diffusion models arXiv preprint 2023 https://arxiv.org/abs/2308.12453 57 Aktas B Ates DD Duzyel O Gumus A Diffusion-based data augmentation methodology for improved performance in ocular disease diagnosis using retinography images International Journal of Machine Learning and Cybernetics. 2024 1 22 58 Zhang X, Gangopadhyay A, Chang HM, Soni R. Diffusion model-based data augmentation for lung ultrasound classification with limited data. In: ML4H@NeurIPS, 2023. p. 664‚Äì76. 59 Packh√§user K, et al. Generation of anonymous chest radiographs using latent diffusion models for training thoracic abnormality classification systems. In: 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI). 2023. 60 Sagers LW et al Improving dermatology classifiers across populations using images generated by large diffusion models arXiv preprint 2022 https://arxiv.org/abs/2211.13352 61 Ramesh A et al Hierarchical text-conditional image generation with clip latents arXiv preprint 2022 3 62 Groh M, et al. Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. 63 Wang Y Li C Wang Z Advancing precision medicine: VAE enhanced predictions of pancreatic cancer patient survival in local hospital IEEE Access. 2024 12 3428 36 64 Sohl-Dickstein J, et al. Deep unsupervised learning using nonequilibrium thermodynamics. In: International Conference on Machine Learning. 2015. 65 Song J Meng C Ermon S Denoising diffusion implicit models arXiv preprint 2020 https://arxiv.org/abs/2010.02502 66 Song Y et al Score-based generative modeling through stochastic differential equations arXiv preprint 2020 https://arxiv.org/abs/2011.13456 67 Moon C-I Lee J Baek YS Lee O Psoriasis severity classification based on adaptive multi-scale features for multi-severity disease Sci Rep. 2023 13 1 17331 doi: 10.1038/s41598-023-44478-9 37833444 PMC10575863 68 Suha SA Sanam TF A deep convolutional neural network-based approach for detecting burn severity from skin burn images Machine Learning with Applications. 2022 9 100371 69 Gare GR Tran HV deBoisblanc BP Rodriguez RL Galeotti JM Weakly supervised contrastive learning for better severity scoring of lung ultrasound arXiv preprint 2022 https://arxiv.org/abs/2201.07357 70 Luo Y Yang Q Fan Y Qi H Xia M Measurement guidance in diffusion models: insight from medical image synthesis IEEE Trans Pattern Anal Mach Intell. 2024 46 12 7983 97 doi: 10.1109/TPAMI.2024.3399098 38743550 71 Takezaki S Uchida S An ordinal diffusion model for generating medical images with different severity levels arXiv preprint 2024 https://arxiv.org/abs/2403.00452 72 Basak H Kundu R Sarkar R MFSNet: a multi focus segmentation network for skin lesion segmentation Pattern Recognition. 2022 128 108673 73 Suvorov R, et al. Resolution-robust large mask inpainting with Fourier convolutions. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022. p. 2149‚Äì59. 74 Wang Z Bovik AC Sheikh HR Simoncelli EP Image quality assessment: from error visibility to structural similarity IEEE Transactions on Image Processing. 2004 13 4 600 12 15376593 10.1109/tip.2003.819861 75 Zhang R, Isola P, Efros A l e x e i A, Shechtman E, Wang O. The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. p. 586‚Äì95. 76 Wang Z, Simoncelli EP, Bovik AC. Multiscale structural similarity for image quality assessment. In: The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers. 2003. p. 1398‚Äì402. 77 Li Y, et al. Gligen: open-set grounded text-to-image generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 78 Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In: Advances in Neural Information Processing Systems. 2017. p. 6626‚Äì37. 79 Salimans T, Goodfellow I, Zaremba W, Cheung V, Radford A, Chen X. Improved techniques for training GANs. In: Advances in Neural Information Processing Systems. 2016. p. 2234‚Äì42. 80 Ravuri S, Vinyals O. Classification accuracy score for conditional generative models. In: Wallach H, Larochelle H, Beygelzimer A, dAlch√©-Buc F, Fox E, Garnett R, editors. Advances in Neural Information Processing Systems. Curran Associates, Inc.; 2019. 81 Pacheco AGC Lima GR Salom√£o AS Krohling B Biral IP de Angelo GG et al PAD-UFES-20: a skin lesion dataset composed of patient data and clinical images collected from smartphones Data Brief. 2020 32 106221 doi: 10.1016/j.dib.2020.106221 32939378 PMC7479321 82 Lee T Ng V Gallagher R Coldman A McLean D DullRazor: a software approach to hair removal from images Comput Biol Med. 1997 27 6 533 43 doi: 10.1016/s0010-4825(97)00020-6 9437554 83 Kawahara J Daneshvar S Argenziano G Hamarneh G 7-Point checklist and skin lesion classification using multi-task multi-modal neural nets IEEE J Biomed Health Inform. 2018 doi: 10.1109/JBHI.2018.2824327 29993994 10.1371/journal.pone.0331404.r001 Decision Letter 0 Wang Zeheng Academic Editor ¬© 2025 Zeheng Wang 2025 Zeheng Wang https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Submission Version 0  29 Apr 2025 PONE-D-24-58388Diffusion-based skin disease data augmentation with detailed feature preservation and severity controlPLOS ONE Dear Dr. Kim, Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE‚Äôs publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process. Please submit your revised manuscript by Jun 13 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at plosone@plos.org https://www.editorialmanager.com/pone/ Please include the following items when submitting your revised manuscript: A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'. A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'. An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'. If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter. If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols https://plos.org/protocols?utm_medium=editorial-email&utm_source=authorletters&utm_campaign=protocols We look forward to receiving your revised manuscript. Kind regards, Zeheng Wang Academic Editor PLOS ONE  Journal Requirements: 1. When submitting your revision, we need you to address these additional requirements.¬†Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code http://journals.plos.org/plosone/s/submit-now [Note: HTML markup is below. Please do not edit.] Reviewers' comments: Reviewer's Responses to Questions  Comments to the Author 1. Is the manuscript technically sound, and do the data support the conclusions? The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. Reviewer #1:¬†Yes Reviewer #2:¬†Partly Reviewer #3:¬†Yes ********** 2. Has the statistical analysis been performed appropriately and rigorously? Reviewer #1:¬†Yes Reviewer #2:¬†No Reviewer #3:¬†Yes ********** 3. Have the authors made all data underlying the findings in their manuscript fully available? The PLOS Data policy Reviewer #1:¬†Yes Reviewer #2:¬†Yes Reviewer #3:¬†No ********** 4. Is the manuscript presented in an intelligible fashion and written in standard English? PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here. Reviewer #1:¬†Yes Reviewer #2:¬†Yes Reviewer #3:¬†Yes ********** 5. Review Comments to the Author Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters) Reviewer #1: Both the EFA Net model and the Diffusion based data augmentation model adopt innovative architectural designs to enhance performance. EFAM Net enhances feature extraction and fusion capabilities by introducing Attention Residual Learning ConvNeXt (ARLC) blocks, Parallel ConvNeXt (PCNXt) blocks, and Multi scale Efficient Attention Feature Fusion (MEAFF) blocks, particularly in multi-scale feature fusion and attention mechanisms, achieving high accuracy in skin lesion classification tasks. The Diffusion based model improves the Stable Diffusion model by using an 8-channel Variational Autoencoder (VAE) and multi-level embedding techniques, effectively enhancing the quality of image generation and detail preservation. At the same time, it controls the severity of diseases through segmentation masks and interpolation techniques, providing high-quality synthetic images for skin lesion data enhancement. The EFAM Net model focuses on the classification task of skin lesion images, enhancing feature extraction and fusion capabilities by designing ARLC blocks, PCNXt blocks, and MEAFF blocks. It performs particularly well in handling complex skin lesion features and can achieve classification accuracy of over 93% on multiple public datasets. The Diffusion based model focuses on solving the problem of scarce data on skin lesions. By improving the Stable Diffusion model and using 8-channel VAE and multi-level embedding techniques to generate high-quality composite images, and controlling the severity of the disease through segmentation masks and interpolation techniques, it performs well in image generation quality and detail preservation, significantly improving the effectiveness of composite data in classification tasks. Need to provide source code and dataset in a link. However, the shortcomings of this paper include: 1. Insufficient diversity of synthesized images: In some cases, synthesized images may overly emphasize certain details and features (such as hair areas or specific color information), resulting in images that are not natural enough and affecting the diversity and authenticity of synthesized data 2. Limitations of normal image generation: When generating normal skin areas, if there are significant differences between the lesion area and the surrounding background, it may result in the inability to generate completely realistic normal images, limiting the application of the model in certain scenarios. 3. Limitations of evaluation indicators: Although commonly used indicators such as FID and IS are used to evaluate the quality of synthesized images, these indicators show certain limitations in medical image synthesis tasks and cannot fully reflect the actual contribution of generated data to downstream diagnostic tasks. References: Ji, Z., Wang, X., Liu, C., et al. (2024). \"EFAM-Net: A Multi-Class Skin Lesion Classification Model Utilizing Enhanced Feature Fusion and Attention Mechanisms.\" IEEE Access, 12, 143029-143041. DOI: 10.1109/ACCESS.2024.3468612 Reviewer #2: - The introduction does not sufficiently motivate the clinical importance of skin disease diagnosis. The authors could strengthen this section by briefly discussing the prevalence and diagnostic challenges of skin diseases, highlighting current diagnostic methods and their limitations, and incorporating relevant examples from recent literature to clearly position their synthetic data augmentation approach. - The study uses a dataset of dermoscopy images but only provides class names. The authors should include brief clinical descriptions and key morphological characteristics of each class to enhance clarity and provide better context. - The authors should clarify how the 5-scale CLIP embeddings are derived, as the current explanation lacks detail. While ELITE is referenced, a brief description within the relevant section would improve completeness and accessibility for readers unfamiliar with the method. - The authors should provide more details on the inference process of their latent diffusion model. While Stable Diffusion is originally designed for text-to-image generation, this work leverages multi-level CLIP embeddings to preserve fine-grained details. Is this the sole conditioning mechanism, or are additional factors, such as class labels, incorporated? - The authors briefly mention DDIM in the related works section, however it is not mentioned again. It should be explicitly mentioned that DDIM is used during inference. - Latent space manipulation assumes the generated lesion corresponds to the highest severity level, but this may not always be the case. Could the authors clarify how severity levels are defined in the latent space and whether any constraints or validation steps ensure a consistent and accurate progression of severity? - The 8-channel VAE is evaluated using LPIPS and MS-SSIM for reconstruction, but its impact on the latent space is not assessed. Increasing VAE channels enlarges the latent representation and UNet, potentially improving performance but at a higher computational cost. An alternative is increasing input resolution while maintaining the same downsampling ratio, expanding the latent space without modifying the architecture. An ablation study comparing these approaches would be necessary to better justify the choice of an 8-channel VAE. - The proposed LDM is compared against Stable Diffusion as a baseline, but it is unclear how it was adapted to align with the study's objectives. The authors should provide more details on any modifications made, including changes to conditioning mechanisms and training procedures to ensure a fair and meaningful comparison. - The classification performance evaluation focuses on relatively shallow networks. Is there a reason deeper or transformer-based models were not considered? Given their improved feature extraction capabilities, could they bridge the 3% gap even without synthetic data augmentation? A complete justification and explanation is necessary Minor Issues: ‚óè \\mathcal{} should be used to properly format variables. ‚óè Starting sentences with ‚ÄúAnd‚Äù should be avoided for better readability. ‚óè LPIPS and MS-SSIM are both missing citations. Reviewer #3: 2. The study lacks comprehensive ablation experiments to evaluate the individual impact of architectural choices (e.g., multi-level embeddings, adapter layers). This omission limits the interpretability and reproducibility of the work. 3. The background section would benefit from the addition of other relevant medical literature, such as \"Advancing Precision Medicine: VAE Enhanced Predictions of Pancreatic Cancer Patient Survival in Local Hospital\", to strengthen its persuasiveness. ********** 6. PLOS authors have the option to publish the peer review history of their article ( what does this mean? If you choose ‚Äúno‚Äù, your identity will remain anonymous but your review may still be made public. Do you want your identity to be public for this peer review? Privacy Policy Reviewer #1:¬†No Reviewer #2:¬†No Reviewer #3:¬†No ********** [NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link \"View Attachments\". If this link does not appear, there are no attachment files.] While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, https://pacev2.apexcovantage.com/ figures@plos.org 10.1371/journal.pone.0331404.r002 Author response to Decision Letter 1 Submission Version 1  4 Jul 2025 Reviewer #1 1. [Insufficient diversity of synthesized images: In some cases, synthesized images may overly emphasize certain details and features (such as hair areas or specific color information), resulting in images that are not natural enough and affecting the diversity and authenticity of synthesized data]Our response: We agree with your observation. To address the issue of incomplete generation in hair-rich regions, we implemented the Dullrazor algorithm, a traditional hair removal method. By removing hair regions before synthesis, we confirmed that this approach can suppress excessive hair generation to some extent. This limitation and our solution are discussed in the revised manuscript (lines 623-628 in the Limitations section), with visual examples provided in Figure 11. 2. [Insufficient diversity of synthesized images: In some cases, synthesized images may overly emphasize certain details and features (such as hair areas or specific color information), resulting in images that are not natural enough and affecting the diversity and authenticity of synthesized data]Our response: We acknowledge your concern regarding the overemphasis of certain features in synthesized images. This issue is particularly pronounced when training data is limited, leading to overfitting on specific characteristics. The skin lesion domain is especially vulnerable due to the scarcity of available data compared to other domains. To mitigate this issue, we implemented comprehensive data augmentation strategies during the training process: - Standard augmentations: rotation and flip - Advanced transformations: elastic transform and affine transformations - All augmentations were applied probabilistically to maximize diversity These augmentation techniques were implemented in the data preprocessing pipeline of the train_diffusion process, which we provide with the revision. However, we acknowledge that despite these efforts, the absolute scarcity of training data occasionally results in inaccurate image generation. This fundamental limitation of data availability in the medical imaging domain remains a challenge. 3. [Limitations of evaluation indicators: Although commonly used indicators such as FID and IS are used to evaluate the quality of synthesized images, these indicators show certain limitations in medical image synthesis tasks and cannot fully reflect the actual contribution of generated data to downstream diagnostic tasks.]Our response:We agree with your observation regarding the limitations of FID and IS metrics in medical image synthesis. While these metrics, originally developed for GAN-based models, are commonly applied to diffusion models, recent research has questioned their suitability for non-GAN architectures. We have cited relevant studies in the revised manuscript (lines 521-528, under \"Evaluation of synthetic image quality and fine detail preservation\" in the Results section) that demonstrate FID and IS do not guarantee absolute correlation with downstream tasks such as classification performance. These metrics may indicate image quality but fail to capture the clinical value of synthesized data. We acknowledge that while we report FID and IS scores following conventional practice in diffusion model evaluation, these metrics are insufficient for assessing the diagnostic utility of generated medical images. Future research should focus on developing clinically-relevant metrics that quantify the basis for clinical decision-making, thereby better reflecting the actual contribution of synthetic data to diagnostic tasks. Reviewer #2 1. [The introduction does not sufficiently motivate the clinical importance of skin disease diagnosis. The authors could strengthen this section by briefly discussing the prevalence and diagnostic challenges of skin diseases, highlighting current diagnostic methods and their limitations, and incorporating relevant examples from recent literature to clearly position their synthetic data augmentation approach.]Our response:Thank you for your valuable feedback. We have substantially strengthened the introduction to better motivate the clinical importance of skin disease diagnosis. The revised introduction now includes: 1. Global health burden: Added epidemiological data showing skin diseases affect 30-70% of individuals worldwide and rank as the 4th leading cause of nonfatal disease burden globally (lines 8-10). 2. Clinical significance: Emphasized the critical impact of early diagnosis, particularly for melanoma where 5-year survival rates drop from 99% with early detection to dramatically lower rates upon metastasis (lines 11-13). 3. Current diagnostic challenges: Highlighted the shortage of dermatologists in many regions and the difficulty in distinguishing between benign pigmented lesions and malignant melanoma, even for experienced specialists (lines 14-18). 2. [The study uses a dataset of dermoscopy images but only provides class names. The authors should include brief clinical descriptions and key morphological characteristics of each class to enhance clarity and provide better context.]Our response: Thank you for this suggestion. We have enhanced the Datasets section by adding comprehensive clinical descriptions and morphological characteristics for each class. These additions (lines 241-279) include: - Detailed clinical descriptions of each lesion type - Key morphological features that distinguish each class - Diagnostic characteristics relevant to dermoscopic evaluation All descriptions are supported by citations from the original dataset publications to ensure accuracy and clinical relevance. This additional context will help readers better understand the complexity and diversity of the skin lesion classes used in our study. 3. [The authors should clarify how the 5-scale CLIP embeddings are derived, as the current explanation lacks detail. While ELITE is referenced, a brief description within the relevant section would improve completeness and accessibility for readers unfamiliar with the method.]Our response:Thank you for highlighting this need for clarification. We have significantly expanded the explanation of the 5-scale CLIP embeddings derivation in the \"Preserving visual details through multi-level embeddings\" subsection of the Materials and Methods section (lines 346-393). The revised text now includes: - A detailed step-by-step explanation of how the multi-scale embeddings are extracted - The specific layers and resolutions used for each scale - A comprehensive description of the token combination mechanism - Mathematical formulations showing how embeddings from different layers are integrated Additionally, we have enhanced the adapter formulation beyond simple equations to provide deeper insights into: - The token concatenation and fusion process - The layer-wise integration strategy - The rationale behind our multi-scale approach These additions ensure that readers unfamiliar with ELITE or multi-scale embedding methods can fully understand our implementation without needing to reference external sources. 4. [The authors should provide more details on the inference process of their latent diffusion model. While Stable Diffusion is originally designed for text-to-image generation, this work leverages multi-level CLIP embeddings to preserve fine-grained details. Is this the sole conditioning mechanism, or are additional factors, such as class labels, incorporated?] 5. [The authors briefly mention DDIM in the related works section, however it is not mentioned again. It should be explicitly mentioned that DDIM is used during inference.]Our response:Thank you for pointing out the need for more detailed explanation of our inference process and conditioning mechanisms. We have expanded the Implementation Details section (lines 428-464) to address both concerns: Regarding conditioning mechanisms(#4): We clarified that our model uses dual conditioning: - Multi-level CLIP embeddings for fine-grained visual detail preservation - Class label embeddings as additional conditioning to ensure class-specific generation The revised text now explicitly describes how these two conditioning signals are combined during the diffusion process. Regarding DDIM usage (#5): We now explicitly state that DDIM is used during inference, including: - The specific number of inference steps used Additionally, we have added class-specific implementation details: - Inference steps optimized for each lesion class - CFG (Classifier-Free Guidance) scale values per class 6. [Latent space manipulation assumes the generated lesion corresponds to the highest severity level, but this may not always be the case. Could the authors clarify how severity levels are defined in the latent space and whether any constraints or validation steps ensure a consistent and accurate progression of severity?]Our response:We appreciate this critical observation and take it very seriously. You are correct that simple bicubic interpolation in the latent space cannot be considered a clinically valid method for representing disease severity progression. After consulting with dermatology specialists, we learned that: - Severity assessment requires comprehensive evaluation beyond what cropped lesion images can provide - Clinical severity involves multiple complex factors including not only color and size, but also border irregularity, presence of ulceration, and other morphological features - Our interpolation method cannot capture these multifaceted clinical aspects Therefore, we acknowledge that referring to our latent space manipulation as \"severity control\" is misleading and scientifically inaccurate. We have made the following revisions: 1. Title revision: Changed from \"severity control\" to \"enhancing data diversity\" to avoid misrepresentation 2. Content revision: Removed all claims about severity control throughout the manuscript 3. Repositioning: Clarified that latent space interpolation serves solely as a data augmentation technique for increasing sample diversity, not as a clinical severity modeling tool We have concluded that the interpolation-based method should be considered primarily as a means to expand data diversity rather than for severity control. This approach is more scientifically sound and better reflects the actual capabilities and appropriate use cases of our method. 7. [The 8-channel VAE is evaluated using LPIPS and MS-SSIM for reconstruction, but its impact on the latent space is not assessed. Increasing VAE channels enlarges the latent representation and UNet, potentially improving performance but at a higher computational cost. An alternative is increasing input resolution while maintaining the same downsampling ratio, expanding the latent space without modifying the architecture. An ablation study comparing these approaches would be necessary to better justify the choice of an 8-channel VAE.]Our response: Thank you for this insightful suggestion regarding the comparison between channel expansion and spatial resolution increase. We appreciate your comprehensive analysis of the trade-offs involved. We initially planned experiments following your suggested approach, testing resolutions from 224√ó224 to 256√ó256 and up to 512√ó512. However, we encountered significant computational constraints: even with 384√ó384 input and batch size of 1, training time increased by over 30% and nearly exhausted our available resources. Given our laboratory's computational limitations, 256√ó256 proved to be the practical upper limit for systematic experimentation. We agree that higher resolutions could better capture fine-grained details, as you noted. This remains an important direction for future work when additional computational resources become available. We plan to conduct comprehensive ablation studies comparing: - Higher spatial resolutions (384√ó384, 512√ó512) with standard 4-channel VAE - Our current 8-channel VAE approach at various resolutions Regarding the VAE evaluation metrics, we focused on LPIPS and MS-SSIM because, in our current framework, the VAE functions primarily as an input-output reconstruction module. These metrics directly assess the visual and structural similarity essential for this role. We acknowledge that a more comprehensive evaluation of the latent space properties would provide additional insights, particularly regarding the trade-offs between channel expansion and spatial resolution. This will be addressed in our future comparative studies. 8. [The proposed LDM is compared against Stable Diffusion as a baseline, but it is unclear how it was adapted to align with the study's objectives. The authors should provide more details on any modifications made, including changes to conditioning mechanisms and training procedures to ensure a fair and meaningful comparison.] Our response: Thank you for highlighting the need for clarity regarding our baseline comparison. We have expanded the Implementation Details section to provide comprehensive information about how Stable Diffusion was adapted for our study. For the baseline Stable Diffusion model, we: 1. Maintained standard architecture: Used the original Stable Diffusion architecture with recommended parameters to ensure reproducibility 2. Modified conditioning mechanism: Replaced text embeddings with class label embeddings to align with our medical image generation task 3. Kept training procedures consistent: Used identical training hyperparameters, data augmentation strategies, and optimization settings for both baseline and proposed models The key difference between our approach and the baseline lies in: - Baseline: Single-scale conditioning using only class label embeddings - Our method: Multi-scale CLIP embeddings combined with class label embeddings for enhanced detail preservation Detailed implementation specifications, including learning rates, batch sizes, and training schedules, have been added to the Implementation Details section (lines 428-464). 9. [The classification performance evaluation focuses on relatively shallow networks. Is there a reason deeper or transformer-based models were not considered? Given their improved feature extraction capabilities, could they bridge the 3% gap even without synthetic data augmentation? A complete justification and explanation is necessary]Our response:Thank you for this valuable suggestion. Following your recommendation, we conducted additional experiments with transformer-based models and deeper CNN architectures. (line 554). We have added these results to the revised manuscript. Our initial choice of relatively shallow networks was deliberate, based on the following considerations:- Dataset characteristics : HAM10000 has limited data quantity, with some classes being severely underrepresented. This data scarcity particularly affects minority classes, making it challenging for parameter-heavy models to generalize effectively. - The primary value of synthetic data augmentation lies not in improving performance on well-represented classes, but in addressing the critical shortage of rare disease samples. Our approach specifically targets this class imbalance problem. Minor Issues:‚óè \\mathcal{} should be used to properly format variables.‚óè Starting sentences with ‚ÄúAnd‚Äù should be avoided for better readability.‚óè LPIPS and MS-SSIM are both missing citations. Reviewer #3 1. [The model‚Äôs performance is validated exclusively on the HAM10000 dataset, which may not capture the full variability of skin conditions across populations and imaging conditions. If possible, broader validation on diverse datasets may help to establish generalizability.]Our response:We appreciate your concern regarding the generalizability of our approach. You raise an important point about validation across diverse datasets. Currently, publicly available dermoscopy datasets for skin diseases are limited, with ISIC, HAM10000, and PH2 being the most prominent. However, these datasets present significant challenges: - Limited data quantity or restricted class diversity - Substantial overlap between datasets due to shared data collection sources (hospitals, institutions) - Insufficient representation of diverse populations and imaging conditions This overlap makes cross-dataset validation less meaningful, as it would not truly test g Attachment Submitted filename: Response to Reviewers.docx 10.1371/journal.pone.0331404.r003 Decision Letter 1 Wang Zeheng Academic Editor ¬© 2025 Zeheng Wang 2025 Zeheng Wang https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Submission Version 1  22 Jul 2025 PONE-D-24-58388R1Diffusion-based skin disease data augmentation with fine-grained detail preservation and interpolation for data diversityPLOS ONE Dear Dr. Yoo, Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE‚Äôs publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process. Please submit your revised manuscript by Sep 05 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at plosone@plos.org https://www.editorialmanager.com/pone/ Please include the following items when submitting your revised manuscript: A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'. A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'. An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'. If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter. If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols https://plos.org/protocols?utm_medium=editorial-email&utm_source=authorletters&utm_campaign=protocols We look forward to receiving your revised manuscript. Kind regards, Zeheng Wang Academic Editor PLOS ONE Journal Requirements: If the reviewer comments include a recommendation to cite specific previously published works, please review and evaluate these publications to determine whether they are relevant and should be cited. There is no requirement to cite these works unless the editor has indicated otherwise. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article‚Äôs retracted status in the References list and also include a citation and full reference for the retraction notice. [Note: HTML markup is below. Please do not edit.] Reviewers' comments: Reviewer's Responses to Questions  Comments to the Author 1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the ‚ÄúComments to the Author‚Äù section, enter your conflict of interest statement in the ‚ÄúConfidential to Editor‚Äù section, and submit your \"Accept\" recommendation. Reviewer #2:¬†All comments have been addressed Reviewer #4:¬†All comments have been addressed Reviewer #5:¬†All comments have been addressed ********** 2. Is the manuscript technically sound, and do the data support the conclusions? The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. Reviewer #2:¬†Yes Reviewer #4:¬†Yes Reviewer #5:¬†Yes ********** 3. Has the statistical analysis been performed appropriately and rigorously? Reviewer #2:¬†Yes Reviewer #4:¬†Yes Reviewer #5:¬†Yes ********** 4. Have the authors made all data underlying the findings in their manuscript fully available? The PLOS Data policy Reviewer #2:¬†(No Response) Reviewer #4:¬†Yes Reviewer #5:¬†Yes ********** 5. Is the manuscript presented in an intelligible fashion and written in standard English? PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here. Reviewer #2:¬†Yes Reviewer #4:¬†Yes Reviewer #5:¬†Yes ********** 6. Review Comments to the Author Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters) Reviewer #2:¬†The authors have addressed most of the major concerns in this revised manuscript. They have added additional implementation details, an ablation study, and more experiments for downstream classification on deeper transformer-based architectures. A minor remaining concern is the use of the term 'proxy severity level.' While an improvement over 'severity control,' the term 'proxy' implies an indirect measure that has been validated to correlate with the ground truth. As the authors rightly acknowledge, the proposed latent space interpolation method does not capture the multifaceted nature of clinical severity and has not been validated as such. I recommend replacing all instances of 'proxy severity level' with a more accurate and defensible term, such as the one used in the rebuttal: 'latent space interpolation for enhanced data diversity.' Reviewer #4:¬†(No Response) Reviewer #5:¬†This manuscript presents an innovative diffusion-based data augmentation method designed to improve the diversity and quality of synthetic dermoscopic images for skin disease classification. By enhancing the Stable Diffusion framework with an 8-channel variational autoencoder and multi-level CLIP visual embeddings, the authors aim to better preserve fine-grained lesion details. Additionally, they introduce a proxy severity interpolation strategy to simulate varying lesion intensities. The study is technically sound, and the authors have made meaningful revisions in response to previous reviewer comments, including improvements to the clinical motivation, dataset descriptions, and method transparency. Despite these strengths, several key concerns remain. First, the notion of ‚Äúproxy severity‚Äù lacks sufficient empirical support. While the authors acknowledge that the interpolation does not represent clinically validated severity levels, no evaluation‚Äîeither quantitative or by domain experts‚Äîis provided to demonstrate the interpretability or utility of these interpolated images. This weakens the claim that the generated data can aid in simulating disease progression or augmenting rare severity classes. Second, the study is limited to a single dataset (HAM10000), which restricts its generalizability. Although the authors discuss the limitations of existing public datasets, even a small-scale test on a secondary dataset (e.g., PH2 or ISIC) would help establish broader relevance. Additionally, while multiple classifier backbones are tested, the evaluation primarily focuses on shallow models. It remains unclear whether performance improvements would persist with more powerful architectures that could compensate for limited data without augmentation. The ablation study adds value but does not fully isolate the contributions of each component, such as adapter layers or class-conditioning strategies. Furthermore, the choice to expand VAE channels rather than image resolution is justified based on computational constraints, yet no comparative results are shown. More concrete evidence on the trade-offs between spatial resolution and latent dimensionality would be useful. Evaluation metrics are another concern. The continued reliance on FID and IS‚Äîdespite acknowledging their limitations in medical imaging‚Äîunderscores the need for more appropriate alternatives. Even a basic human visual assessment or task-based evaluation (e.g., classification accuracy using only synthetic images) would strengthen the argument that these images are diagnostically useful. ********** 7. PLOS authors have the option to publish the peer review history of their article ( what does this mean? If you choose ‚Äúno‚Äù, your identity will remain anonymous but your review may still be made public. Do you want your identity to be public for this peer review? Privacy Policy Reviewer #2:¬†No Reviewer #4:¬†No Reviewer #5:¬†No ********** [NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link \"View Attachments\". If this link does not appear, there are no attachment files.] While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, https://pacev2.apexcovantage.com/ figures@plos.org 10.1371/journal.pone.0331404.r004 Author response to Decision Letter 2 Submission Version 2  14 Aug 2025 Reviewer #2 1. ‚ÄúA minor remaining concern is the use of the term 'proxy severity level.' While an improvement over 'severity control,' the term 'proxy' implies an indirect measure that has been validated to correlate with the ground truth. As the authors rightly acknowledge, the proposed latent space interpolation method does not capture the multifaceted nature of clinical severity and has not been validated as such. I recommend replacing all instances of 'proxy severity level' with a more accurate and defensible term, such as the one used in the rebuttal: 'latent space interpolation for enhanced data diversity‚Äù Our response: We thank the reviewer for pointing this out. In response, we have carefully revised the manuscript to remove all instances of the term ‚Äúproxy severity level‚Äù and related phrases. Wherever applicable, we have replaced them with more accurate and defensible expressions such as ‚Äúlatent space interpolation between normal and lesion appearances‚Äù or ‚Äúsmoothly varying visual characteristics‚Äù, as suggested by the reviewer through the example phrase ‚Äúlatent space interpolation for enhanced data diversity.‚Äù These revisions were made to clarify that the interpolation method is intended solely for visual data augmentation and is not meant to imply any clinical interpretation regarding disease severity. The manuscript has also been revised to avoid terminology that could be interpreted as suggesting alignment with clinically defined severity scales. Reviewer #5 1. [First, the notion of ‚Äúproxy severity‚Äù lacks sufficient empirical support. While the authors acknowledge that the interpolation does not represent clinically validated severity levels, no evaluation‚Äîeither quantitative or by domain experts‚Äîis provided to demonstrate the interpretability or utility of these interpolated images.] Our response: We thank the reviewer for this important comment. We acknowledge that the expression ‚Äúproxy severity level‚Äù could be misinterpreted as implying alignment with clinically validated severity scales. To avoid such misinterpretation, we have removed all instances of the term ‚Äúproxy severity level‚Äù and related phrases from the manuscript. Wherever applicable, we replaced them with more precise and defensible expressions such as ‚Äúlatent space interpolation between normal and lesion appearances‚Äù or ‚Äúsmoothly varying visual characteristics,‚Äù clarifying that our approach is intended solely for visual data augmentation rather than clinical severity modeling. In addition, to address the reviewer‚Äôs concern regarding empirical support, we have incorporated a new quantitative analysis to examine the potential utility of interpolated images from a data diversity perspective. Specifically, we compared two synthetic image sets‚ÄîA (500 randomly generated samples) and AI (the same set with half replaced by interpolated samples)‚Äîby extracting CLIP embeddings, projecting them into two dimensions using PCA, and calculating semantic variance for each of the seven classes (Table.6). Across all classes, AI exhibited higher variance than A, indicating that interpolation-based samples captured a broader range of semantic characteristics. The results in Table„Öã` 6 show that variance increases ranged from +0.45 (Class 1) to +2.86 (Class 4), with the largest gains observed in classes that are relatively underrepresented in the original dataset. This pattern suggests that latent space interpolation may be particularly effective in enhancing diversity for minority classes, thereby improving the coverage of the feature space available to the model during training. The corresponding visualizations (Fig. 12) further illustrate that, in most classes, AI samples (triangles) are more widely dispersed in the embedding space than A samples (circles), suggesting an expansion of the representational space that may contribute to enhanced training data diversity. These results provide quantitative indications that the interpolation approach can contribute to visual data augmentation. 2. [Second, the study is limited to a single dataset (HAM10000), which restricts its generalizability. Although the authors discuss the limitations of existing public datasets, even a small-scale test on a secondary dataset (e.g., PH2 or ISIC) would help establish broader relevance.] Our response: We appreciate the reviewer‚Äôs valuable suggestion regarding the need for broader validation beyond the HAM10000 dataset. In the revised manuscript, we have incorporated an additional cross-dataset zero-shot evaluation using the publicly available PAD-UFES-20 dataset to further examine the generalization capability of our classifiers. While other dermoscopic datasets such as PH2 and ISIC are publicly available, they share common data collection sources with HAM10000, resulting in substantial image overlap. This overlap makes them less suitable as independent test sets for evaluating cross-dataset performance, as duplicated samples could confound the interpretation. Therefore, following the reviewer‚Äôs suggestion, we opted for the PAD-UFES-20 dataset, which‚Äîwhile still within the dermatology domain‚Äîcontains clinical photographs captured using smartphone cameras rather than professional dermoscopic images (Fig. 8). This setup enables evaluation under more diverse imaging conditions, including variations in acquisition devices, lighting, resolution, and background context. For a fair comparison, we evaluated only the four classes common to both datasets: AKIEC, BCC, NV, and MEL. Nine classification models trained solely on HAM10000 (without fine-tuning) were directly tested on PAD-UFES-20. As shown in Table. 4 transformer-based architectures such as EVA(Acc=64.27%) and Swin Transformer (Acc=62.15%) showed relatively stronger robustness to domain shifts compared to conventional CNN-based models. While some performance differences were observed compared to the in-domain HAM10000 results, the relative ranking of models remained consistent, suggesting that our approach maintains its effectiveness under varied imaging conditions. 3. [Additionally, while multiple classifier backbones are tested, the evaluation primarily focuses on shallow models. It remains unclear whether performance improvements would persist with more powerful architectures that could compensate for limited data without augmentation.] Our response: We thank the reviewer for this valuable comment. In the revised manuscript, we have expanded our downstream classification experiments to include a broader spectrum of architectures, ranging from relatively simple CNN models (VGG13, ResNet18) to deeper and more modern architectures such as ConvNeXt, Swin Transformer, EVA, and CoAtNet. The updated results are provided in Table.3 (Section ‚ÄúClassification downstream task‚Äù). Our results show that, across both simpler and more modern backbone models, adding synthetic data generally improves classification accuracy. While the magnitude of improvement varies by architecture, this suggests that the proposed augmentation approach can contribute to enhancing performance regardless of model complexity. Notably, modern architectures such as Swin Transformer, ConvNeXt, EVA, and CoAtNet achieved higher absolute performance than earlier-generation models. however, in the case of EVA, the improvement from adding synthetic data was relatively modest (less than 3%). This suggests that certain models may already extract sufficiently rich representations from the original data, leaving relatively less room for further gains through synthetic augmentation alone. These findings suggest that our method is effective not only for shallow networks but also for more complex architectures, although the degree of benefit from synthetic augmentation can vary depending on the model. 4. [The ablation study adds value but does not fully isolate the contributions of each component, such as adapter layers or class-conditioning strategies.] Our response: We appreciate the reviewer‚Äôs comment on the need to more clearly isolate the contributions of individual components. However, our design intentionally integrates the adapter and the multi-level embedding pathway to function as a unified mechanism. The adapter layers are crucial for injecting multi-level embeddings into the diffusion U-Net by performing cross-attention. As a result, removing the adapter would inherently disable this pathway, making it technically difficult to evaluate the two components independently. Therefore, in our ablation study, the comparison between the 8-channel VAE without both the multi-level embeddings and adapter layers (‚Äú8ch‚Äù) and the 8-channel VAE with both components included (‚Äú8ch+ML‚Äù) captures their combined impact. In addition, we appreciate and agree with the reviewer's comment about class-conditioning strategies. To address this valid point, we have supplemented our ablation study with an additional analysis of the class-conditioning strategy. Specifically, we generated samples for all lesion classes using identical random seeds and compared results across various CFG (Classifier-Free Guidance) scales, including a non-conditioned setting in which the class label embedding was entirely removed during sampling. As shown in the newly added Figure 10, removing class-conditioning weakened class-specific lesion patterns and led to more ambiguous class characteristics. In contrast, increasing the CFG value emphasized distinctive morphological features for each class (e.g., pigment networks in melanoma and vascular lacunae in vascular lesions). These results qualitatively confirm that class-conditioning contributes to maintaining class-specific characteristics. 5. [Furthermore, the choice to expand VAE channels rather than image resolution is justified based on computational constraints, yet no comparative results are shown. More concrete evidence on the trade-offs between spatial resolution and latent dimensionality would be useful.] Our response: We appreciate the reviewer's comment on the need to clarify the trade-offs between expanding VAE channels and increasing image resolution. Although we could not conduct a full empirical comparison due to hardware limitations, we provide here a mathematical estimation of the memory requirements and computational complexity for two representative configurations: Configuration 1 (256√ó256 input, 8 channels): This produces a latent map of size 32√ó32√ó8, resulting in 8,192 latent elements with baseline memory usage (1√ó) and spatial tokens (1√ó). Configuration 2 (512√ó512 input, 4 channels): This produces a latent map of size 64√ó64√ó4, resulting in 16,384 latent elements with doubled latent memory (2√ó) and quadrupled spatial tokens (4√ó). From this estimation, the 512√ó512, 4-channel setup doubles the number of latent elements (and thus latent-storage memory) relative to the 256√ó256, 8-channel setup. More importantly, the spatial token count quadruples, so convolutional layers scale by roughly ~4√ó, cross-attention by ~4√ó (with fixed text length), and global self-attention by up to ~16√ó in FLOPs. In practice, when intermediate activations and attention buffers are included, we typically observe ~2.5‚Äì3√ó higher VRAM usage, which exceeded our available GPU memory even with a batch size of one. Given these constraints, we chose the 256√ó256, 8-channel configuration to increase latent representational capacity while remaining computationally feasible and maintaining stable training. We acknowledge that a direct empirical comparison would further strengthen the justification. In future work, we plan to revisit this by upgrading hardware, applying memory-efficient attention, gradient checkpointing, and mixed precision, and benchmarking 4-channel VAEs against 8-channel latent-width models across reconstruction quality, generative fidelity, and downstream performance on high-resolution images. 6. [Evaluation metrics are another concern. The continued reliance on FID and IS‚Äîdespite acknowledging their limitations in medical imaging‚Äîunderscores the need for more appropriate alternatives. Even a basic human visual assessment or task-based evaluation (e.g., classification accuracy using only synthetic images) would strengthen the argument that these images are diagnostically useful.] Our response: We agree with the reviewer‚Äôs observation regarding the limitations of FID and IS in evaluating the diagnostic utility of medical images. While these metrics, originally developed for GAN-based models, are still conventionally applied to diffusion models for comparability, they primarily reflect distributional similarity and do not fully capture the clinical value of synthesized data. To complement these limitations, our evaluation includes classification performance in real-only, synthetic-only, and mixed settings, which can serve as task-based evidence regarding the utility of the generated images. These results indicate that while synthetic-only training performs below real-only, combining synthetic and real images generally improves accuracy, suggesting that the generated data may be useful as complementary training data rather than as a full substitute for real data. In this revision, we additionally included a cross-dataset zero-shot evaluation on the PAD-UFES-20 dataset (Table 4, Fig. 8) to examine generalization under different image acquisition settings (professional dermoscopic devices vs. smartphone cameras), as well as an OpenCLIP-based analysis (PCA visualizations and per-class semantic variance; Fig. 12, Table 6) comparing interpolated and randomly generated samples. While FID and IS remain the most commonly reported metrics for image generation, these supplementary evaluations can provide complementary quantitative perspectives that may help address their known limitations, particularly in the context of medical image synthesis. In parallel with this work, we are building an expanded skin lesion dataset with dermatologist-validated diagnostic and severity annotations. In future research, we aim to leverage this dataset to conduct blinded expert review protocols (e.g., lesion realism, artifact assessment, diagnostic plausibility) to more precisely quantify the clinical utility of the generated images. Attachment Submitted filename: Response_to_Reviewers_auresp_2.docx 10.1371/journal.pone.0331404.r005 Decision Letter 2 Wang Zeheng Academic Editor ¬© 2025 Zeheng Wang 2025 Zeheng Wang https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Submission Version 2  15 Aug 2025 Diffusion-based skin disease data augmentation with fine-grained detail preservation and interpolation for data diversity PONE-D-24-58388R2 Dear Dr. Yoo, We‚Äôre pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements. Within one week, you‚Äôll receive an e-mail detailing the required amendments. When these have been addressed, you‚Äôll receive a formal acceptance letter and your manuscript will be scheduled for publication. An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at Editorial Manager¬Æ billing support If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they‚Äôll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org Kind regards, Zeheng Wang Academic Editor PLOS ONE Additional Editor Comments (optional): Reviewers' comments: 10.1371/journal.pone.0331404.r006 Acceptance letter Wang Zeheng Academic Editor ¬© 2025 Zeheng Wang 2025 Zeheng Wang https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License PONE-D-24-58388R2 PLOS ONE Dear Dr. Yoo, I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team. At this stage, our production department will prepare your paper for publication. This includes ensuring the following: * All references, tables, and figures are properly cited * All relevant supporting information is included in the manuscript submission, * There are no issues that prevent the paper from being properly typeset You will receive further¬†instructions from the production team, including instructions on how to review your proof when it¬†is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps. Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org You will receive an invoice from PLOS for your publication fee after your manuscript has reached the completed accept phase. If you receive an email requesting payment before acceptance or for any other service, this may be a phishing scheme. Learn how to identify phishing emails and protect your accounts at https://explore.plos.org/phishing If we can help with anything else, please email us at customercare@plos.org Thank you for submitting your work to PLOS ONE and supporting open access. Kind regards, PLOS ONE Editorial Office Staff on behalf of Dr. Zeheng Wang Academic Editor PLOS ONE ",
  "metadata": {
    "Title of this paper": "Acceptance letter",
    "Journal it was published in:": "PLOS One",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12494260/"
  }
}
{
  "title": "Paper_696",
  "abstract": "pmc Diagnostics (Basel) Diagnostics (Basel) 2841 diagno diagnostics Diagnostics 2075-4418 Multidisciplinary Digital Publishing Institute  (MDPI) PMC12468718 PMC12468718.1 12468718 12468718 41008698 10.3390/diagnostics15182326 diagnostics-15-02326 1 Article Automated Brain Tumor MRI Segmentation Using ARU-Net with Residual-Attention Modules https://orcid.org/0000-0002-9004-4802 Özbay Erdal Conceptualization Methodology Investigation Data curation Supervision 1 * https://orcid.org/0000-0003-0629-6888 Altunbey Özbay Feyza Conceptualization Methodology Investigation Data curation 2 Li Xingfeng Academic Editor 1 2 faltunbey@firat.edu.tr * erdalozbay@firat.edu.tr 13 9 2025 9 2025 15 18 497628 2326 20 8 2025 09 9 2025 12 9 2025 13 09 2025 27 09 2025 29 09 2025 © 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ Background/Objectives: Methods: Results: Conclusions: brain tumor segmentation attention Res-UNet neural network magnetic resonance imaging This research received no external funding. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction The growth of abnormal cells in the brain can lead to the formation of brain tumors, which are a major cause of morbidity and mortality worldwide. Gliomas, originating from glial cells, are broadly categorized into Higher Grade Gliomas (HGG) and Lower Grade Gliomas (LGG) [ 1 2 Manual segmentation of brain tumors in MRI images requires expert radiological knowledge and is prone to inter- and intra-observer variability. Automatic segmentation methods offer a reliable alternative by enabling the quantitative assessment of the tumor size, location, morphology, and grade [ 3 4 Recent advances in Deep Learning (DL) have demonstrated remarkable performance in brain tumor segmentation by learning complex hierarchical features from MRI data [ 5 6 7 8 9 10 11 12 13 14 Pre-processing effectiveness: We demonstrate that applying CLAHE, denoising, and Linear Kuwahara filtering significantly enhances the MRI image quality and directly improves the segmentation performance across multiple tumor classes. Novel ARU-Net architecture: We propose a U-shaped deep neural network that integrates residual connections with dual attention modules to better capture both channel and spatial dependencies. Adaptive Channel Attention (ACA) in the encoder: Applied to the lower convolutional and residual layers, ACA improves the feature refinement and strengthens the representation learning. Dimensional-space Triplet Attention (DTA) in the decoder: Applied to the upper convolutional layers, DTA enables more effective extraction and fusion of multi-scale features, leading to smoother and more accurate tumor boundaries. Comprehensive evaluation: Experimental results on the BTMRII dataset demonstrate that ARU-Net consistently outperforms U-Net, DenseNet121, and Xception, achieving superior quantitative metrics and qualitative segmentation quality. The remainder of the paper is organized as follows: Section 2 Section 3 Section 4 Section 5 Section 6 2. Related Works Brain tumor segmentation has long been a critical research area in medical image analysis due to its importance in clinical decision-making. Early studies were dominated by classical image processing techniques such as region-growing [ 15 16 17 18 19 With the advent of deep learning, CNN-based methods have rapidly replaced traditional approaches, achieving superior segmentation performance [ 20 21 22 23 24 25 26 27 Attention-based CNN extensions have also shown strong performance. Khorasani et al. embedded pre-trained backbones into U-Net for glioma subregion segmentation [ 28 29 30 31 More recently, Transformer and Mamba-based designs have gained attention. The SLCA-UNet [ 32 33 3 34 35 Overall, CNNs and FCNs have laid the foundation for modern brain tumor segmentation [ 5 6 7 8 36 37 38 39 40 41 42 3. Materials and Methods 3.1. Dataset The brain tumor MRI images used in this study were obtained from the BTMRII dataset, shared on the public platform Kaggle [ 43 Figure 1 3.2. Pre-Processing In this study, a three-stage pre-processing strategy was employed to improve the success of the segmentation model in MRI-based brain tumor images. First, CLAHE enhances the visibility of tumor structures by locally increasing the contrast between the tumors and surrounding tissues. However, this process also carries the risk of introducing noise; therefore, a powerful denoising method such as non-local means denoising is then applied to suppress random noise in the image. In the final stage, edge-preserving smoothing is performed with a Linear Kuwahara filter to preserve the edge details while smoothing the overall textural image. This sequence both corrects for density artifacts and helps compensate for noise that may occur after CLAHE application. The CLAHE, Denoising, and Kuwahara filter sequence is the ideal approach for suppressing post-contrast noise and preserving edges. This three-stage approach aimed to increase the segmentation performance of the model while preserving the anatomical accuracy of the tumor. Low-frequency illumination/bias (intensity inhomogeneity) in MRI images, resulting from coil and magnetic field imbalances, can affect segmentation and contrast processing. Therefore, algorithms applied in the pre-processing steps aim to address this imbalance [ 44 CLAHE enhances local contrast, facilitating the separation of tumor/edema regions from their surroundings. For this purpose, each MR image is divided into I x y 45 (1) I ′ = r o u n d L − 1 M N ∑ k = 0 i h ( k ) Here, L h k MN 46 (2) I ^ x = ∑ y ϵ Ω w x , y  I y ,  w x , y ∝ e x p − P x − P y 2 2 h 2 Here, P h The Linear Kuwahara filter was used to smooth the brain MRI images while preserving their edges [ 47 W μ k (3) μ k = 1 | R k | ∑ ( i , j ) ϵ R k I i , j ,  σ k 2 = 1 | R k | ∑ ( i , j ) ϵ R k ( I i , j − μ k ) 2 The average of the region with the lowest variance is selected as the output pixel, as given in Equation (4): (4) I ′ ( x , y ) = μ k * ,  k * = a r g min k σ k 2 In the linear variants, weighted sums of subregion averages were computed, yielding more stable outcomes. A 5 × 5 square window was employed. Consequently, the linear variants demonstrated greater robustness against block selection instability. Figure 2 3.3. General Framework In this study, the ARU-Net architecture was adopted as the baseline to achieve enhanced segmentation performance for brain tumor detection. The developed ARU-Net model consists of three main components: Context Information Transmission (CIT) [ 48 Figure 3 49 The input image has dimensions H W C H W C H W C K K S P H W C 49 (5) ω = t a n h ( α · ( U a , U b ) ( g ( z ) ) ) Here, g ( z ) = 1 W · H ∑ i = 1 H ∑ j = 1 W z i j 50 α · x α U a U b C × C r C r × C 49 3.3.1. Context Information Transmission (CIT) The CIT module begins by processing the input image with a 3 × 3 convolution, batch normalization, and ReLU activation [ 51 Figure 4 (6) Q 1 = R e L U ( B N ( C o n v 3 × 3 ( I ) ) ) Here, Q 1 Q 1 Q 2 Q 2 Q 3 Q 1 Q 3 R 1 Q 1 Q 3 3.3.2. Adaptive Channel Attention (ACA) The ACA module employs a 1D convolution-based structure to capture inter-channel dependencies between neighboring channels [ 52 Figure 5 (7) ω = t a n h ( α · C 1 D k ( y ) ) Here, k C 1 D k k C γ β (8) C = ϕ ( k ) ,  k ≈ l o g 2 ( C ) γ + β γ o d d This adaptive process enables the ARU-Net model, which was created for the purpose of detecting brain tumors, to highlight significant channels in multi-channel data and capture tumor traits at various sizes. 3.3.3. Dimensional-Space Triplet Attention (DTA) During feature fusion on the decoder side, an enhanced dimensional-space attention module is employed [ 53 Figure 6 x H W C W C W C 54 (9) y = 1 3 · ( x ^ 1 · ω 1 + x ^ 2 · ω 2 + x ^ 3 · ω 3 ) Here, ω 1 ω 2 ω 3 (10) ω a = tanh ( α · ψ a ( x ^ a * ) ) ψ a a To assess the segmentation performance of the proposed ARU-Net and baseline models, we employed several widely used quantitative evaluation metrics, including accuracy, precision, recall, F1-score, DSC, and IoU. These metrics provide complementary perspectives on the classification correctness, overlap quality, and boundary precision, and their mathematical definitions are given in the Section 4 4. Experimental Results In order to evaluate the effectiveness of the proposed model, a series of experiments were conducted under a controlled computational environment. The training and testing processes were implemented in Python 3.10 using the PyTorch 2.1.0 framework, running on a workstation equipped with an Intel Core i7 4.70 GHz processor, an NVIDIA RTX 4060Ti GPU, and 32 GB of RAM. During training, a batch size of six samples was employed to stabilize the gradient updates, while the Adam optimizer was adopted to achieve efficient convergence. In the pre-processing stage, all images from the BTMRII dataset were resized to ensure uniformity across the samples. To guarantee a fair evaluation, the dataset was randomly divided into three independent subsets: 60% for training, 20% for validation, and 20% for testing, following a 6:2:2 ratio. The results obtained from this setup are presented and analyzed in the subsequent subsections, with comparisons drawn against the related works in the literature. In addition to the proposed framework, several well-established DL architectures were also trained for comparative analysis. DenseNet121 and Xception models were implemented using the TensorFlow/Keras 2.15 environment. The input resolution was set to 224 × 224 for DenseNet121 and 299 × 299 for Xception, respectively. Both models employed the Adamax optimizer (learning rate = 0.001), along with an early stopping strategy (patience = 10, based on validation accuracy) to prevent overfitting. The best-performing checkpoints were stored during training for subsequent evaluation. For the U-Net configuration, the network was structured as a segmentation-based classifier, where the encoder backbone was initialized with EfficientNetB0. The input resolution was set to 224 × 224, and training was performed using the Adamax optimizer (learning rate = 1 × 10 −4 −4 In this study, DenseNet121, Xception, U-Net, and the proposed ARU-Net were selected for evaluation based on their complementary strengths and established performance in medical image analysis. DenseNet121, with its densely connected convolutional blocks, enables efficient feature reuse and mitigates vanishing gradient problems, making it a strong candidate for extracting discriminative features from MRI data. Xception, on the other hand, leverages depthwise separable convolutions to capture fine-grained spatial features while maintaining computational efficiency, which is particularly advantageous for high-resolution medical imaging tasks. For segmentation-oriented classification, U-Net was incorporated due to its widespread success in biomedical image segmentation, where the encoder–decoder structure and skip connections allow precise localization of tumor regions. To further advance this line of research, the proposed ARU-Net model was developed, integrating architectural refinements and an optimized training strategy to better handle class imbalance and heterogeneous tumor morphology. The inclusion of these models provided both baseline comparisons with well-established DL architectures and a platform to demonstrate the added value of the proposed ARU-Net in brain tumor classification and segmentation. To address the class imbalance within the BTMRII dataset, we employed two complementary strategies. First, we utilized a class-weighted categorical cross-entropy loss function, where higher weights were assigned to underrepresented classes. This adjustment penalized misclassification of rare tumor types more strongly, guiding the model toward improved balance across categories. Second, a balanced mini-batch sampling procedure was adopted to ensure that each training batch contained approximately equal representation from both majority and minority tumor classes. This reduced the risk of biased learning toward dominant categories. During the training phase, categorical cross-entropy loss was employed as the objective function for all classification-based models, including DenseNet121, Xception, U-Net, and the proposed ARU-Net. Cross-entropy is widely recognized as a suitable loss function for multi-class classification problems, as it measures the dissimilarity between the true label distribution and the predicted probability distribution [ 55 (11) L = − 1 N ∑ i = 1 N ∑ c = 1 C y i , c log ( y ^ i , c ) Here, the total number of samples is defined by N C y 56 57 (12) A c c u r a c y = T P + T N T P + T N + F P + F N (13) P r e c i s i o n = T P T P + F P (14) R e c a l l = T P T P + F N  (15) F 1 − s c o r e = 2 · P r e c i s i o n · P r e c i s i o n P r e c i s i o n + P r e c i s i o n  (16) I o U = T P T P + F P + F N (17) D S C = 2 T P 2 T P + F P + F N Here, TP TN FP FN Performance Analysis To systematically compare the effectiveness of different DL approaches in brain tumor segmentation, a series of experiments were conducted using the BTMRII dataset, and the corresponding results are organized into comprehensive tables and visualizations. Table 1 Table 1 In this study, the performance of four DL models, DenseNet121, Xception, U-Net, and the proposed ARU-Net, was systematically evaluated on the BTMRII dataset using both original (raw) and pre-processed MRI images. Table 1 U-Net also performed strongly, achieving 93.9% accuracy on pre-processed images, which underscores the advantage of encoder–decoder architectures for pixel-wise segmentation tasks. DenseNet121 and Xception, while primarily designed for image-level classification, achieved reasonable performance improvements with pre-processing but were outperformed by U-Net and ARU-Net in metrics sensitive to spatial segmentation quality (IoU and DSC). To evaluate the computational efficiency of each network, the training time per epoch was measured on an NVIDIA RTX 4060Ti GPU using a batch size of 8. This metric complements the reported accuracy and model complexity (GFlops and parameter count) by providing practical insights into the relative training speed of each architecture. Among the four models, ARU-Net consistently outperformed the others across all metrics and dataset types. On the pre-processed images, ARU-Net achieved the highest precision (99.0%), recall (95.7%), F1-score (98.1%), IoU (96.3%), and DSC (98.1%), while maintaining moderate computational complexity (788.4 GFlops) and a manageable number of parameters (30.45 M). This demonstrates the efficacy of its attention-based architecture and EfficientNetB0 encoder in capturing multi-scale contextual information. Confusion matrices, accuracy and loss curves, and multiclass ROC curves of the pre-processed experiment results of the BTMRII dataset for four different models are shown in Figure 7 Figure 8 Figure 9 Figure 10 Figure 11 Comparing the models on the raw versus pre-processed images highlights the importance of data enhancement strategies. All models exhibited lower performance on the raw images, particularly in IoU and DSC metrics, which are directly related to the segmentation quality. For example, DenseNet121’s IoU decreased from 70.3% (pre-processed) to 65.0% (raw), and ARU-Net’s DSC decreased from 98.1% to 95.0%. These differences suggest that pre-processing not only improves the classification accuracy but also enables models to better delineate tumor boundaries, which is critical for clinical applicability. To mitigate the potential overfitting observed in the training curves spanning 0 to 40 epochs (plotted in increments of 5), early stopping was applied based on validation loss with a patience of 10 epochs. Additionally, the input images were normalized, and the pre-processing steps including CLAHE, denoising, and Linear Kuwahara filtering were used to enhance the feature quality. A mini batch size of eight provided balanced gradient stability with model generalization. The comparative analysis also includes the computational efficiency. While U-Net has higher GFlops (796.5) due to its encoder–decoder design, ARU-Net achieves superior performance with lower computational cost (788.4 GFlops), demonstrating a favorable balance between accuracy and efficiency. DenseNet121 and Xception, although deeper and parameter-heavy, did not achieve comparable segmentation metrics, indicating that the model architecture and suitability for segmentation tasks are as important as the model depth and parameter count. Overall, these results emphasize three key points: (i) the application of pre-processing significantly enhances the model performance by improving the image quality and tumor visibility; (ii) attention-based encoder–decoder architectures such as ARU-Net are particularly effective for multi-class brain tumor segmentation; (iii) computationally efficient models can achieve a high segmentation performance without excessive parameter overhead, which is crucial for practical deployment in clinical settings. In conclusion, the combination of pre-processing strategies and carefully designed DL architectures can substantially improve automated brain tumor segmentation, enabling more accurate diagnosis and treatment planning. In this section, we present the experimental results obtained from the benchmark models (DenseNet121, Xception, and U-Net) and the proposed ARU-Net architecture. To comprehensively evaluate the performance, confusion matrices, multiclass ROC curves, and accuracy/loss curves were generated for each model. In addition, the quantitative performance metrics reported in Table 1 Table 1 The observed volatility in validation loss and accuracy in Figure 10 −4 To further highlight the differences between models, segmentation outputs are visualized in Figure 11 As illustrated in Figure 11 Overall, these findings confirm that ARU-Net not only improves upon traditional CNN-based architectures but also extends the capabilities of U-Net in both accuracy and robustness. The integration of cross-dimensional attention and residual connections in ARU-Net enables more discriminative feature representation, contributing to its superior segmentation quality. This strong performance across both quantitative metrics and qualitative evaluations underscores ARU-Net’s potential as a reliable model for automated brain tumor segmentation in clinical practice. As shown in Table 2 5. Discussion The results demonstrate that ARU-Net provides notable advantages over traditional U-Net and other deep learning architectures. The consistent performance gains indicate that residual connections effectively mitigate gradient degradation, while ACA and DTA modules enhance multi-scale feature extraction and channel–spatial dependency modeling. These findings align with recent studies that have emphasized the role of attention mechanisms in refining segmentation accuracy [ 21 22 23 24 25 26 27 Although our framework does not include a separate feature selection or classifier stage, the impact of each architectural component was assessed through ablation studies. Specifically, residual connections, ACA, and DTA modules were incrementally added to the baseline U-Net, and their respective contributions to segmentation performance were quantified. This provides a clear understanding of how each design choice contributes to the overall effectiveness of ARU-Net. One limitation of this study is the inherent class imbalance present in the BTMRII dataset, where certain tumor types are significantly underrepresented. To mitigate this issue, we adopted balanced mini-batch sampling and a class-weighted loss function rather than applying extensive data augmentation. This choice was made to preserve the original distributional characteristics of the dataset while still preventing bias toward the majority classes. Although effective in improving segmentation consistency across categories, future work could further investigate advanced imbalance handling strategies, such as focal loss or synthetic data generation, to enhance robustness. While the BTMRII dataset provides high-quality manual annotations, one limitation is that intra-tumoral heterogeneity (e.g., necrosis, edema, enhancing regions) was not separately annotated. Instead, tumors were annotated as whole regions within their respective categories. This choice simplifies the segmentation task and ensures inter-rater consistency across a large dataset, but it does not fully capture the biological complexity of tumor sub-regions. Future work could leverage datasets such as BraTS, where multi-label annotations of tumor sub-structures are available, to extend ARU-Net for more fine-grained clinical applications. Recent studies have also highlighted the effectiveness of integrating advanced deep learning strategies to enhance medical image segmentation. For instance, Sharif et al. demonstrated that hybrid convolutional recurrent architectures can significantly improve the modeling of spatial and contextual dependencies in medical images [ 58 59 60 In comparison with recent advanced architectures, the proposed ARU-Net demonstrates distinct advantages in multi-class brain tumor segmentation. For instance, the SLCA-UNet [ 32 33 3 34 35 In conclusion, the experimental results demonstrate that ARU-Net delivers outstanding performance in brain tumor segmentation. Compared to the traditional U-Net and other attention-based variants, ARU-Net not only enhances the segmentation accuracy but also shows superior generalization capability. This establishes ARU-Net as an effective and practical solution for automated brain tumor image analysis, making it a strong candidate for clinical applications. Nevertheless, future work should focus on expanding evaluations to broader tumor types and larger datasets, with particular attention to addressing potential information loss in complex tumor shapes. 6. Conclusions Brain tumors are among the most critical and life-threatening medical conditions, making accurate and automated segmentation of tumor regions in MRI scans essential for clinical diagnosis and treatment planning. In this study, we proposed ARU-Net, a novel DL architecture combining residual connections, ACA, and DTA modules to enhance the segmentation accuracy and robustness. The pre-processing pipeline, incorporating CLAHE, denoising, and Linear Kuwahara filtering, proved effective in improving the image quality, enhancing the feature clarity, and facilitating precise boundary delineation. Quantitative results on the BTMRII dataset show that ARU-Net significantly outperforms conventional methods, achieving 98.3% accuracy, 98.1% DSC, 96.3% IoU, and superior F1-scores, compared to DenseNet121, Xception, and U-Net. Moreover, visualizations of tumor regions demonstrate that ARU-Net provides smoother boundaries, more accurate contour delineation, and clearer differentiation of heterogeneous tumor structures than the other models. These findings confirm that the proposed architecture, combined with effective pre-processing, leads to highly reliable and precise tumor segmentation. Overall, ARU-Net presents a robust practical solution for automated brain tumor image analysis, contributing novel methodological insights to the literature and exhibiting strong potential for clinical application. Future work will focus on extending evaluations to more diverse tumor subtypes and larger datasets, while addressing challenges related to complex tumor morphology and information loss in segmentation. Disclaimer/Publisher’s Note: Author Contributions Conceptualization, E.Ö. and F.A.Ö.; data curation, E.Ö. and F.A.Ö.; investigation, E.Ö. and F.A.Ö.; methodology, E.Ö. and F.A.Ö.; supervision, E.Ö. All authors declare equal and joint responsibility for the study. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement All relevant data are fully available within the manuscript without restriction. The Kaggle data that support this study’s conclusions are publicly available online: https://www.kaggle.com/datasets/fernando2rad/brain-tumor-mri-images-17-classes Conflicts of Interest The authors declare no conflicts of interest. References 1. Ahir B.K. Engelhard H.H. Lakka S.S. Tumor development and angiogenesis in adult brain tumor: Glioblastoma Mol. Neurobiol. 2020 57 2461 2478 10.1007/s12035-020-01892-8 32152825 PMC7170819 2. Hu L.S. Hawkins-Daarud A. Wang L. Li J. Swanson K.R. Imaging of intratumoral heterogeneity in high-grade glioma Cancer Lett. 2020 477 97 106 10.1016/j.canlet.2020.02.025 32112907 PMC7108976 3. Seshimo H. Rashed E.A. Segmentation of low-grade brain tumors using mutual attention multimodal MRI Sensors 2024 24 7576 10.3390/s24237576 39686112 PMC11644188 4. Mohammed Y.M. El Garouani S. Jellouli I. A survey of methods for brain tumor segmentation-based MRI images J. Comput. Des. Eng. 2023 10 266 293 10.1093/jcde/qwac141 5. Liu Z. Tong L. Chen L. Jiang Z. Zhou F. Zhang Q. Zhang X. Jin Y. Zhou H. Deep learning based brain tumor segmentation: A survey Complex Intell. Syst. 2023 9 1001 1026 10.1007/s40747-022-00815-5 6. Allah A.M.G. Sarhan A.M. Elshennawy N.M. Edge U-Net: Brain tumor segmentation using MRI based on deep U-Net model with boundary information Expert Syst. Appl. 2023 213 118833 10.1016/j.eswa.2022.118833 7. Zebari N.A. Alkurdi A.A. Marqas R.B. Salih M.S. Enhancing brain tumor classification with data augmentation and densenet121 Acad. J. Nawroz Univ. 2023 12 323 334 10.25007/ajnu.v12n4a1985 8. Sathya R. Mahesh T.R. Bhatia Khan S. Malibari A.A. Asiri F. Rehman A.U. Malwi W.A. Employing Xception convolutional neural network through high-precision MRI analysis for brain tumor diagnosis Front. Med. 2024 11 1487713 10.3389/fmed.2024.1487713 39606635 PMC11601128 9. Havaei M. Davy A. Warde-Farley D. Biard A. Courville A. Bengio Y. Pal C. Jodoin P.-M. Larochelle H. Brain tumor segmentation with deep neural networks Med. Image Anal. 2017 35 18 31 10.1016/j.media.2016.05.004 27310171 10. Khan M.A. Park H. Adaptive channel attention and multi-path convolutional architecture for brain tumor detection using MRI images Multimed. Tools Appl. 2025 1 28 10.1007/s11042-025-20911-1 11. Li Y. Yang J. Ni J. Elazab A. Wu J. TA-Net: Triple attention network for medical image segmentation Comput. Biol. Med. 2021 137 104836 10.1016/j.compbiomed.2021.104836 34507157 12. Ghosh S. Chaki A. Santosh K.C. Improved U-Net architecture with VGG-16 for brain tumor segmentation Phys. Eng. Sci. Med. 2021 44 703 712 10.1007/s13246-021-01019-w 34047928 13. Xu W. Yang H. Zhang M. Cao Z. Pan X. Liu W. Brain tumor segmentation with corner attention and high-dimensional perceptual loss Biomed. Signal Process. Control. 2022 73 103438 10.1016/j.bspc.2021.103438 14. Yogalakshmi G. Rani B.S. Sailfish optimizer based CLAHE with U-NET for MRI brain tumour segmentation Meas. Sens. 2024 33 101229 10.1016/j.measen.2024.101229 15. Biratu E.S. Schwenker F. Debelee T.G. Kebede S.R. Negera W.G. Molla H.T. Enhanced region growing for brain tumor MR image segmentation J. Imaging 2021 7 22 10.3390/jimaging7020022 34460621 PMC8321280 16. Agrawal R. Sharma M. Singh B.K. Segmentation of brain tumour based on clustering technique: Performance analysis J. Intell. Syst. 2019 28 291 306 10.1515/jisys-2017-0027 17. Sivakumar V. Janakiraman N. A novel method for segmenting brain tumor using modified watershed algorithm in MRI image with FPGA Biosystems 2020 198 104226 10.1016/j.biosystems.2020.104226 32861800 18. Özbay E. Özbay F.A. Interpretable features fusion with precision MRI images deep hashing for brain tumor detection Comput. Methods Programs Biomed. 2023 231 107387 10.1016/j.cmpb.2023.107387 36738605 19. Saleh M.M. Salih M.E. Ahmed M.A. Hussein A.M. From traditional methods to 3d u-net: A comprehensive review of brain tumour segmentation techniques J. Biomed. Sci. Eng. 2025 18 1 32 10.4236/jbise.2025.181001 20. Sajjanar R. Dixit U.D. Vagga V.K. Advancements in hybrid approaches for brain tumor segmentation in MRI: A comprehensive review of machine learning and deep learning techniques Multimed. Tools Appl. 2024 83 30505 30539 10.1007/s11042-023-16654-6 21. Zhou R. Wang J. Xia G. Xing J. Shen H. Shen X. Cascade residual multiscale convolution and mamba-structured unet for advanced brain tumor image segmentation Entropy 2024 26 385 10.3390/e26050385 38785634 PMC11120374 22. Cao T. Wang G. Ren L. Li Y. Wang H. Brain tumor magnetic resonance image segmentation by a multiscale contextual attention module combined with a deep residual UNet (MCA-ResUNet) Phys. Med. Biol. 2022 67 095007 10.1088/1361-6560/ac5e5c 35294935 23. Zhu Z. Wang Z. Qi G. Mazur N. Yang P. Liu Y. Brain tumor segmentation in MRI with multi-modality spatial information enhancement and boundary shape correction Pattern Recognit. 2024 153 110553 10.1016/j.patcog.2024.110553 24. Raza R. Bajwa U.I. Mehmood Y. Anwar M.W. Jamal M.H. dResU-Net: 3D deep residual U-Net based brain tumor segmentation from multimodal MRI Biomed. Signal Process. Control. 2023 79 103861 10.1016/j.bspc.2022.103861 25. Diakogiannis F.I. Waldner F. Caccetta P. Wu C. ResUNet-a: A deep learning framework for semantic segmentation of remotely sensed data ISPRS J. Photogramm. Remote Sens. 2020 162 94 114 10.1016/j.isprsjprs.2020.01.013 26. Yadav A.C. Kolekar M.H. Sonawane Y. Kadam G. Tiwarekar S. Kalbande D.R. Effunet++: A novel architecture for brain tumor segmentation using flair MRI images IEEE Access 2024 12 152430 152443 10.1109/ACCESS.2024.3480271 27. Aboussaleh I. Riffi J. el Fazazy K. Mahraz A.M. Tairi H. 3DUV-NetR+: A 3D hybrid semantic architecture using transformers for brain tumor segmentation with MultiModal MR images Results Eng. 2024 21 101892 10.1016/j.rineng.2024.101892 28. Khorasani A. Enhanced glioma semantic segmentation using U-net and pre-trained backbone U-net architectures Sci. Rep. 2025 15 31821 10.1038/s41598-025-17895-1 40883396 PMC12397208 29. Ma B. Sun Q. Ma Z. Li B. Cao Q. Wang Y. Yu G. DTASUnet: A local and global dual transformer with the attention supervision U-network for brain tumor segmentation Sci. Rep. 2024 14 28379 10.1038/s41598-024-78067-1 39551805 PMC11570615 30. Wen L. Sun H. Liang G. Yu Y. A deep ensemble learning framework for glioma segmentation and grading prediction Sci. Rep. 2025 15 4448 10.1038/s41598-025-87127-z 39910114 PMC11799385 31. Khorasani A. Kafieh R. Saboori M. Tavakoli M.B. Glioma segmentation with DWI weighted images, conventional anatomical images, and post-contrast enhancement magnetic resonance imaging images by U-Net Phys. Eng. Sci. Med. 2022 45 925 934 10.1007/s13246-022-01164-w 35997927 32. Tejashwini P.S. Thriveni J. Venugopal K.R. A novel SLCA-UNet architecture for automatic MRI brain tumor segmentation Biomed. Signal Process. Control. 2025 100 107047 10.1016/j.bspc.2024.107047 33. Chen B. Sun Q. Han Y. Liu B. Zhang J. Zhang Q. Adaptive cascaded transformer U-Net for MRI brain tumor segmentation Phys. Med. Biol. 2024 69 115036 10.1088/1361-6560/ad4081 38636503 34. Wang G. Li Y. Chen W. Ding M. Cheah W.P. Qu R. Shen L. S 3 Proceedings of the AAAI Conference on Artificial Intelligence Burlingame, CA, USA 31 March–2 April 2025 Volume 39 7655 7664 10.1609/aaai.v39i7.32824 35. Zhang M. Sun Q. Han Y. Zhang J. Edge-interaction Mamba Network for MRI Brain Tumor Segmentation Proceedings of the ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) Hyderabad, India 6–11 April 2025 1 5 10.1109/ICASSP49660.2025.10889470 36. Rabby S.F. Arafat M.A. Hasan T. BT-Net: An end-to-end multi-task architecture for brain tumor classification, segmentation, and localization from MRI images Array 2024 22 100346 10.1016/j.array.2024.100346 37. Pacal I. Celik O. Bayram B. Cunha A. Enhancing EfficientNetv2 with global and efficient channel attention mechanisms for accurate MRI-Based brain tumor classification Clust. Comput. 2024 27 11187 11212 10.1007/s10586-024-04532-1 38. Saifullah S. Dreżewski R. Yudhana A. Wielgosz M. Caesarendra W. Modified U-Net with attention gate for enhanced automated brain tumor segmentation Neural Comput. Appl. 2025 37 5521 5558 10.1007/s00521-024-10919-3 39. Weld A. Dixon L. Anichini G. Patel N. Nimer A. Dyck M. O’nEill K. Lim A. Giannarou S. Camp S. Challenges with segmenting intraoperative ultrasound for brain tumours Acta Neurochir. 2024 166 317 10.1007/s00701-024-06179-8 39090435 PMC11294268 40. Rasool N. Bhat J.I. Wani N.A. Ahmad N. Alshara M. Transresunet: Revolutionizing glioma brain tumor segmentation through transformer-enhanced residual unet IEEE Access 2024 12 72105 72116 10.1109/ACCESS.2024.3402947 41. Das S. Goswami R.S. Advancements in brain tumor analysis: A comprehensive review of machine learning, hybrid deep learning, and transfer learning approaches for MRI-based classification and segmentation Multimed. Tools Appl. 2025 84 26645 26682 10.1007/s11042-024-20203-0 42. Musthafa N. Memon Q.A. Masud M.M. Advancing Brain Tumor Analysis: Current Trends, Key Challenges, and Perspectives in Deep Learning-Based Brain MRI Tumor Diagnosis Eng 2025 6 82 10.3390/eng6050082 43. Brain Tumor MRI Images 17 Classes Available online: https://www.kaggle.com/datasets/fernando2rad/brain-tumor-mri-images-17-classes (accessed on 20 August 2025) 44. Veerabadrappa K. Marappan S. Natesan S.V. John R. Lung Disease Prediction using Multi-Level Atrous Spatial Pyramid Pooling Based Convolutional Neural Network Int. J. Intell. Eng. Syst. 2025 18 622 632 10.22266/ijies2025.0531.40 45. Roy S. Bhalla K. Patel R. Mathematical analysis of histogram equalization techniques for medical image enhancement: A tutorial from the perspective of data loss Multimed. Tools Appl. 2024 83 14363 14392 10.1007/s11042-023-15799-8 46. Juneja M. Rathee A. Verma R. Bhutani R. Baghel S. Saini S.K. Jindal P. Denoising of magnetic resonance images of brain tumor using BT-Autonet Biomed. Signal Process. Control. 2024 87 105477 10.1016/j.bspc.2023.105477 47. Kafadar Ö. Applications of the Kuwahara and Gaussian filters on potential field data J. Appl. Geophys. 2022 198 104583 10.1016/j.jappgeo.2022.104583 48. Xiao L. Zhou B. Fan C. Automatic brain MRI tumors segmentation based on deep fusion of weak edge and context features Artif. Intell. Rev. 2025 58 154 10.1007/s10462-025-11151-8 49. Özbay E. Özbay F.A. Interpretable pap-smear image retrieval for cervical cancer detection with rotation invariance mask generation deep hashing Comput. Biol. Med. 2023 154 106574 10.1016/j.compbiomed.2023.106574 36738706 50. Malla P.P. Sahu S. Alutaibi A.I. Classification of tumor in brain MR images using deep convolutional neural network and global average pooling Processes 2023 11 679 10.3390/pr11030679 51. Sankar M. Baiju B.V. Preethi D. Ananda K.S. Sandeep K.M. Mohd A.S. Efficient brain tumor grade classification using ensemble deep learning models BMC Med. Imaging 2024 24 1 22 10.1186/s12880-024-01476-1 39487431 PMC11529038 52. Shawly T. Alsheikhy A.A. A Novel Self-Attention Transfer Adaptive Learning Approach for Brain Tumor Categorization Int. J. Intell. Syst. 2024 2024 8873986 10.1155/2024/8873986 53. Rutoh E.K. Guang Q.Z. Bahadar N. Raza R. Hanif M.S. GAIR-U-Net: 3D guided attention inception residual u-net for brain tumor segmentation using multimodal MRI images J. King Saud Univ.-Comput. Inf. Sci. 2024 36 102086 10.1016/j.jksuci.2024.102086 54. Safwan M.N. Rahman S. Mahadi M.H. Mobin M.I. Jabir T.M. Aung Z. Mridha M.F. T3SSLNet: Triple-Method Self-Supervised Learning for Enhanced Brain Tumor Classification in MRI IEEE Access 2025 13 127852 127867 10.1109/ACCESS.2025.3589619 55. Shafiei N.M. Jahani H. Khodarahmi M. Zahiri J. Yekaninejad M.S. A quantitative comparison between focal loss and binary cross-entropy loss in brain tumor auto-segmentation using U-Net SSRN Electron. J. 2022 11 1 19 10.2139/ssrn.4142314 56. Sarıateş M. Özbay E. Transfer Learning-Based Ensemble of CNNs and Vision Transformers for Accurate Melanoma Diagnosis and Image Retrieval Diagnostics 2025 15 1928 10.3390/diagnostics15151928 40804891 PMC12346797 57. Kumar P.S. Sakthivel V.P. Raju M. Sathya P.D. Brain tumor segmentation of the FLAIR MRI images using novel ResUnet Biomed. Signal Process. Control. 2023 82 104586 10.1016/j.bspc.2023.104586 58. Sharif M.I. Khan M.A. Alhussein M. Aurangzeb K. Raza M. A decision support system for multimodal brain tumor classification using deep learning Complex Intell. Syst. 2022 8 3007 3020 10.1007/s40747-021-00321-0 59. Khalil M. Sharif M.I. Naeem A. Chaudhry M.U. Rauf H.T. Ragab A.E. Deep Learning-Enhanced Brain Tumor Prediction via Entropy-Coded BPSO in CIELAB Color Space Comput. Mater. Contin. 2023 77 2031 2047 10.32604/cmc.2023.043687 60. Khan M.A. Rubab S. Kashif A. Sharif M.I. Muhammad N. Shah J.H. Satapathy S.C. Lungs cancer classification from CT images: An integrated design of contrast based classical features fusion and selection Pattern Recognit. Lett. 2020 129 77 85 10.1016/j.patrec.2019.11.014 Figure 1 Sample images from each of the six classes in the BTMRII dataset. Figure 2 Sample images of each pre-processed stage from the BTMRII dataset. Figure 3 General framework of the implemented ARU-Net network with three branches. Figure 4 The procedure of the basic CIT module. Figure 5 The procedure of the basic ACA module. Figure 6 The procedure of the basic DTA module. Figure 7 Performance results of the DenseNet121 model. Figure 8 Performance results of the Xception model. Figure 9 Performance results of the U-Net model. Figure 10 Performance results of the proposed ARU-Net model. Figure 11 Segmentation results of the models for BRMRII dataset. diagnostics-15-02326-t001_Table 1 Table 1 Performance results of the BTMRII dataset. Dataset Models Acc (%) Pre (%) Rec (%) F1 (%) IoU (%) DSC (%) GFlops Param Time/ Original DenseNet121 77.5 82.0 75.0 78.4 65.0 78.4 412.3 8 M 45 Xception 81.0 82.0 79.0 80.5 70.0 80.5 675.2 22.9 M 52 U-Net 90.0 91.0 88.5 89.7 83.0 89.7 796.5 31.05 M 60 ARU-Net 96.0 97.0 93.0 95.0 94.0 95.0 788.4 30.45 M 58 Pre-processed DenseNet121 82.2 86.3 80.1 82.4 70.3 82.4 412.3 8 M 45 Xception 85.3 86.7 83.3 85.2 74.3 85.2 675.2 22.9 M 52 U-Net 93.9 94.3 93.3 93.7 86.6 93.7 796.5 31.05 M 60 ARU-Net 98.3 99.0 95.7 98.1 96.3 98.1 788.4 30.45 M 58 diagnostics-15-02326-t002_Table 2 Table 2 Ablation results of the BTMRII dataset. Methods Acc (%) Pre (%) Rec (%) F1 (%) IoU (%) DSC (%) U-Net 93.9 94.3 93.3 93.7 86.6 93.7 U-Net + Res 95.7 95.9 95.3 95.6 91.6 95.6 U-Net + Res + ACA 97.1 97.2 96.9 97.0 94.3 97.0 U-Net + Res + ACA + DTA (ARU-Net) 98.3 99.0 95.7 98.1 96.3 98.1 ",
  "metadata": {
    "Title of this paper": "Lungs cancer classification from CT images: An integrated design of contrast based classical features fusion and selection",
    "Journal it was published in:": "Diagnostics",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12468718/"
  }
}
{
  "title": "Paper_342",
  "abstract": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12485157 PMC12485157.1 12485157 12485157 41028129 10.1038/s41598-025-13824-4 13824 1 Article Attention-enhanced hybrid U-Net for prostate cancer grading and explainability Zaheer Ahmad Nawaz alpha@jxut.edu.cn 1 Farhan Muhammad 2 Min Guilin 1 3 Alotaibi Faiz Abdullah 4 Alnfiai Mrim M. 5 1 https://ror.org/05k2j8e48 grid.495244.a 0000 0004 1761 5722 BOYA International College, Jiangxi University of Technology, 2 https://ror.org/00nqqvk19 grid.418920.6 0000 0004 0607 0704 Department of Computer Science, COMSATS University Islamabad, Sahiwal Campus, 3 https://ror.org/05k2j8e48 grid.495244.a 0000 0004 1761 5722 College of Finance and Economics, Jiangxi University of Technology, 4 https://ror.org/02f81g417 grid.56302.32 0000 0004 1773 5396 Department of Information Science, College of Humanities and Social Sciences, King Saud University, 5 https://ror.org/014g1a453 grid.412895.3 0000 0004 0419 5255 Department of Information Technology, College of Computers and Information Technology, Taif University, 30 9 2025 2025 15 478255 34038 8 4 2025 28 7 2025 30 09 2025 02 10 2025 02 10 2025 © The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access http://creativecommons.org/licenses/by-nc-nd/4.0/ Prostate cancer remains a leading cause of mortality, necessitating precise histopathological segmentation for accurate Gleason Grade assessment. However, existing deep learning-based segmentation models lack contextual awareness and explainability, leading to inconsistent performance across heterogeneous tissue structures. Conventional U-Net architectures and CNN-based approaches struggle with capturing long-range dependencies and fine-grained histopathological patterns, resulting in suboptimal boundary delineation and model generalizability. To address these limitations, we propose a transformer-attention hybrid U-Net (TAH U-Net), integrating hybrid CNN-transformer encoding, attention-guided skip connections, and a multi-stage guided loss mechanism for enhanced segmentation accuracy and model interpretability. The ResNet50-based convolutional layers efficiently capture local spatial features, while Vision Transformer (ViT) blocks model global contextual dependencies, improving segmentation consistency. Attention mechanisms are incorporated into skip connections and decoder pathways, refining feature propagation by suppressing irrelevant tissue noise while enhancing diagnostically significant regions. A novel hierarchical guided loss function optimizes segmentation masks at multiple decoder stages, improving boundary refinement and gradient stability. Additionally, Explainable AI (XAI) techniques such as LIME, Occlusion Sensitivity, and Partial Dependence Analysis (PDP), validate the model’s decision-making transparency, ensuring clinical reliability. The experimental evaluation on the SICAPv2 dataset demonstrates state-of-the-art performance, surpassing traditional U-Net architectures with a 4.6% increase in Dice Score, 5.1% gain in IoU, along with notable improvements in Precision (+ 4.2%) and Recall (+ 3.8%). This research significantly advances AI-driven prostate cancer diagnostics by providing an interpretable and highly accurate segmentation framework, enhancing clinical trust in histopathology-based grading within medical imaging and computational pathology. Keywords Prostate cancer segmentation Transformer-attention hybrid U-Net Histopathology imaging Explainable AI (XAI) Multi-stage guided loss Gleason grade classification Subject terms Cancer Health care Mathematics and computing Taif University, Saudi Arabia TU-DSPP-2024-41 Alnfiai Mrim M. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement © Springer Nature Limited 2025 Introduction However, prostate cancer is still one of the most common and frightening cancers in men the world over. Grading prostate cancer early and accurately to determine more effective treatment plans, which is important for better patient outcomes 1 2 3 4 5 7 8 Recent advances in medical image analysis have leveraged attention mechanisms and multi-scale feature fusion to improve diagnostic accuracy across various clinical domains. For instance, MLCA2F introduced a multi-level context attentional framework for COVID-19 lesion segmentation from CT scans 9 10 11 12 These models are remarkable in extracting high-quality fine-grained spatial features 13 14 15 16 17 18 19 A significant step in prostate cancer grading is hybrid U-Net architectures combining the strength of traditional U Net segmentation models with attention mechanisms and a transformer-based encoder. This attention gated and squeeze and excitation (SE) blocked models 20 21 In this paper, we put forward a novel segmentation architecture, namely, Transformer Attention Hybrid U-Net, specifically engineered for prostate cancer grading. In addition, the model provides encoders that are able to extract both local spatial features and global contextual dependencies by integrating a hybrid CNN-transformer encoder into the model; hence, it is able to accurately segment the Gleason Grade regions. For fine-grained feature extraction, the encoder is based on convolutional layers of ResNet50 followed by ViT layers, to learn long-range dependencies in histopathological images. Attention-guided skips are included in the decoder to refine feature maps and keep critical tissue structures. Furthermore, a multi-stage guided loss mechanism is introduced to enable multiple stages of hierarchical feature representations to be refined in order to positively impact segment accuracy. The model makes use of XAI techniques, Grad-CAM, and attention heatmaps to present visual understanding of its decision making, improving clinical interpretability and reliable diagnostics. The novelty of TAH U-Net lies in its unified design that combines ViT-based self-attention with convolutional encoding in a hybrid structure. In contrast to models like Attention U-Net that apply attention solely at skip connections, TAH U-Net employs both cross-branch attention fusion and attention-guided refinement in the decoder. This allows the model to capture more discriminative features across spatial scales and improves interpretability in complex histopathology settings. The main contribution of this is as follows: The proposed model integrates a hybrid Transformer-CNN encoder, combining ResNet50-based convolutional layers for local feature extraction with ViT blocks to capture long-range dependencies and contextual relationships in prostate histopathology images. This novel fusion enhances the segmentation of heterogeneous Gleason Grade regions, addressing limitations of traditional U-Net architectures that struggle with complex tissue structures. A novel attention-enhanced skip connection mechanism is introduced, leveraging spatial and channel-wise attention (Squeeze-and-Excitation Blocks) to selectively refine feature propagation between the encoder and decoder stages. This ensures diagnostically relevant tissue regions are emphasized while suppressing background noise, significantly improving segmentation precision for prostate cancer grading. Unlike conventional models that rely on a single loss function at the output layer, the proposed approach employs a hierarchical guided loss mechanism, applying Binary Cross-Entropy (BCE), Dice Loss, and intermediate Guided Loss at multiple decoder stages. This strategy stabilizes gradient flow, enhances boundary preservation, and reduces false positives, leading to highly refined and clinically interpretable segmentation masks. The model incorporates XAI techniques, including Grad-CAM and transformer-based attention heatmaps, to provide visual transparency in segmentation decisions. This novel interpretability framework ensures that critical histopathological regions influencing model predictions are identifiable, bridging the gap between AI-driven automation and clinical trustworthiness, making the system highly suitable for real-world diagnostic applications. Literature review Methodology Experimental results Comparison with cutting edge technologies Discussion and analysis Conclusion and future work Literature review Imaging and orthogonal diagnostic advancements are driven by prostate cancer, a leading cause of cancer-related deaths in men. Prostate cancer detection, segmentation, and classification are revolutionized with deep learning, pushing higher accuracy, efficiency, and reproducibility. Key innovations in Dl architectures, hybrid models, and explainable n are used in various clinical tasks to tackle the challenges of annotation noise, image variability, and dataset imbalance in DL models, which are then shown to have clinical impact. MicroSegNet is a multi-scale transformer UNet model for accurate prostate segmentation on micro-US images, which overcomes artifacts and indistinct boundaries as proposed by Jiang et al. 22 23 The challenge of personalized radiotherapy for prostate cancer is described by Bhandary et al. 24 25 26 A deep learning masked (DLM) auto-fixed volume of interest (VOI) segmentation method for diagnosing clinically significant prostate cancer (CS PCa) from biparametric MRI is illustrated by Bleker et al. 27 28 Recent studies have advanced medical image segmentation and classification through the integration of attention mechanisms, residual connections, and transformer-based architectures. The DCSSGA-UNet model 29 30 31 32 An attempt at aligning machine learning with pathologists’ diagnostic criteria for prostate cancer detection and grading is proposed by Ferrero et al. 33 34 35 Advancement of prostate cancer diagnosis, segmentation, and grading by deep learning techniques is discussed in the literature review. MicroSegNet, ResQuNet, NRD net, and hybrid frameworks achieve high accuracy across imaging modalities, including micro ultrasound, MRI, and histopathology. Challenges related to the annotation noise and dataset imbalance are approached using AI-driven approaches such as Vision Transformers and XGBoost. Among those, Attention-Enhanced Hybrid U-Net showcases effectiveness by enhancing the capture of global and local features, improving segmentation accuracy, and boosting diagnostic transparency. These attention-based hybrid models show promise as a reliable alternative method for prostate cancer grading and clinical decision making. Methodology In this section, we present the TAH U-Net, a novel deep learning architecture for prostate cancer segmentation. The proposed model leverages a hybrid CNN-Transformer encoder, attention mechanisms, and multi-stage guided loss to enhance segmentation accuracy and clinical interpretability. Dataset preparation In this paper, the SICAPv2 dataset, a publicly available dataset of histopathological Whole Slide Images (WSI) with Gleason grades for prostate cancer grading, is used as our dataset. It consists of expert-verified high-resolution tissue samples for reliable AI-based segmentation and classification 36 37 1 Table 1 Dataset summary of SICAPv2 including composition, annotations, Gleason grading, and preprocessing details. Attribute 1 Value 1 Attribute 2 Value 2 Attribute 3 Value 3 Dataset name SICAPv2 Source Public histopathology dataset Total images 10,000 Number of patients 1500 Annotated slides 8500 Resolution 1024 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} Dataset size (GB) \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\approx$$\\end{document} Color channels RGB (3 channels) Gleason grade 3 3500 Gleason grade 4 3000 Gleason grade 5 2000 Non-cancerous samples 1500 Annotation format Pixel-level annotations Segmentation labels Cancerous, non-cancerous Normalization applied Stain normalization Quality control Expert verification, outlier removal Training set 7000 Validation set 1500 Testing set 1500 Augmentation techniques Rotation, flipping, zooming, contrast adjustments – – Preprocessing A preprocessing pipeline is employed such that the Whole Slide Images (WSI) of the SICAPv2 dataset used for grading on the prostate cancer data are also optimized for segmentation in deep learning-based frameworks. In the first step, all histopathological images are resampled to 1024 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^\\circ$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$^\\circ$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma$$\\end{document} 38 Feature extraction The feature extraction process shown in Fig. 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} 39 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} The ViT was selected over alternatives such as Swin Transformer, UNETR, and SAM due to its ability to model full-sequence global self-attention across image patches. Swin Transformer restricts attention to local windows, which limits its effectiveness in capturing long-range dependencies. UNETR integrates transformers within a U-Net framework but typically requires extensive pretraining and more training time. SAM is optimized for general-purpose segmentation and lacks histopathology-specific supervision mechanisms. The ViT-CNN hybrid encoder used in this model combines low-level spatial feature extraction with global contextual learning, providing an efficient and effective design for prostate cancer histopathology segmentation. Figure 1 Feature extraction pipeline using ResNet50 with frozen layers, batch normalization and fully connected layers for prostate cancer grading. Attention-enhanced hybrid U-Net model for prostate cancer grading A novel segmentation architecture, called the Attention-Enhanced Hybrid U-Net, is proposed for the task of prostate cancer grading from WSI. However, existing U-Net models lack in global context understanding, and are thus ineffective in recognizing complex patterns of histopathological patterns. For this, our model applies Attention Mechanisms such as Squeeze-and-Excitation (SE) Blocks, Spatial Attention, and Dual Attention Modules at different levels in this network. They selectively enhance the important region and suppress the irrelevant background noise so as to improve the representation capability. Moreover, attention mechanisms are introduced in the encoder, decoder, as well as skip connections to preserve fine-grained spatial details and capture long-range dependencies. This approach resorts to WSI to emphasize diagnostically significant regions within the image and improves the segmentation and classification precision and reliability of Gleason grades (GG3, GG4, GG5). An innovation of this architecture is to integrate a Hybrid Encoder that contains ViT layers and CNN. The model combines the former via convolutional layers and the latter via the transformer encoder, as a hybrid approach. Feature extraction using the Transformer improves the model’s capability for finding complicated tissue structures and its capacity for transferring across different staining protocols, improving the robustness in real-world clinical settings. Furthermore, the ASPP module in the bottleneck also uses multi-scale contextual information, which improves the segmentation accuracy. Skip connections in the attention-enhanced stage ensure efficient feature fusion between the encoder and decoder stages, without losing much information and giving an improved segmentation boundary. This hybrid U-Net architecture provides significant improvement in prostate cancer detection, achieving state-of-the-art in segmentation, AI-driven diagnostics having the trustworthiness, and practical clinical applicability. The overall architecture of the TAH U-Net for prostate cancer grading, integrating ViT and attention mechanisms for improved segmentation, is demonstrated in Fig. 2 2 Figure 2 Overall architecture of the attention-enhanced hybrid U-Net for prostate cancer grading, integrating vision transformers and attention mechanisms for improved segmentation. The hybrid encoder takes the input image of dimensions \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I\\times X\\times E$$\\end{document} D \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ViT$$\\end{document} m 1 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} G^{(m)} = ViT \\left( Conv \\left( G^{(m-1)}, X^{(m)}, c^{(m)} \\right) \\right) \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G^{(m)}$$\\end{document} m \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X^{\\left( m\\right) },c^{(m)}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ViT$$\\end{document} 2 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} A = MSA \\left( NN (Y) + Y \\right) + MLP \\left( NN (A) + A \\right) \\end{aligned}$$\\end{document} A \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ViT$$\\end{document} Y \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$NN$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$MSA$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$MLP$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Y$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$V$$\\end{document} 3 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} h_j = \\text {softmax}\\left( \\frac{Q_j K_j^\\top }{\\sqrt{d_k}} \\right) V_j \\end{aligned}$$\\end{document} 4 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} MSA (R, L, W) = Concat (h_1, h_2, \\dots , h_i) X^p \\end{aligned}$$\\end{document} R L W \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_l$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X^p$$\\end{document} 5 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\alpha = \\sigma _2\\left( X_h h + X_y y + c_{ att } \\right) \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha$$\\end{document} h y \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X_h$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X_y$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c_{ att }$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _2$$\\end{document} 6 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} G_{ att } = \\alpha \\odot y \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{ att }$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\odot }$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha$$\\end{document} y 7 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} a = \\frac{1}{I \\times X} \\sum _{j=1}^I \\sum _{k=1}^X G(j, k) \\end{aligned}$$\\end{document} 8 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} t = \\sigma _2\\left( X_2 \\, \\delta \\left( X_1 a \\right) \\right) \\end{aligned}$$\\end{document} 9 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} G_{ SE } = t \\odot G \\end{aligned}$$\\end{document} G a \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X_1$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X_2$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _2$$\\end{document} m 10 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} G_{ up }^{(m)} = Upsample \\left( G^{(m+1)}\\right) + G_{ att }^{(m)} \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{ up }^{\\left( m\\right) }$$\\end{document} m \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Upsample$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_{ att }^{\\left( m\\right) }$$\\end{document} 11 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} M = \\lambda _1 M_{ BCE }(Q, Z) + \\lambda _2 M_{ Dice }(Q, Z) \\end{aligned}$$\\end{document} Enhanced hybrid U-Net for prostate cancer segmentation This paper proposes TAH U-Net, a new multi-scale segmentation architecture that is specifically designed for prostate cancer histopathology imaging by extending the U-Net with hybrid CNN-transformer encoders, guided loss mechanisms, and attention-enhanced skip connections, and yields significant performance improvement for segmentation accuracy. To make this architecture for prostate tissue analysis, multi-stage supervision is used for iteratively refining segmentation masks across different abstraction layers. The model leverages on hybrid encoding scheme and fine-tuned loss function to address the major problems of existing prostate cancer segmentation methods based on U-Net: the lack of boundary preservation, context-aware feature extraction, and better interpretation of the diagnostic reasoning for the segmentation. SICAPv2 dataset, which contains 10,000 high-resolution WSIs at 1024 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} 3 Figure 3 Internal architecture of the TAH U-Net with multi-stage guided loss. Intermediate decoder outputs \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_1, D_2, D_3$$\\end{document} Attention-guided skip connections Attention-guided skip connections are introduced in order to retain diagnostically significant features in the prostate cancer segmentation, in the sense that the network filters out irrelevant regions and preserves important information. The application emphasizes high sensitivity and precision, which is particularly important for Gleason Grade classification that requires an appreciation of fine tissue structures. The spatial attention mechanism of each skip connection increases the emphasis on vital regions in and around cancer areas and decreases emphasis on background regions, thereby not contributing to noise and accelerating the network’s focus on cancer areas. Moreover, SE Blocks recalibrate the feature channels such that they dynamically scale the contributions from various feature maps to improve the segmentation accuracy. With these mechanisms integrated into the model, it can capture local and global characteristics of prostate tissue, which results in better delineation of the boundary and thus more reliable segmentation outputs. These attention-guided skip connections play a dual role: (1) suppressing irrelevant or noisy tissue structures such as stromal regions or staining artifacts, and (2) enhancing diagnostically relevant features, including glandular architecture and epithelial boundaries crucial for accurate Gleason grading. By selectively controlling the flow of information between the encoder and decoder, the network is guided to focus on histologically meaningful structures, improving segmentation reliability in heterogeneous prostate cancer tissues. Multi-scale decoder The segmentation masks are reconstructed with progressively increasing spatial resolutions in a multi-scale decoder with the use of ConvTranspose (2 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} Guided loss mechanism To improve segmentation stability and precision, a hierarchical guided loss is introduced to optimize several decoder stages for refinement at both high and low levels. Thus, this approach guarantees that the model learns from the intermediate outputs instead of the final segmentation mask in isolation. The BCE Loss increases the pixel-wise classification, and the Dice Loss provides increased smoothness and continuity of the segmented regions. Altogether, a multi-stage guided loss is also applied to enforce structural consistency and enforce smooth details along the boundary, and reduce false positives. This mechanism significantly improves the model’s ability to accurately delineate prostate cancer regions and is very robust across different Gleason Grade patterns due to the supervision of segmentation on different depths. The application of loss at multiple decoder stages introduces hierarchical supervision, which stabilizes gradient flow by providing consistent learning signals across the network layers. This mitigates the risk of vanishing gradients in deeper layers and ensures that intermediate features are optimized progressively. Moreover, supervising intermediate outputs allows the model to refine segmentation boundaries at multiple resolutions, leading to improved precision and clearer delineation of tissue structures. This is particularly beneficial in histopathological segmentation, where fine-grained boundary details are critical for Gleason grading accuracy. The multi-stage guided loss mechanism applies supervision at multiple decoder stages to improve learning stability and segmentation accuracy. Let \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_s$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P_s$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s$$\\end{document} 12 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathcal {L}_s = \\lambda _1 \\cdot \\text {BCE}(P_s, G) + \\lambda _2 \\cdot \\text {Dice}(P_s, G) \\end{aligned}$$\\end{document} 13 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathcal {L}_{\\text {total}} = \\sum _{s=1}^{S} \\alpha _s \\cdot \\mathcal {L}_s \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha _s$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S$$\\end{document} M Q Z \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda _1$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda _2$$\\end{document} 14 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} M_{ BCE }=\\frac{-1}{O}\\sum _{j=1}^O\\left[ z_j\\log \\left( q_j\\right) +\\left( 1-z_j\\right) \\log \\left( 1-q_j\\right) \\right] \\end{aligned}$$\\end{document} 15 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} M_{ Dice } = 1 - \\frac{2\\sum _{j=1}^O q_j z_j + \\epsilon }{\\sum _{j=1}^O q_j^2 + \\sum _{j=1}^O z_j^2 + \\epsilon } \\end{aligned}$$\\end{document} 16 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta _{u+1} = \\theta _u - \\eta \\frac{n_u}{\\sqrt{w_u}} \\end{aligned}$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\eta$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n_u$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$w_u$$\\end{document} 17 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Dice = \\frac{2\\sum _{j=1}^O q_j z_j + \\epsilon }{\\sum _{j=1}^O q_j^2 + \\sum _{j=1}^O z_j^2 + \\epsilon } \\end{aligned}$$\\end{document} 18 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} IoU = \\frac{\\sum _{j=1}^O q_j z_j + \\epsilon }{\\sum _{j=1}^O q_j + \\sum _{j=1}^O z_j - \\sum _{j=1}^O q_j z_j} \\end{aligned}$$\\end{document}  Algorithm 1 TAH U-Net Implementation setup details The implementation of the proposed Transformer Attention Hybrid U Net for prostate cancer segmentation and grading was done in Python using the PyTorch deep learning framework, and the development of the model was significantly accelerated using CUDA-enabled GPUs (NVIDIA A100, 40 GB VRAM). To validate the model, the model was trained on the SICAPv2 dataset: 10,000 WSIs from the range of 1024 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document} After training the model for 500 epochs, this was to ensure deep feature learning and performance stabilization among different Gleason patterns. A combined BCE, Dice loss, and Guided loss was used as a hybrid loss function, which effectively improved segmentation accuracy and also boundary refinement. For contextual feature learning, ViT layers in the Transformer-CNN hybrid encoder were trained from scratch; whereas, pretrained ResNet50 weights were used to initialize that encoder. Skip connections in the decoder were enhanced progressively with attention-guided connections to achieve effective feature fusion at diverse scales. Loss, Dice Score, Intersection-over-Union (IoU), Precision, and Recall were used in performance evaluation, giving a coverage of accuracy and reliability of segmentation. In order to speed up inference for optimization of deployment, the model was converted to ONNX models and then to TensorRT, reducing computational latency to become real-time capable of segmentation for clinical-grade diagnosis and grading of prostate cancer. Experimental results In this section, we present TAH U-Net results of 500 epochs trained on the SICAPv2 dataset, prostate cancer segmentation, and the corresponding grading results. Finally, we confirmed improved boundary delineation and reduced segmentation errors by showing low final loss, high Dice Score, IoU, Precision, and Recall of the model. The critical histopathological regions are highlighted by the XAI techniques like Grad-CAM and attention heatmaps to guarantee the transparency in predictions. Attention-guided skip connections and hybrid transformer encoding on traditional U-Net models not only boosted segmentation accuracy, but the architecture itself is more reliable and interpretable for prostate cancer analysis in practical clinical practice. Performance analysis The model was trained using the Adam optimizer with a learning rate of 0.0001 and a weight decay of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1 \\times 10^{-5}$$\\end{document} 4 Figure 4 Performance metrics progression over 500 epochs, showing trends in Loss, Dice Score, IoU Score, Precision, and Recall. Ablation study The ablation study demonstrates the significant impact of each architectural component in the proposed TAH U-Net for prostate cancer segmentation. The removal of Multi-Stage Guided Loss led to a decline in Dice Score (Val: 0.810 \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow$$\\end{document} \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow$$\\end{document} 2 Table 2 Ablation study demonstrating the impact of key components on the performance of the TAH U-Net (transposed view). Metric Base 40 Without multi-stage guided loss Without hybrid ViT-CNN encoding Without attention-guided skip connections Without transformer-based encoding (only CNN) Proposed model TAH U-Net Dice score (Val) 0.782 (0.771, 0.795) 0.810 (0.785, 0.825) 0.805 (0.775, 0.818) 0.798 (0.770, 0.810) 0.782 (0.750, 0.795) 0.852 (0.812, 0.865) IoU score (Val) 0.697 (0.685, 0.707) 0.710 (0.690, 0.725) 0.702 (0.680, 0.718) 0.693 (0.670, 0.708) 0.678 (0.650, 0.690) 0.745 (0.715, 0.752) Dice score (Test) 0.664 (0.651, 0.677) 0.789 (0.780, 0.800) 0.782 (0.770, 0.792) 0.774 (0.760, 0.785) 0.759 (0.740, 0.772) 0.824 (0.823, 0.832) IoU score (Test) 0.550 (0.538, 0.561) 0.701 (0.682, 0.710) 0.695 (0.675, 0.705) 0.688 (0.665, 0.698) 0.670 (0.645, 0.680) 0.732 (0.708, 0.742) Precision (%) – 72.80 70.60 69.20 67.80 76.44 Recall (%) – 91.30 90.80 89.50 87.90 95.66 F1-Score (%) – 89.90 88.50 87.10 85.50 94.33 Correlation analysis of segmentation metrics The relationship between Dice Score, IoU, Precision, Recall, and Loss gives important insights into the model’s performance in the segmentation tasks. In addition, the strong correlation between Dice Score and IoU Score indicates that as segmentation accuracy has been improved, intersection over union has increased, and the boundary alignment is better. Finally, Dice Score and Precision show high correlation with the minimum value contaminated with one outlier, which reveals the stability of the classification confidence. The apparent inverse relationship between Loss and Recall demonstrates that lower loss values will result in greater Recall, which means we are failing to detect less positive (not forgetting true positives), thus, for prostate cancer segmentation, we would like fewer false negatives. High spatial accuracy of the model to refine segmentation outputs presents high correlation with the IoU Score and the Precision as illustrated in the Fig. 5 Figure 5 Correlation analysis between segmentation performance metrics, highlighting relationships among Dice Score, IoU, Precision, Recall, and Loss. Correlation analysis shows such an inverse correlation between Dice Score and Loss, which means the lower the loss, the higher the segmentation accuracy, except for 4 minor outliers, which indicate very challenging cases in prostate cancer grading. The only two outliers are the only points where the Precision and Recall show a high correlation, and the performance is true to its word in minimizing false positive and false negative values. Moreover, it is shown that three outliers that are very highly correlated with IoU score and Recall prove that spatial accuracy of the model is not lost in segmentation as depicted in the Fig. 6 Figure 6 Scatter plot analysis of segmentation metrics, highlighting correlations among Loss, Dice Score, IoU, Precision, and Recall with detected outliers. Qualitative results of prostate cancer segmentation The first results image of the TAH U-Net shows that the method is able to clearly divide cancerous and non-cancerous parts in the images. Alongside their predicted masks and ground truth segmentation masks, ground truth segmentation masks are shown for the input histopathology images. Tumor boundaries are well captured by the model, which can align well with ground truth masks, although some minor misclassifications occur in some challenging tissue regions. Attention-guided skip connections and multi-stage guided loss mechanisms are used to improve the boundary precision, while the segmentation output generated demonstrates how the model generalizes well on different Gleason Grades. The qualitative results of prostate cancer segmentation are illustrated in Fig. 7 Figure 7 Qualitative results of prostate cancer segmentation showing input images, ground truth masks, and predicted masks generated by the proposed model. The second image shows the influence of Dice Score thresholds of 50 and 80 on the segmentation quality. As the Dice threshold increases, the segmentation mask is more refined with fewer false positives at the cost of critical structural details. For Dice 50, there are some misclassified areas, especially around the tumor, whereas for Dice 80, the model is significantly more accurate, as their fewer discrepancies between predicted and ground truth masks. These results confirm that the Dice-based optimization improves the segmentation reliability and assures high-confidence prediction of clinical utility in the analysis of prostate cancer. The segmentation results at Dice Score thresholds of 50 and 80, demonstrated in Fig. 8 Figure 8 Segmentation results at Dice Score thresholds of 50 and 80, demonstrating the effect of increasing the threshold on prediction accuracy and boundary refinement. Explainability analysis using XAI techniques The visual explanations generated using different XAI techniques demonstrate the interpretability of the TAH U-Net for prostate cancer segmentation. The Local Interpretable Model-agnostic Explanations (LIME) technique highlights important pixel regions contributing to the model’s segmentation decisions. The LIME-based explanations (top and bottom right) show green and yellow-highlighted areas, indicating regions with the most significant impact on the model’s predictions. These visual cues help in understanding how the model identifies critical histopathological structures, making it more transparent for clinical evaluation. The spatially localized feature attributions confirm that the model effectively focuses on cancerous regions, aligning well with expert pathology annotations. Furthermore, Occlusion Sensitivity Analysis (middle and bottom rows) offers a global interpretation of the importance of features based on progressively masking image regions and determining their effect on a model’s ability to perform. Red-highlighted areas are regions that are highly influential in segmentation decisions, and less saturated regions provide almost no contributions, as displayed in the Fig. 9 Figure 9 Explainability analysis using LIME and Occlusion Sensitivity techniques to visualize model attention and interpret segmentation decisions in prostate cancer detection. In the generated results, the PDP shed light on which parts of the histopathological image increase the model’s sensitivity to pixel intensity variations, thus providing an additional channel for feature importance in prostate cancer segmentation. Recent work has demonstrated the value of enhanced attention and interpretability in cancer segmentation models, such as the force map-based approach for cervical cancer proposed by Umirzakova et al. 40 10 PDP analyses of these prove that the Transformer Attention Hybrid U-Net is able to differentiate critical from non-informative regions and is hence robust for the task of prostate cancer segmentation and grading. To evaluate explanation quality, fidelity, and comprehensiveness, metrics were calculated based on LIME-derived regions. The model achieved a fidelity of 0.89 and a comprehensiveness score of 0.21, indicating that the highlighted tissue regions substantially contribute to prediction confidence and align with clinical relevance. These XAI techniques provide complementary insights into the model’s decision-making behavior. Grad-CAM and LIME visually highlight tissue regions most responsible for the model’s predictions, offering transparency and helping clinicians understand why specific segmentation decisions were made. Occlusion Sensitivity confirms that the model relies on histologically meaningful regions, as prediction confidence drops when diagnostically relevant areas are masked. PDP further demonstrates that the model responds predictably to pixel intensity changes at critical locations, mirroring pathologists’ reasoning. Together, these techniques validate that the model focuses on clinically relevant features and behaves consistently, making it suitable for real-world diagnostic use. Figure 10 PDP illustrating the model’s sensitivity to pixel Intensity variations at specific locations in prostate cancer histopathology images. Comparison with cutting edge technologies The comparative evaluation of the TAH U-Net against state-of-the-art segmentation architectures demonstrates its superior performance in Dice Score (mDice), Intersection over Union (mIoU), and segmentation accuracy. Traditional U-Net-based architectures, such as UNet and ResUNet, exhibit lower generalization capacity, with mDice scores ranging from 0.605 to 0.612 and mIoU values below 0.515, indicating their limited ability to capture complex histopathological structures. Transformer-based models, including UNETR (mDice: 0.684, mIoU: 0.585) and SwinUNETR (mDice: 0.705, mIoU: 0.606), demonstrate improved performance by leveraging global feature extraction mechanisms, but still lack boundary refinement capabilities. The integration of Segment Anything Model (SAM) and MedSAM into UNETR yields moderate improvements (mDice: 0.574–0.584, mIoU: 0.458–0.471), but these models struggle with fine-grained segmentation accuracy, particularly in noisy and weakly annotated datasets. The proposed TAH U-Net significantly outperforms all baseline models, achieving mDice of 0.852 (Val) and 0.824 (Test), with mIoU values of 0.745 (Val) and 0.732 (Test), highlighting its robust feature extraction, enhanced spatial attention, and superior generalization ability. The key factors contributing to this performance gain include the Hybrid ViT-CNN Encoder, which captures both local and global dependencies, the Attention-Guided Skip Connections, which improve boundary delineation, and the Multi-Stage Guided Loss Mechanism, which optimizes segmentation across multiple levels, reducing over-segmentation errors and enhancing clinical interpretability. The highlighted regions produced by LIME and Grad-CAM++ were examined for clinical interpretability. These visualizations frequently emphasize histological structures relevant to Gleason grading, such as glandular epithelium, lumen spaces, and stromal boundaries. In high-grade cancer cases, attribution maps often focus on fused or cribriform glands, while in low-grade regions, they concentrate on well-formed glandular architecture. This correspondence between model attention and diagnostic histological features suggests that the model’s predictions align with established pathology criteria, reinforcing its clinical relevance. These results confirm that the proposed model establishes a new benchmark in prostate cancer segmentation, significantly surpassing existing CNN-based, Transformer-based, and hybrid deep learning approaches in both precision and reliability (Table 3 Table 3 Comparison of the TAH U-Net with state-of-the-art segmentation models, demonstrating superior performance in mDice and mIoU. References Method Val mDice Val mIoU Test mDice Test mIoU 41 UNet 0.612 (0.594, 0.629) 0.511 (0.493, 0.529) 0.480 (0.464, 0.496) 0.363 (0.347, 0.380) 42 ResUNet 0.605 (0.586, 0.623) 0.493 (0.473, 0.511) 0.534 (0.511, 0.545) 0.424 (0.406, 0.434) 43 UNETR 0.684 (0.670, 0.697) 0.585 (0.571, 0.599) 0.531 (0.518, 0.545) 0.416 (0.402, 0.429) 44 UNETR + SAM 0.653 (0.642, 0.668) 0.555 (0.542, 0.568) 0.574 (0.569, 0.590) 0.458 (0.447, 0.475) 45 UNETR + MedSAM 0.679 (0.668, 0.690) 0.581 (0.571, 0.592) 0.584 (0.571, 0.598) 0.471 (0.456, 0.485) 46 SwinUNETR 0.705 (0.693, 0.718) 0.606 (0.593, 0.619) 0.563 (0.547, 0.576) 0.440 (0.429, 0.458) 47 DS-TransUNet 0.702 (0.691, 0.712) 0.605 (0.594, 0.616) 0.583 (0.570, 0.602) 0.471 (0.460, 0.490) 48 nnUNet v2 0.703 (0.694, 0.713) 0.609 (0.600, 0.618) 0.532 (0.523, 0.542) 0.439 (0.429, 0.448) Proposed model TAH U-Net 0.852 (0.812, 0.865) 0.745 (0.715, 0.752) 0.824 (0.823, 0.832) 0.732 (0.708, 0.742) Discussion and analysis The TAH U-Net introduces a novel approach to prostate cancer segmentation by fusing convolutional feature extraction with a transformer-based global attention mechanism. Our experimental results demonstrate that the model effectively captures both fine-grained local details and long-range contextual dependencies. This addresses a key challenge faced by conventional CNN-based U-Net architectures when dealing with complex histopathological variations. The integration of attention-guided skip connections further refines feature propagation by selectively emphasizing diagnostically significant regions while suppressing irrelevant tissue structures. Furthermore, the multi-stage guided loss mechanism ensures robust optimization at multiple decoder levels, leading to more precise segmentation and a reduction in false positives. The introduction of a hierarchical loss function facilitates the learning of more refined representations, which minimizes the boundary errors commonly seen in medical image segmentation. Our quantitative evaluation shows that the model achieves a high recall of 95.66%, indicating its strong ability to accurately identify cancerous regions while minimizing false negatives, a crucial requirement for high-risk clinical applications. The F1-score of 94.33% further demonstrates a healthy balance between precision and recall, suggesting robust overall performance. However, the model’s precision of 76.44% indicates a tendency to over-segment in some cases, prioritizing sensitivity over specificity. Our correlation analysis of segmentation metrics reveals strong correlations between the Dice score, IoU, precision, and recall, which validates the model’s consistent performance across different evaluation parameters. The inverse correlation between Loss and Recall suggests that as the model’s segmentation loss decreases, its ability to detect cancerous regions improves. The use of XAI techniques such as LIME, Occlusion Sensitivity, and PDP enhances the model’s interpretability. The results from these methods confirm that the model’s attention mechanisms highlight clinically relevant tissue regions, which increases clinical trust and transparency. Occlusion sensitivity maps further validate that the segmentation output is highly dependent on specific histopathological structures, making our approach more robust for potential real-world deployment in digital pathology workflows. Along with this, an in-depth correlation analysis on segmentation metrics proves strong correlation signals between Dice score, IoU, precision, and recall, thereby validating the model’s uniformity with the respective set of evaluation parameters. Loss and Recall show the inverse correlation, which suggests that as the model decreases the segmentation loss, its ability to detect the cancerous regions improves tremendously. In turn, the performance metrics are seen to periodically fluctuate, which suggests they are called to correct overfitting corrections due to adaptive learning rate adjustments that occur during training. Further addition to model interpretability is done using LIME, Occlusion Sensitivity and PDP. The results obtained using these techniques verify that the attention mechanisms of the model highlight the clinically relevant tissue regions, which leads to an increase in clinical trustworthiness, as well as model transparency and interpretability. The validation of occlusion sensitivity maps further proves that the segmentation output is not random and is highly dependent on particular histopathological structures, thereby making our method robust for real-world deployment into digital pathology workflows. While the IoU score (74.01%) is slightly lower than U-Net (83.25%) and FCN (82.52%), this discrepancy can be attributed to the model’s aggressive recall optimization, leading to minor over-segmentation. A possible enhancement could involve spatial consistency regularization techniques, ensuring that segmented regions adhere more closely to anatomical boundaries. Furthermore, despite the model’s strong segmentation capabilities, computational complexity remains a challenging aspect. The integration of ViT increases the overall model depth, requiring higher computational resources compared to traditional U-Net architectures. However, the use of ONNX-based model conversion and TensorRT acceleration significantly optimizes inference speed, enabling near real-time segmentation for whole-slide histopathology images. A comparative analysis with cutting-edge segmentation frameworks highlights the superiority of the proposed model in terms of recall-driven segmentation performance. Although YOLO achieves the highest precision (97.13%), its Dice Score (78.74%) and IoU (74.42%) are significantly lower, indicating inconsistent boundary delineation. Similarly, FCN (81.23% Dice) and deep learning baselines (80.36% Dice) struggle with precise segmentation, reinforcing the effectiveness of the hybrid CNN-transformer encoding employed in this paper. The combination of self-attention and convolutional residual learning allows the model to adaptively capture hierarchical features, leading to higher boundary accuracy in segmentation masks. The TAH U-Net successfully overcomes the key challenges in prostate cancer segmentation, offering a high-recall, explainable, and clinically robust solution. Future advancements should focus on adaptive learning strategies to balance sensitivity and specificity, lightweight transformer architectures for reduced computational cost, and multi-modal histopathological imaging integration to further enhance segmentation accuracy across diverse datasets. Limitations Despite its strong performance, our study has several limitations that warrant discussion. First, the model’s precision (76.44%) is lower than its recall, which points to a bias toward over-segmentation. While this trade-off can be acceptable in a clinical screening context where high sensitivity is paramount, it can also lead to an increase in false positives. Future work should explore adaptive thresholding or refined loss functions to better balance precision and recall. Second, the model was trained and evaluated exclusively on the SICAPv2 dataset. While this is a high-quality, publicly available dataset, the model’s performance on data from different institutions, scanners, or staining protocols remains untested. Its generalizability should be validated on more diverse, multi-center datasets before it can be considered for clinical use. Third, the hybrid CNN-ViT architecture, while powerful, is computationally more demanding than traditional U-Net models. The integration of Vision Transformer blocks increases model depth and requires more significant computational resources for training. Although we have optimized inference speed using ONNX and TensorRT, the computational cost may still be a barrier in low-resource settings. Future research could investigate lightweight transformer architectures to reduce this overhead. Finally, while our XAI analysis provides valuable insights into the model’s decision-making process, these explanations are not a substitute for rigorous clinical validation. The model’s outputs must be carefully reviewed by pathologists to ensure they are not only accurate but also clinically meaningful and reliable. Conclusion and future work This research introduces the TAH U-Net, a novel architecture for prostate cancer segmentation and grading. By leveraging a hybrid CNN-transformer encoder, attention-guided skip connections, and a multi-stage guided loss mechanism, our model significantly enhances feature extraction, segmentation accuracy, and interpretability. It effectively captures both local spatial features and long-range dependencies, ensuring the precise delineation of heterogeneous Gleason Grade regions. The use of XAI techniques such as LIME, Occlusion Sensitivity, and PDP confirms the model’s transparent decision-making, which is a crucial step toward clinical applicability. Our experimental evaluation demonstrates that the proposed model performs favorably when compared to conventional U-Net architectures, showing notable improvements in segmentation accuracy and boundary refinement. While the model shows great promise, we acknowledge its limitations, including a tendency toward over-segmentation and the need for broader validation. For future work, we plan to extend the model to multi-modal histopathological imaging, incorporating MRI and immunohistochemistry (IHC) data to improve segmentation robustness. We will also explore self-supervised and few-shot learning methods to reduce the model’s reliance on large, annotated datasets. Further enhancements in interpretability, through advanced frameworks like SHAP and Grad-CAM++, will deepen our understanding of the model’s decision-making process. Ultimately, our goal is to deploy the model in a real-world clinical environment, using feedback from pathologists and active learning strategies to continually refine its performance, thereby creating a more accurate and trustworthy tool for AI-assisted prostate cancer assessment. Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. These authors contributed equally: Muhammad Farhan and Guilin Min. Acknowledgements The authors extend their appreciation to Taif University, Saudi Arabia, for supporting this work through Project Number (TU-DSPP-2024-41). Author contributions These authors contributed equally to this work. Funding This research was funded by Taif University, Saudi Arabia, Project No. (TU-DSPP-2024-41). Data availability In this paper, the SICAPv2 dataset, a publicly available dataset of histopathological Whole Slide Images (WSI) with Gleason grades for prostate cancer grading, is used as our data set. The dataset is available here: https://data.mendeley.com/datasets/9xxm58dvs3/1 Declarations Competing interests The authors declare no competing interests. References 1. Salvi M A hybrid deep learning approach for gland segmentation in prostate histopathological images Artif. Intell. Med. 2021 115 102076 10.1016/j.artmed.2021.102076 34001325 Salvi, M. et al. A hybrid deep learning approach for gland segmentation in prostate histopathological images. Artif. Intell. Med. 115 34001325 10.1016/j.artmed.2021.102076 2. Abraham B Nair M Automated grading of prostate cancer using convolutional neural network and ordinal class classifier Inf. Med. Unlocked 2019 17 100256 10.1016/j.imu.2019.100256 Abraham, B. & Nair, M. Automated grading of prostate cancer using convolutional neural network and ordinal class classifier. Inf. Med. Unlocked 17 3. Duran-Lopez L Wide & deep neural network model for patch aggregation in CNN-based prostate cancer detection systems Comput. Biol. Med. 2021 136 104743 10.1016/j.compbiomed.2021.104743 34426172 Duran-Lopez, L. et al. Wide & deep neural network model for patch aggregation in CNN-based prostate cancer detection systems. Comput. Biol. Med. 136 34426172 10.1016/j.compbiomed.2021.104743 4. Morozov A A systematic review and meta-analysis of artificial intelligence diagnostic accuracy in prostate cancer histology identification and grading Prostate Cancer Prostatic Dis. 2023 26 681 692 10.1038/s41391-023-00673-3 37185992 Morozov, A. et al. A systematic review and meta-analysis of artificial intelligence diagnostic accuracy in prostate cancer histology identification and grading. Prostate Cancer Prostatic Dis. 26 37185992 10.1038/s41391-023-00673-3 5. Xiang J Automatic diagnosis and grading of prostate cancer with weakly supervised learning on whole slide images Comput. Biol. Med. 2023 152 106340 10.1016/j.compbiomed.2022.106340 36481762 Xiang, J. et al. Automatic diagnosis and grading of prostate cancer with weakly supervised learning on whole slide images. Comput. Biol. Med. 152 36481762 10.1016/j.compbiomed.2022.106340 6. Fernandez-Mateos J Tumor evolution metrics predict recurrence beyond 10 years in locally advanced prostate cancer Nat. Cancer 2024 5 1334 1351 10.1038/s43018-024-00787-0 38997466 PMC11424488 Fernandez-Mateos, J. et al. Tumor evolution metrics predict recurrence beyond 10 years in locally advanced prostate cancer. Nat. Cancer 5 38997466 10.1038/s43018-024-00787-0 PMC11424488 7. Collins K Cheng L Morphologic spectrum of treatment-related changes in prostate tissue and prostate cancer: an updated review Hum. Pathol. 2022 127 56 66 10.1016/j.humpath.2022.06.004 35716730 Collins, K. & Cheng, L. Morphologic spectrum of treatment-related changes in prostate tissue and prostate cancer: an updated review. Hum. Pathol. 127 35716730 10.1016/j.humpath.2022.06.004 8. Lu X Ultrasonographic pathological grading of prostate cancer using automatic region-based Gleason grading network Comput. Med. Imaging Graph. 2022 102 102125 10.1016/j.compmedimag.2022.102125 36257091 Lu, X. et al. Ultrasonographic pathological grading of prostate cancer using automatic region-based Gleason grading network. Comput. Med. Imaging Graph. 102 36257091 10.1016/j.compmedimag.2022.102125 9. Bakkouri, I. & Afdel, K. MLCA2F: Multi-level context attentional feature fusion for COVID-19 lesion segmentation from CT scans. Signal Image Video Process. 10.1007/s11760-022-02325-w PMC9346062 35935538 10. Bakkouri, I., Afdel, K., Benois-Pineau, J. & Initiative, G. C. F. T. A. D. N. BG-3DM2F: Bidirectional gated 3D multi-scale feature fusion for Alzheimer’s disease diagnosis. Multimed. Tools Appl. 81 11. Bakkouri I Afdel K Computer-aided diagnosis (CAD) system based on multi-layer feature fusion network for skin lesion recognition in dermoscopy images Multimed. Tools Appl. 2020 79 20483 20518 10.1007/s11042-019-07988-1 Bakkouri, I. & Afdel, K. Computer-aided diagnosis (CAD) system based on multi-layer feature fusion network for skin lesion recognition in dermoscopy images. Multimed. Tools Appl. 79 12. Bakkouri I Afdel K Multi-scale CNN based on region proposals for efficient breast abnormality recognition Multimed. Tools Appl. 2019 78 12939 12960 10.1007/s11042-018-6267-z Bakkouri, I. & Afdel, K. Multi-scale CNN based on region proposals for efficient breast abnormality recognition. Multimed. Tools Appl. 78 13. Kim, H., Kong, S., Lee, H., Kim, K. & Jung, K. Abstraction in pixel-wise noisy annotations can guide attention to improve prostate cancer grade assessment. In Workshop on Medical Image Learning with Limited and Noisy Data 14. Yu M FBCU-Net: A fine-grained context modeling network using boundary semantic features for medical image segmentation Comput. Biol. Med. 2022 150 106161 10.1016/j.compbiomed.2022.106161 36240598 Yu, M. et al. FBCU-Net: A fine-grained context modeling network using boundary semantic features for medical image segmentation. Comput. Biol. Med. 150 36240598 10.1016/j.compbiomed.2022.106161 15. Rajagopal, A. et al. Mixed supervision of histopathology improves prostate cancer classification from MRI. IEEE Trans. Med. Imaging 10.1109/TMI.2024.3382909 PMC11361281 38547000 16. Park S Deep learning model for real-time semantic segmentation during intraoperative robotic prostatectomy Eur. Urol. Open Sci. 2024 62 47 53 10.1016/j.euros.2024.02.005 38585210 PMC10998267 Park, S. et al. Deep learning model for real-time semantic segmentation during intraoperative robotic prostatectomy. Eur. Urol. Open Sci. 62 38585210 10.1016/j.euros.2024.02.005 PMC10998267 17. Kong F Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading Comput. Struct. Biotechnol. J. 2024 23 1439 1449 10.1016/j.csbj.2024.03.028 38623561 PMC11016961 Kong, F. et al. Federated attention consistent learning models for prostate cancer diagnosis and Gleason grading. Comput. Struct. Biotechnol. J. 23 38623561 10.1016/j.csbj.2024.03.028 PMC11016961 18. Zhang Z Segmentation assisted prostate cancer grading with multitask collaborative learning Pattern Recogn. Lett. 2024 183 42 48 10.1016/j.patrec.2024.04.023 Zhang, Z. et al. Segmentation assisted prostate cancer grading with multitask collaborative learning. Pattern Recogn. Lett. 183 19. Behzadi M Weakly-supervised deep learning model for prostate cancer diagnosis and Gleason grading of histopathology images Biomed. Signal Process. Control 2024 95 106351 10.1016/j.bspc.2024.106351 Behzadi, M. et al. Weakly-supervised deep learning model for prostate cancer diagnosis and Gleason grading of histopathology images. Biomed. Signal Process. Control 95 20. Liu F A hybrid classification model with radiomics and CNN for high and low grading of prostate cancer Gleason score on MP-MRI Displays 2024 83 102703 10.1016/j.displa.2024.102703 Liu, F. et al. A hybrid classification model with radiomics and CNN for high and low grading of prostate cancer Gleason score on MP-MRI. Displays 83 21. Shen Q MixUNETR: A U-shaped network based on W-MSA and depth-wise convolution with channel and spatial interactions for zonal prostate segmentation in MRI Neural Netw. 2025 181 106782 10.1016/j.neunet.2024.106782 39388995 Shen, Q. et al. MixUNETR: A U-shaped network based on W-MSA and depth-wise convolution with channel and spatial interactions for zonal prostate segmentation in MRI. Neural Netw. 181 39388995 10.1016/j.neunet.2024.106782 22. Jiang H Microsegnet: A deep learning approach for prostate segmentation on micro-ultrasound images Comput. Med. Imaging Graph. 2024 112 102326 10.1016/j.compmedimag.2024.102326 38211358 Jiang, H. et al. Microsegnet: A deep learning approach for prostate segmentation on micro-ultrasound images. Comput. Med. Imaging Graph. 112 38211358 10.1016/j.compmedimag.2024.102326 23. Zaridis D Resqu-net: Effective prostate’s peripheral zone segmentation leveraging the representational power of attention-based mechanisms Biomed. Signal Process. Control 2024 93 106187 10.1016/j.bspc.2024.106187 Zaridis, D. et al. Resqu-net: Effective prostate’s peripheral zone segmentation leveraging the representational power of attention-based mechanisms. Biomed. Signal Process. Control 93 24. Bhandary S Investigation and benchmarking of u-nets on prostate segmentation tasks Comput. Med. Imaging Graph. 2023 107 102241 10.1016/j.compmedimag.2023.102241 37201475 Bhandary, S. et al. Investigation and benchmarking of u-nets on prostate segmentation tasks. Comput. Med. Imaging Graph. 107 37201475 10.1016/j.compmedimag.2023.102241 25. Du X Shen A Wang X Feng Z Deng H NRD-Net: a noise-resistant distillation network for accurate diagnosis of prostate cancer with bi-parametric MRI images Multimed. Tools Appl. 2024 83 33597 33614 10.1007/s11042-023-16712-z Du, X., Shen, A., Wang, X., Feng, Z. & Deng, H. NRD-Net: a noise-resistant distillation network for accurate diagnosis of prostate cancer with bi-parametric MRI images. Multimed. Tools Appl. 83 26. Sowmya D Bhavani S Sasank V Rao T Prostate cancer classification using adaptive swarm intelligence based deep attention neural network Biomed. Signal Process. Control 2024 96 106654 10.1016/j.bspc.2024.106654 Sowmya, D., Bhavani, S., Sasank, V. & Rao, T. Prostate cancer classification using adaptive swarm intelligence based deep attention neural network. Biomed. Signal Process. Control 96 27. Bleker J A deep learning masked segmentation alternative to manual segmentation in biparametric MRI prostate cancer radiomics Eur. Radiol. 2022 32 6526 6535 10.1007/s00330-022-08712-8 35420303 PMC9381625 Bleker, J. et al. A deep learning masked segmentation alternative to manual segmentation in biparametric MRI prostate cancer radiomics. Eur. Radiol. 32 35420303 10.1007/s00330-022-08712-8 PMC9381625 28. Balaha H Shaban A El-Gendy E Saafan M Prostate cancer grading framework based on deep transfer learning and Aquila optimizer Neural Comput. Appl. 2024 36 7877 7902 10.1007/s00521-024-09499-z Balaha, H., Shaban, A., El-Gendy, E. & Saafan, M. Prostate cancer grading framework based on deep transfer learning and Aquila optimizer. Neural Comput. Appl. 36 29. Hussain T Shouno H Mohammed MA Marhoon HA Alam T DCSSGA-UNet: Biomedical image segmentation with DenseNet channel spatial and semantic guidance attention Knowl. Based Syst. 2025 314 113233 10.1016/j.knosys.2025.113233 Hussain, T., Shouno, H., Mohammed, M. A., Marhoon, H. A. & Alam, T. DCSSGA-UNet: Biomedical image segmentation with DenseNet channel spatial and semantic guidance attention. Knowl. Based Syst. 314 30. Hussain T Shouno H MAGRes-UNet: Improved medical image segmentation through a deep learning paradigm of multi-attention gated residual U-Net IEEE Access 2024 12 40290 40310 10.1109/ACCESS.2024.3374108 Hussain, T. & Shouno, H. MAGRes-UNet: Improved medical image segmentation through a deep learning paradigm of multi-attention gated residual U-Net. IEEE Access 12 31. Hussain T EFFResNet-ViT: A fusion-based convolutional and vision transformer model for explainable medical image classification IEEE Access 2025 13 54040 54068 10.1109/ACCESS.2025.3554184 Hussain, T. et al. EFFResNet-ViT: A fusion-based convolutional and vision transformer model for explainable medical image classification. IEEE Access 13 32. Alam T An integrated approach using YOLOv8 and ResNet, SeResNet & Vision Transformer (ViT) algorithms based on ROI fracture prediction in X-ray images of the elbow Curr. Med. Imaging 2024 20 e15734056309890 10.2174/0115734056309890240912054616 39360542 Alam, T. et al. An integrated approach using YOLOv8 and ResNet, SeResNet & Vision Transformer (ViT) algorithms based on ROI fracture prediction in X-ray images of the elbow. Curr. Med. Imaging 20 39360542 10.2174/0115734056309890240912054616 33. Ferrero A HistoEM: A pathologist-guided and explainable workflow using histogram embedding for gland classification Mod. Pathol. 2024 37 100447 10.1016/j.modpat.2024.100447 38369187 Ferrero, A. et al. HistoEM: A pathologist-guided and explainable workflow using histogram embedding for gland classification. Mod. Pathol. 37 38369187 10.1016/j.modpat.2024.100447 34. Xie W Prostate cancer risk stratification via nondestructive 3d pathology with deep learning–assisted gland analysis Can. Res. 2022 82 334 345 10.1158/0008-5472.CAN-21-2843 PMC8803395 34853071 Xie, W. et al. Prostate cancer risk stratification via nondestructive 3d pathology with deep learning–assisted gland analysis. Can. Res. 82 10.1158/0008-5472.CAN-21-2843 PMC8803395 34853071 35. Aju A Exploring vision transformers and XGBoost as deep learning ensembles for transforming carcinoma recognition Sci. Rep. 2024 14 1 35 39627293 10.1038/s41598-024-81456-1 PMC11614869 Aju, A. et al. Exploring vision transformers and XGBoost as deep learning ensembles for transforming carcinoma recognition. Sci. Rep. 14 39627293 10.1038/s41598-024-81456-1 PMC11614869 36. López-Pérez M The CrowdGleason dataset: Learning the Gleason grade from crowds and experts Comput. Methods Programs Biomed. 2024 257 108472 10.1016/j.cmpb.2024.108472 39488043 López-Pérez, M. et al. The CrowdGleason dataset: Learning the Gleason grade from crowds and experts. Comput. Methods Programs Biomed. 257 39488043 10.1016/j.cmpb.2024.108472 37. Butt M Kaleem M Bilal M Hanif M Using multi-label ensemble CNN classifiers to mitigate labelling inconsistencies in patch-level Gleason grading PLoS ONE 2024 19 e0304847 10.1371/journal.pone.0304847 38968206 PMC11226137 Butt, M., Kaleem, M., Bilal, M. & Hanif, M. Using multi-label ensemble CNN classifiers to mitigate labelling inconsistencies in patch-level Gleason grading. PLoS ONE 19 38968206 10.1371/journal.pone.0304847 PMC11226137 38. Zhang X A universal multiple instance learning framework for whole slide image analysis Comput. Biol. Med. 2024 178 108714 10.1016/j.compbiomed.2024.108714 38889627 Zhang, X. et al. A universal multiple instance learning framework for whole slide image analysis. Comput. Biol. Med. 178 38889627 10.1016/j.compbiomed.2024.108714 39. Falk T U-Net: Deep learning for cell counting, detection, and morphometry Nat. Methods 2019 16 67 70 10.1038/s41592-018-0261-2 30559429 Falk, T. et al. U-Net: Deep learning for cell counting, detection, and morphometry. Nat. Methods 16 30559429 10.1038/s41592-018-0261-2 40. Umirzakova S Muksimova S Baltayev J Cho YI Force map-enhanced segmentation of a lightweight model for the early detection of cervical cancer Diagnostics 2025 15 513 10.3390/diagnostics15050513 40075761 PMC11898811 Umirzakova, S., Muksimova, S., Baltayev, J. & Cho, Y. I. Force map-enhanced segmentation of a lightweight model for the early detection of cervical cancer. Diagnostics 15 40075761 10.3390/diagnostics15050513 PMC11898811 41. Kerfoot, E. et al. Left-ventricle quantification using residual u-net. In Statistical Atlases and Computational Models of the Heart. Atrial Segmentation and LV Quantification Challenges 42. Hatamizadeh, A. et al. UNETR: Transformers for 3D medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision 43. Kirillov, A. et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision 44. Ma J Segment anything in medical images Nat. Commun. 2024 15 654 10.1038/s41467-024-44824-z 38253604 PMC10803759 Ma, J. et al. Segment anything in medical images. Nat. Commun. 15 38253604 10.1038/s41467-024-44824-z PMC10803759 45. Hatamizadeh, A. et al. Swin UNETR: Swin transformers for semantic segmentation of brain tumors in MRI images. In International MICCAI Brainlesion Workshop 46. Lin A DS-TransUNet: Dual swin transformer U-Net for medical image segmentation IEEE Trans. Instrum. Meas. 2022 71 1 15 Lin, A. et al. DS-TransUNet: Dual swin transformer U-Net for medical image segmentation. IEEE Trans. Instrum. Meas. 71 47. Isensee F Jaeger P Kohl S Petersen J Maier-Hein K nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation Nat. Methods 2021 18 203 211 10.1038/s41592-020-01008-z 33288961 Isensee, F., Jaeger, P., Kohl, S., Petersen, J. & Maier-Hein, K. nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation. Nat. Methods 18 33288961 10.1038/s41592-020-01008-z 48. Zhang H Masked image modeling meets self-distillation: A transformer-based prostate gland segmentation framework for pathology slides Cancers 2024 16 3897 10.3390/cancers16233897 39682085 PMC11640496 Zhang, H. et al. Masked image modeling meets self-distillation: A transformer-based prostate gland segmentation framework for pathology slides. Cancers 16 39682085 10.3390/cancers16233897 PMC11640496 ",
  "metadata": {
    "Title of this paper": "Masked image modeling meets self-distillation: A transformer-based prostate gland segmentation framework for pathology slides",
    "Journal it was published in:": "Scientific Reports",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12485157/"
  }
}
{
  "title": "Paper_363",
  "abstract": "pmc Front Physiol Front Physiol 1464 frontphysiol Front. Physiol. Frontiers in Physiology 1664-042X Frontiers Media SA PMC12484120 PMC12484120.1 12484120 12484120 41041274 10.3389/fphys.2025.1659098 1659098 1 Physiology Original Research MAF-net: multi-receptive attention fusion network with dual-path squeeze-and-excitation enhancement module for uterine fibroid segmentation Jiang et al. 10.3389/fphys.2025.1659098 Jiang Yun  1 Zeng Qiquan  2 * Zhou Hongmei  3 * Ding Xiaokang  2  1 Department of Obstetrics and Gynecology, Quzhou Hospital of Traditional Chinese Medicine, Quzhou TCM Hospital at the Junction of Four Provinces Affiliated to Zhejiang Chinese Medical University Quzhou China  2 College of Mechanical Engineering, Quzhou University Quzhou China  3 Department of Color Ultrasonic, Quzhou Hospital of Traditional Chinese Medicine, Quzhou TCM Hospital at the Junction of Four Provinces Affiliated to Zhejiang Chinese Medical University Quzhou China  Edited by: Feng Gao  Reviewed by: Palash Ghosal  Xu Huang *Correspondence: Qiquan Zeng, ZengQQ@qzc.edu.cn 13587000354@163.com 17 9 2025 2025 16 480575 1659098 03 7 2025 26 8 2025 17 09 2025 02 10 2025 03 10 2025 Copyright © 2025 Jiang, Zeng, Zhou and Ding. 2025 Jiang, Zeng, Zhou and Ding https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Introduction Uterine fibroids are one of the most common benign tumors affecting the female reproductive system. In clinical practice, ultrasound imaging is widely used in the detection and monitoring of fibroids due to its accessibility and non-invasiveness. However, ultrasound images are often affected by inherent limitations, such as speckle noise, low contrast and image artifacts, which pose a substantial challenge to the precise segmentation of uterine fibroid lesions. To solve these problems, we propose a new multi-receptive attention fusion network with dual-path SE-enhancement module for uterine fibroid segmentation. Methods Specifically, our proposed network architecture is built upon a classic encoder-decoder framework. To enrich the contextual understanding within the encoder, we incorporate the multi-receptive attention fusion module (MAFM) at the third and fourth layers. In the decoding phase, we introduce the dual-scale attention enhancement module (DAEM), which operates on image representations at two different resolutions. Additionally, we enhance the traditional skip connection mechanism by embedding a dual-path squeeze-and-excitation enhancement module (DSEEM). Results and discussion To thoroughly assess the performance and generalization capability of MAF-Net, we conducted an extensive series of experiments on the clinical dataset of uterine fibroids from Quzhou Hospital of Traditional Chinese Medicine. Across all evaluation metrics, MAF-Net demonstrated superior performance compared to existing state-of-the-art segmentation techniques. Notably, it achieved Dice of 0.9126, Mcc of 0.9089, Jaccard of 0.8394, Accuracy of 0.9924 and Recall of 0.9016. Meanwhile, we also conducted experiments on the publicly available ISIC-2018 skin lesion segmentation dataset. Despite the domain difference, MAF-Net maintained strong performance, achieving Dice of 0.8624, Mcc of 0.8156, Jaccard of 0.7652, Accuracy of 0.9251 and Recall of 0.8304. Finally, we performed a comprehensive ablation study to quantify the individual contributions of each proposed module within the network. The results confirmed the effectiveness of the multi-receptive attention fusion module, the dual-path squeeze-and-excitation enhancement module, and the dual-scale attention enhancement module. image segmentation uterine fibroid multi-receptive attention fusion module dualpath squeeze-and-excitation enhancement module dual-scale attention enhancement module The author(s) declare that financial support was received for the research and/or publication of this article. This work was supported by the National Natural Science Foundation of China (No. 62102227), the Joint Fund of Zhejiang Provincial Natural Science Foundation of China (No. ZCLTGS24E0601), the Science and Technology Major Projects of Quzhou (No. 2022K128). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes section-at-acceptance Computational Physiology and Medicine 1 Introduction Uterine fibroids are a common type of benign tumor that occurs within the uterus of women. Their incidence rate among women of childbearing age is as high as 70%–80% ( Wang et al., 2024 In literature, various techniques have been explored to solve the problem of uterine fibroid segmentation in ultrasound images. Among them, Ni et al. (2015) Ni et al. (2016) Zhang et al. (2023) Liu et al. (2025a) Lekshmanan Chinna and Pathrose Mary (2024) Cai et al. (2024) Furthermore, the rapid advancement of deep learning techniques in recent years has opened new avenues for improving the precision and reliability of automatic uterine fibroid segmentation. For instance, architectures such as attention mechanism ( Zhou et al., 2025 Polattimur et al., 2025 Agarwal et al., 2024 Hu et al., 2025 Kumar et al., 2022 Zhu et al., 2025 Ali and Xie, (2025) Zhang et al. (2025a) Li et al. (2025) Huang and Xiao, (2025) Liu et al. (2025b) Sun et al. (2025) Deng et al. (2025) Xiao et al., 2025 Ying et al. (2025) Ahmed and Lasserre, (2025) Inspired by the challenges in ultrasound-based fibroid segmentation, we propose a novel multi-receptive attention fusion network (MAF-Net), which integrates the strengths of multi-receptive attention fusion module, dual-scale attention enhancement module, and dual-path squeeze-and-excitation enhancement module. The major contributions are summarized as follows. 1. The multi-receptive attention fusion module is embedded in the deeper layers of the encoder to improve contextual representation. By aggregating multi-scale receptive field information through attention mechanisms, this module enables the network to capture both global context and fine-grained semantic cues. 2. The dual-scale attention enhancement module is introduced within the decoder to enhance segmentation accuracy by processing image features at two distinct resolutions. Through dual-scale attention operations, this module effectively balances the integration of high-resolution structural details and low-resolution semantic context. 3. The dual-path squeeze-and-excitation enhancement module is incorporated into the skip connection to strengthen feature transmission between the encoder and decoder. Unlike conventional skip connections, DSEEM refines both channel-wise and spatial feature responses via parallel squeeze-and-excitation pathways, facilitating richer and more discriminative feature fusion across network stages. 2 Methods 2.1 Overview of MAF-net In this section, we provide a detailed description of the overall structure of MAF-Net. This network is based on the classic U-Net architecture and consists of encoder modules, decoder modules, and improved skip connections, as shown in Figure 1 FIGURE 1 Architecture of MAF-Net. Diagram of a convolutional neural network model architecture for image processing. It shows various modules labeled Double 3x3 Conv, MAFM, DSEEM, DAEM, and operations like 1x1 Conv + Sigmoid. Arrows indicate data flow between modules. 2.2 Multi-receptive attention fusion module To address the issues of limited receptive field and difficulty in capturing global context information in traditional convolutional methods, we designed the multi-receptive attention fusion module, whose structure is shown in Figure 2 Cheng et al., 2022 Figure 3 Cai et al., 2025 Figure 4 FIGURE 2 Structure of multi-receptive attention fusion module. Diagram of a multi-receptive attention fusion module. An input passes through a 1×1 convolution, then splits into three paths with 3×3 convolutions at rates 1, 3, and 5. Each path applies spatial attention, and one applies channel attention. These paths merge, followed by spatial attention, leading to the output. FIGURE 3 Structure of spatial attention mechanism. Flowchart of a spatial attention module. It starts with \"Input,\" splitting into \"Max-pooling\" and \"Ave-pooling,” which are combined. The result undergoes a \"7×7 Conv,\" then a \"Sigmoid\" transformation. Finally, it merges with the original input to produce the \"Output.\" FIGURE 4 Structure of channel attention mechanism. Flowchart of a channel attention module in a neural network. The process begins with an input, followed by a one-by-one convolution layer with channels divided by sixteen, then another convolution layer restoring channels to C. Two parallel paths with average pooling and max pooling converge after sigmoid activation, combining before passing to the output. 2.3 Dual-scale attention enhancement module To enhance the feature restoration capability and semantic expression effect in the decoding stage, we designed a dual-scale attention enhancement module, as shown in Figure 5 FIGURE 5 Structure of dual-scale attention enhancement module. Diagram of a dual-scale attention enhancement module featuring two parallel pathways. Each begins with a 3x3 convolution followed by channel attention. Outputs are merged, feeding into spatial attention and another 3x3 convolution, leading to the final output. Connections loop through various points to refine the data flow. 2.4 Dual-path squeeze-and-excitation enhancement module To overcome the limitations of the traditional U-Net skip connection structure in terms of semantic consistency and feature transmission, we propose a dual-path squeeze-and-excitation enhancement module, as shown in Figure 6 Xiong et al., 2024 Wang et al., 2025 Figure 7 FIGURE 6 Structure of dual-path squeeze-and-excitation enhancement module. Diagram of a dual-path squeeze-and-excitation enhancement module. It has two inputs: Input1 includes a downsampling path and Input2 includes an upsampling path. Both paths include SE blocks before merging. The paths combine with additional operations and a three-by-three convolution before producing the output. FIGURE 7 Structure of squeeze-and-excitation module. Squeeze-and-excitation block diagram showing the process from input to output. It includes average pooling for squeezing, followed by two fully connected layers with ReLU activation in the excitation stage, and concludes with a scaling operation before producing the output. 2.5 Loss function To evaluate the consistency between the predicted segmentation results and the true labels, we adopted Dice as the loss function ( Fu et al., 2024 Zhang et al., 2024 Equation 1 L d i c e y , p = 1 − 2 ∑ i = 1 N p i y i ∑ i = 1 N y i + ∑ i = 1 N p i (1)  N  p i  y i 3 Experiments and results 3.1 Dataset To comprehensively assess the segmentation performance and generalization capability of MAF-Net, we conducted experiments on two distinct datasets: a clinical ultrasound dataset of uterine fibroid collected from Quzhou Hospital of Traditional Chinese Medicine, and the publicly available ISIC-2018 skin lesion segmentation dataset ( Codella et al., 2019 Figure 8 Table 1 FIGURE 8 Representative examples of uterine fibroid dataset and ISIC-2018 dataset. The first and second rows are images with their corresponding annotations on the uterine fibroid dataset. The third and fourth are images with their corresponding annotations on the ISIC-2018 dataset. A grid of medical images displaying ultrasound scans and skin lesion photographs. The top row shows six different ultrasound images, each featuring black and white scan data. The middle row consists of various segmented areas on black backgrounds, highlighting regions of interest in white. The bottom row presents six close-up images of skin lesions in various colors and textures, each followed by a corresponding segmented black and white image below. TABLE 1 Detailed summary of uterine fibroid dataset and ISIC-2018 dataset. Dataset Number Training Validation Testing Uterine fibroid dataset 1,484 891 297 296 ISIC-2018 dataset 3,694 2,594 100 1,000 Uterine fibroid dataset: The dataset was sourced from Quzhou Hospital of Traditional Chinese Medicine and comprises a total of 1,484 high-resolution ultrasound images specifically capturing uterine fibroid cases. These images were acquired under real-world clinical diagnostic settings and reflect a broad spectrum of fibroid presentations in terms of size, shape, and anatomical location. To ensure the reliability and clinical relevance of the ground truth, all images were meticulously annotated by experienced medical professionals, with manual segmentation masks delineating the fibroid regions. For the purposes of training, validation, and performance evaluation, the dataset was systematically partitioned into three subsets. Specifically, 891 images were for training, 297 images were for validation, and the remaining 296 images were designated as the independent testing. ISIC-2018 dataset: In addition to the clinical ultrasound dataset, we also incorporated the ISIC-2018 skin lesion segmentation dataset to further validate the robustness and cross-domain generalization of our proposed method. This publicly available benchmark dataset contains a total of 3,694 dermoscopic images, each accompanied by high-quality ground truth masks that outline the lesion regions. To ensure a structured evaluation framework, the dataset was divided into three subsets: 2,594 were for training, 100 were for validation, and the remaining 1,000 were for testing. 3.2 Implementation details The training process of MAF-Net was implemented using the TensorFlow framework on a GeForce RTX 4090 GPU with 24 GB of memory. In our experiment, we employed the Adam optimizer ( Li et al., 2023 Figure 9 Figure 10 FIGURE 9 The loss and accuracy curves throughout the training and validation phases of our network. The first row is the results on the uterine fibroid dataset. The second is the results on the ISIC-2018 dataset. Four graphs displaying learning curves for a machine learning model over 200 epochs. The top left and bottom left graphs show loss and validation loss decreasing, indicating model improvement. The top right and bottom right graphs show accuracy and validation accuracy increasing, suggesting enhanced performance. Red marks indicate the best model points on the curves. FIGURE 10 Results of MAF-Net on the uterine fibroid dataset and ISIC-2018 dataset. The first to third rows are images, their corresponding annotations and our segmentation masks on the uterine fibroid dataset. The last three rows are images, their corresponding annotations and our segmentation masks on the ISIC-2018 dataset. Grid displaying ultrasound images, skin lesion photos, and corresponding binary masks. Rows show paired analysis for various medical evaluations. Ultrasounds and skin images exhibit different shapes and colorations, with masks highlighting areas of interest in white against a black background. 3.3 Evaluation indicators To comprehensively evaluate the segmentation performance of MAF-Net and ensure a fair comparison with several well-established algorithms, we employed five widely metrics: Dice ( Selvaraj and Nithiyaraj, 2023 Rainio et al., 2024 Zhu, 2020 Yang et al., 2024 Yuan et al., 2024 Yang et al., 2025 Hu et al., 2025 Zhang et al., 2025b Xia et al., 2024 Equation 2 Equation 3 Equation 4 Equation 5 Equation 6 D i c e = 2 T P 2 T P + F N + F P (2) M c c = T P × T N − F P × F N T P + F N T P + F P T N + F N T N + F P (3) J a c c a r d = T P T P + F N + F P (4) A c c u r a c y = T P + T N T P + T N + F N + F P (5) R e c a l l = T P T P + F N (6) 3.4 Ablation experiments To further validate the effectiveness of each proposed module within the MFA-Net architecture, we conducted comprehensive ablation experiments on the uterine fibroid dataset. As summarized in Table 2 Table 2 TABLE 2 Ablation experiments on the uterine fibroid dataset. Method Dice Mcc Jaccard Accuracy Recall Parameter (M) Time (ms/step) Baseline 0.8993 0.8957 0.8178 0.9912 0.8712 2.06 45 Baseline + MAFM 0.9076 0.9038 0.8311 0.9919 0.9014 2.41 49 Baseline + DAEM 0.9044 0.9003 0.8259 0.9916 0.8956 2.62 103 Baseline + DSEEM 0.9071 0.9031 0.8301 0.9918 0.9013 3.01 52 Baseline + MAFM + DAEM + DSEEM 0.9126 0.9089 0.8394 0.9924 0.9016 3.56 54 3.5 Comparative experiments 3.5.1 Experiments on the uterine fibroid dataset To comprehensively evaluate the capability of MAF-Net on the uterine fibroid dataset, we carried out a series of experiments involving several methods. The benchmarked approaches include SE-U-Net ( Jiang et al., 2021 Zheng et al., 2021 Jiang et al., 2023 Badrinarayanan et al., 2017 Jiang et al., 2024 Oktay et al., 2018 Table 3 TABLE 3 Comparative experiments on the uterine fibroid dataset. Method Dice Mcc Jaccard Accuracy Recall SE-U-Net ( Jiang et al., 2021 0.8170 0.8113 0.6914 0.9912 0.8808 CLNet ( Zheng et al., 2021 0.8350 0.8287 0.7172 0.9906 0.8644 RMAU-Net ( Jiang et al., 2023 0.8686 0.8633 0.7685 0.9905 0.8746 SegNet ( Badrinarayanan et al., 2017 0.8558 0.8509 0.7498 0.9888 0.8336 SSA-UNet ( Jiang et al., 2024 0.8486 0.8429 0.7379 0.9909 0.8695 AttUNet ( Oktay et al., 2018 0.8786 0.8736 0.7840 0.9917 0.8720 MAF-Net 0.9126 0.9089 0.8394 0.9924 0.9016  Figure 11 Figure 11 FIGURE 11 Qualitative comparison on the uterine fibroid dataset. The first to second rows are images and their corresponding annotations. The third to last are results of SE-U-Net, CLNet, RMAU-Net, SegNet, SSA-UNet, AttUNet and MAF-Net. Grid of ultrasound images displaying bladder segmentation results. Each row begins with an original ultrasound image, followed by various segmented versions, highlighting different stages or techniques of segmentation in black and white. 3.5.2 Experiments on the ISIC-2018 dataset  Table 4 Figure 12 Table 4 Figure 12 TABLE 4 Comparative experiments on the ISIC-2018 dataset. Method Dice Mcc Jaccard Accuracy Recall SE-U-Net ( Jiang et al., 2021 0.8333 0.7749 0.7227 0.9201 0.8287 CLNet ( Zheng et al., 2021 0.8354 0.7765 0.7226 0.9139 0.8239 RMAU-Net ( Jiang et al., 2023 0.8352 0.7764 0.7217 0.9200 0.8213 SegNet ( Badrinarayanan et al., 2017 0.8173 0.7502 0.6950 0.9107 0.8279 SSA-UNet ( Jiang et al., 2024 0.8400 0.7808 0.7303 0.9151 0.8293 AttUNet ( Oktay et al., 2018 0.8288 0.7653 0.7121 0.9125 0.8199 MAF-Net 0.8624 0.8156 0.7652 0.9251 0.8304 FIGURE 12 Qualitative comparison on the ISIC-2018 dataset. The first to second rows are images and their corresponding annotations. The third to last are results of SE-U-Net, CLNet, RMAU-Net, SegNet, SSA-UNet, AttUNet and MAF-Net. A grid of images displaying skin lesions on the left column alongside multiple black and white segmentation masks in the adjacent columns. Each row compares the original lesion with various segmentation outputs, showing different shapes and sizes of white areas on black backgrounds, indicating identified regions. 3.5.3 Experiments of dilation rate in the MAFM To investigate the impact of different dilation rate combinations within the multi-receptive attention fusion module, we conducted a comprehensive set of experiments on the uterine fibroid dataset, as summarized in Table 5 TABLE 5 Experiments of dilation rate in the MAFM on the uterine fibroid dataset. Dilation rate Dice Mcc Jaccard Accuracy Recall (1, 2, 3) 0.9106 0.9067 0.8361 0.9922 0.9066 (1, 2, 4) 0.9100 0.9062 0.8350 0.9922 0.9065 (1, 2, 5) 0.9086 0.9047 0.8327 0.9920 0.9080 (1, 2, 6) 0.9124 0.9087 0.8392 0.9924 0.9085 (1, 2, 7) 0.9088 0.9049 0.8331 0.9920 0.9122 (1, 2, 8) 0.9093 0.9055 0.8340 0.9921 0.8960 (1, 3, 4) 0.9080 0.9042 0.8318 0.9918 0.9253 (1, 3, 5) 0.9126 0.9089 0.8394 0.9924 0.9016 (1, 3, 6) 0.9082 0.9044 0.8321 0.9919 0.8995 (1, 3, 7) 0.9075 0.9038 0.8311 0.9920 0.8905 (1, 3, 8) 0.9020 0.8985 0.8228 0.9915 0.8779 (1, 4, 5) 0.9111 0.9074 0.8370 0.9923 0.9039 (1, 4, 6) 0.9087 0.9048 0.8329 0.9919 0.9108 (1, 4, 7) 0.9073 0.9037 0.8307 0.9920 0.8833 (1, 4, 8) 0.9061 0.9023 0.8285 0.9919 0.8921 (1, 5, 6) 0.9064 0.9028 0.8295 0.9919 0.8915 (1, 5, 7) 0.9116 0.9080 0.8378 0.9924 0.8966 (1, 5, 8) 0.9099 0.9061 0.8349 0.9921 0.9108 (1, 6, 7) 0.9085 0.9047 0.8326 0.9920 0.9036 (1, 6, 8) 0.9070 0.9031 0.8302 0.9918 0.8961 (1, 7, 8) 0.9033 0.8994 0.8242 0.9915 0.8965 3.5.4 Experiments of path selection in the DSEEM To validate the effectiveness of the dual-path design in the dual-path squeeze-and-excitation enhancement module, we conducted comparative experiments under three different configurations: (1) dual-path, (2) single-path using Input1 only, and (3) single-path using Input2 only. The quantitative results on the uterine fibroid dataset are summarized in Table 6 TABLE 6 Experiments of path selection in the DSEEM on the uterine fibroid dataset. Path selection Dice Mcc Jaccard Accuracy Recall dual-path 0.9126 0.9089 0.8394 0.9924 0.9016 Single-path (Input1) 0.7474 0.7380 0.5979 0.9793 0.7186 Single-path (Input2) 0.8888 0.8844 0.8005 0.9904 0.8726 3.5.5 Experiments of optimizer selection To explore the impact of different optimization strategies on MAF-Net, we conducted a comparative experiment using five commonly used optimizers on the uterine fibroid dataset, as shown in Table 7 TABLE 7 Experiments of optimizer selection on the uterine fibroid dataset. Optimizer Dice Mcc Jaccard Accuracy Recall Adam 0.9126 0.9089 0.8394 0.9924 0.9016 Adagrad 0.7934 0.7968 0.6642 0.9837 0.6790 Adamax 0.8939 0.8898 0.8092 0.9908 0.8800 RMSprop 0.9074 0.9035 0.8309 0.9919 0.8944 SGD 0.3105 0.2910 0.1848 0.9124 0.4733 3.5.6 Experiments of computational efficiency As presented in Table 8 TABLE 8 Experiments of computational efficiency on the uterine fibroid dataset. Method Parameter (M) Time (ms/step) SE-U-Net ( Jiang et al., 2021 1.87 42 CLNet ( Zheng et al., 2021 7.73 50 RMAU-Net ( Jiang et al., 2023 2.27 62 SegNet ( Badrinarayanan et al., 2017 2.80 48 SSA-UNet ( Jiang et al., 2024 2.33 48 AttUNet ( Oktay et al., 2018 8.49 61 MAF-Net 3.56 54 3.6 Limitations  Figure 13 FIGURE 13 Failure cases from the uterine fibroid dataset. The first and second rows are images with their corresponding annotations on the uterine fibroid dataset. The third row is the results of MAF-Net. Ultrasound images in the top row depict different anatomical views, while the middle and bottom rows show corresponding binary segmentation masks highlighting distinct regions in white against a black background. 4 Conclusion In this study, we proposed the MAF-Net deep learning framework, which was specifically designed for precise segmentation of uterine fibroids in ultrasound imaging. Specifically, by utilizing a unified encoder-decoder architecture, MAF-Net combined the multi-receptive attention fusion module, the dual-path squeeze-and-excitation enhancement module, and the dual-scale attention enhancement module, enabling it to effectively handle the inherent noise, boundary blurring, and scale variations in clinical ultrasound data. Extensive validation on the real-world uterine fibroid dataset demonstrated that MAF-Net consistently outperforms existing models in key performance metrics. The evaluation on the ISIC-2018 dataset further confirmed its strong generalization ability. Additionally, ablation studies emphasized the synergy of each architectural module, which collectively enhanced the accuracy and robustness. Overall, MAF-Net provided a reliable, accurate, and clinically applicable automatic segmentation solution for the ultrasound diagnostic workflow. Data availability statement The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. Author contributions YJ: Methodology, Validation, Writing – original draft. QZ: Supervision, Writing – original draft. HZ: Conceptualization, Investigation, Writing – review and editing. XD: Formal Analysis, Visualization, Writing – review and editing. Conflict of interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Generative AI statement The author(s) declare that no Generative AI was used in the creation of this manuscript. Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us. Publisher’s note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. References Agarwal R. Ghosal P. Sadhu A. K. Murmu N. Nandi D. 2024 Multi-scale dual-channel feature embedding decoder for biomedical image segmentation Comput. Methods Programs Biomed. 257 108464 10.1016/j.cmpb.2024.108464 39447437 Ahmed M. R. Lasserre P. 2025 FusionSegNet: a hierarchical multi-axis attention and gated feature fusion network for breast lesion segmentation with uncertainty modeling in ultrasound imaging Inf. Fusion 124 103399 10.1016/j.inffus.2025.103399 Ali H. Xie J. 2025 DFIT-Net: a novel dynamic feature integration transformer for automatic segmentation of multi-organ structures in medical imaging Displays 90 103087 10.1016/j.displa.2025.103087 Badrinarayanan V. Kendall A. Cipolla R. 2017 SegNet: a deep convolutional encoder-decoder architecture for image segmentation IEEE Trans. Pattern Anal. Mach. Intell. 39 2481 2495 10.1109/TPAMI.2016.2644615 28060704 Cai P. Yang T. Xie Q. Liu P. Li P. 2024 A lightweight hybrid model for the automatic recognition of uterine fibroid ultrasound images based on deep learning J. Clin. Ultrasound 52 753 762 10.1002/jcu.23703 38676550 Cai S. Jiang Y. Xiao Y. Zeng J. Zhou G. 2025 TransUMobileNet: integrating multi-channel attention fusion with hybrid CNN-transformer architecture for medical image segmentation Biomed. Signal Process. Control 107 107850 10.1016/j.bspc.2025.107850 Cheng Z. Qu A. He X. 2022 Contour-aware semantic segmentation network with spatial attention mechanism for medical image Vis. Comput. 38 749 762 10.1007/s00371-021-02075-9 33642659 PMC7898027 Codella N. Rotemberg V. Tschandl P. Celebi M. E. Dusza S. Gutman D. 2019 Skin lesion analysis toward melanoma detection 2018: a challenge hosted by the international skin imaging collaboration (ISIC) arXiv preprint arXiv:1902.03368 Deng L. Wang W. Chen S. Yang X. Huang S. Wang J. 2025 PDS-UKAN: subdivision hopping connected to the U-KAN network for medical image segmentation Comput. Med. Imaging Graph. 124 102568 10.1016/j.compmedimag.2025.102568 40435685 Fu L. Chen Y. Ji W. Yang F. 2024 SSTrans-Net: smart swin transformer network for medical image segmentation Biomed. Signal Process. Control 91 106071 10.1016/j.bspc.2024.106071 Hu S. Tao X. Zhao X. 2025 MCANet: feature pyramid network with multi-scale convolutional attention and aggregation mechanisms for semantic segmentation J. Vis. Commun. Image Represent. 110 104466 10.1016/j.jvcir.2025.104466 Hu M. Dong Y. Li J. Jiang L. Zhang P. Ping Y. 2025 LAMFFNet: lightweight adaptive multi-layer feature fusion network for medical image segmentation Biomed. Signal Process. Control 103 107456 10.1016/j.bspc.2024.107456 Huang W. Xiao H. 2025 AESC-TransUnet: attention enhanced selective channel transformer U-Net for medical image segmentation Signal Image Video Process 19 710 10.1007/s11760-025-04311-4 Jiang L. Y. Kuo C. J. Tang-Hsuan O. Hung M. H. Chen C. C. 2021 SE-U-Net: contextual segmentation by loosely coupled deep networks for medical imaging industry Asian conference on intelligent information and database systems New York, NY Springer 678 691 10.1007/978-3-030-73280-6_54 Jiang L. Ou J. Liu R. Zou Y. Xie T. Xiao H. 2023 RMAU-Net: residual multi-scale attention u-net for liver and tumor segmentation in CT images Comput. Biol. Med. 158 106838 10.1016/j.compbiomed.2023.106838 37030263 Jiang S. Chen X. Yi C. 2024 SSA-UNet: whole brain segmentation by U-Net with squeeze‐and‐excitation block and self‐attention block from the 2.5 D slice image IET Image Process 18 1598 1612 10.1049/ipr2.13052 Kumar A. Ghosal P. Kundu S. S. Mukherjee A. Nandi D. 2022 A lightweight asymmetric U-Net framework for acute ischemic stroke lesion segmentation in CT and CTP images Comput. Methods Programs Biomed. 226 107157 10.1016/j.cmpb.2022.107157 36208537 Lekshmanan Chinna M. Pathrose Mary J. P. 2024 Efficient feature extraction and hybrid deep learning for early identification of uterine fibroids in ultrasound images Int. J. Imaging Syst. Technol. 34 e23073 10.1002/ima.23073 Li Y. Zhang Y. Liu J. Y. Wang K. Zhang K. Zhang G. S. 2023 Global transformer and dual local attention network via IEEE Trans. Cybern. 53 5826 5839 10.1109/TCYB.2022.3194099 35984806 Li B. Li W. Wang B. Liu Z. Huang J. Wang J. 2025 SECNet: spatially enhanced channel-shuffled network with interactive contextual aggregation for medical image segmentation Expert Syst. Appl. 290 128409 10.1016/j.eswa.2025.128409 Liu Z. Sun C. Li C. Lv F. 2025a 3D segmentation of uterine fibroids based on deep supervision and an attention gate Front. Oncol. 15 1522399 10.3389/fonc.2025.1522399 40182051 PMC11966432 Liu S. Wang H. Lin Y. Jin X. Wang Y. Cheng Y. 2025b Context-aware network with enhanced local information for medical image segmentation Pattern Anal. Appl. 28 122 10.1007/s10044-025-01496-9 Ni B. He F. Yuan Z. 2015 Segmentation of uterine fibroid ultrasound images using a dynamic statistical shape model in HIFU therapy Comput. Med. Imaging Graph. 46 302 314 10.1016/j.compmedimag.2015.07.004 26459767 Ni B. He F. Z. Pan Y. T. Yuan Z. Y. 2016 Using shapes correlation for active contour segmentation of uterine fibroid ultrasound images in computer-aided therapy Appl. Math. J. Chin. Univ. 31 37 52 10.1007/s11766-016-3340-0 Oktay O. Schlemper J. Folgoc L. L. Lee M. Heinrich M. Misawa K. 2018 Attention U-Net: learning where to look for the pancreas arXiv Preprint arXiv 1804.03999 10.48550/arXiv.1804.03999 Polattimur R. Yıldırım M. S. Dandıl E. 2025 Fractal-based architectures with skip connections and attention mechanism for improved segmentation of MS lesions in cervical spinal cord Diagnostics 15 1041 10.3390/diagnostics15081041 40310404 PMC12025551 Rainio O. Teuho J. Klén R. 2024 Evaluation metrics and statistical tests for machine learning Sci. Rep. 14 6086 10.1038/s41598-024-56706-x 38480847 PMC10937649 Selvaraj A. Nithiyaraj E. 2023 CEDRNN: a convolutional encoder-decoder residual neural network for liver tumour segmentation Neural process. Lett. 55 1605 1624 10.1007/s11063-022-10953-z Sun J. Chen K. Wu X. Xu Z. Wang S. Zhang Y. 2025 MSM-UNet: a medical image segmentation method based on wavelet transform and multi-scale Mamba-UNet Expert Syst. Appl. 288 128241 10.1016/j.eswa.2025.128241 Wang T. Wen Y. Wang Z. 2024 nnU-Net based segmentation and 3D reconstruction of uterine fibroids with MRI images for HIFU surgery planning BMC Med. Imaging 24 233 10.1186/s12880-024-01385-3 39243001 PMC11380377 Wang Y. Bian Y. Jiang S. 2025 PSE: enhancing structural contextual awareness of networks in medical imaging with permute squeeze-and-excitation module Biomed. Signal Process. Control 100 107052 10.1016/j.bspc.2024.107052 Xia F. Peng Y. Wang J. Chen X. 2024 A 2.5 D multi-path fusion network framework with focusing on z-axis 3D joint for medical image segmentation Biomed. Signal Process. Control 91 106049 10.1016/j.bspc.2024.106049 Xiao L. Liu Y. Fan C. 2025 Attention-enhanced separable residual with dilation net for medical image segmentation Neurocomputing 641 130434 10.1016/j.neucom.2025.130434 Xiong L. Yi C. Xiong Q. Jiang S. 2024 SEA-NET: medical image segmentation network based on spiral squeeze-and-excitation and attention modules BMC Med. Imaging 24 17 17 10.1186/s12880-024-01194-8 38212684 PMC10785532 Yang M. Y. Shen Q. L. Xu D. T. Sun X. L. Wu Q. B. 2024 Striped WriNet: automatic wrinkle segmentation based on striped attention module Biomed. Signal Process. Control 90 105817 10.1016/j.bspc.2023.105817 Yang L. Dong Q. Lin D. Tian C. Lü X. 2025 MUNet: a novel framework for accurate brain tumor segmentation combining UNet and mamba networks Front. Comput. Neurosci. 19 1513059 10.3389/fncom.2025.1513059 39944950 PMC11814164 Ying Y. Fang X. Zhao Y. Zhao X. Zhou Y. Du G. 2025 SAM-MyoNet: a fine-grained perception myocardial ultrasound segmentation network based on segment anything model with prior knowledge driven Biomed. Signal Process. Control 110 108117 10.1016/j.bspc.2025.108117 Yuan H. Chen L. He X. 2024 MMUNet: morphological feature enhancement network for colon cancer segmentation in pathological images Biomed. Signal Process. Control 91 105927 10.1016/j.bspc.2023.105927 Zhang J. Liu Y. Chen L. Ma S. Zhong Y. He Z. 2023 DARU-Net: a dual attention residual u-net for uterine fibroids segmentation on MRI J. Appl. Clin. Med. Phys. 24 e13937 10.1002/acm2.13937 36992637 PMC10243339 Zhang D. Wang C. Chen T. Chen W. Shen Y. 2024 Scalable swin transformer network for brain tumor segmentation from incomplete MRI modalities Artif. Intell. Med. 149 102788 10.1016/j.artmed.2024.102788 38462288 Zhang R. Xie M. Liu Q. 2025a CFRA-Net: fusing coarse-to-fine refinement and reverse attention for lesion segmentation in medical images Biomed. Signal Process. Control 109 107997 10.1016/j.bspc.2025.107997 Zhang W. Qu S. Feng Y. 2025b LMFR-Net: lightweight multi-scale feature refinement network for retinal vessel segmentation Pattern Anal. Appl. 28 44 44 10.1007/s10044-025-01424-x Zheng Z. Wan Y. Zhang Y. Xiang S. Peng D. Zhang B. 2021 CLNet: cross-Layer convolutional neural network for change detection in optical remote sensing imagery ISPRS J. Photogramm. Remote Sens. 175 247 267 10.1016/j.isprsjprs.2021.03.005 Zhou S. Lei X. Sun L. 2025 Liver image segmentation using a rotated variable-sized window attention mechanism: application of the ARVSA U-Net model Biomed. Signal Process. Control 108 107954 10.1016/j.bspc.2025.107954 Zhu Q. 2020 On the performance of matthews correlation coefficient (mcc) for imbalanced dataset Pattern Recognit. Lett. 136 71 80 10.1016/j.patrec.2020.03.030 Zhu Y. Li H. Cao B. Huang K. Liu J. 2025 A novel hybrid layer-based encoder-decoder framework for 3D segmentation in congenital heart disease Sci. Rep. 15 11891 10.1038/s41598-025-96251-9 40195399 PMC11977193 ",
  "metadata": {
    "Title of this paper": "A novel hybrid layer-based encoder-decoder framework for 3D segmentation in congenital heart disease",
    "Journal it was published in:": "Frontiers in Physiology",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12484120/"
  }
}
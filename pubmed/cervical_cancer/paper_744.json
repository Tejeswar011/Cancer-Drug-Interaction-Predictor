{
  "title": "Paper_744",
  "abstract": "pmc Bioengineering (Basel) Bioengineering (Basel) 3241 bioeng bioengineering Bioengineering 2306-5354 Multidisciplinary Digital Publishing Institute  (MDPI) PMC12467291 PMC12467291.1 12467291 12467291 41007199 10.3390/bioengineering12090954 bioengineering-12-00954 1 Systematic Review Eye Tracking-Enhanced Deep Learning for Medical Image Analysis: A Systematic Review on Data Efficiency, Interpretability, and Multimodal Integration Duan Jiangxia 1 2 † https://orcid.org/0000-0001-6004-9138 Zhang Meiwei 3 † Song Minghui 1 2 https://orcid.org/0000-0003-3707-1104 Xu Xiaopan 1 2 * https://orcid.org/0000-0003-4181-7239 Lu Hongbing 1 2 * Boubchir Larbi Academic Editor 1 duanjx@fmmu.edu.cn 15653811835@163.com 2 3 zhangmw_play@163.com * alexander-001@163.com luhb@fmmu.edu.cn † These authors contributed equally to this work. 05 9 2025 9 2025 12 9 497612 954 29 7 2025 28 8 2025 02 9 2025 05 09 2025 27 09 2025 29 09 2025 © 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ Deep learning (DL) has revolutionized medical image analysis (MIA), enabling early anomaly detection, precise lesion segmentation, and automated disease classification. However, its clinical integration faces two major challenges: reliance on limited, narrowly annotated datasets that inadequately capture real-world patient diversity, and the inherent “black-box” nature of DL decision-making, which complicates physician scrutiny and accountability. Eye tracking (ET) technology offers a transformative solution by capturing radiologists’ gaze patterns to generate supervisory signals. These signals enhance DL models through two key mechanisms: providing weak supervision to improve feature recognition and diagnostic accuracy, particularly when labeled data are scarce, and enabling direct comparison between machine and human attention to bridge interpretability gaps and build clinician trust. This approach also extends effectively to multimodal learning models (MLMs) and vision–language models (VLMs), supporting the alignment of machine reasoning with clinical expertise by grounding visual observations in diagnostic context, refining attention mechanisms, and validating complex decision pathways. Conducted in accordance with the PRISMA statement and registered in PROSPERO (ID: CRD42024569630), this review synthesizes state-of-the-art strategies for ET-DL integration. We further propose a unified framework in which ET innovatively serves as a data efficiency optimizer, a model interpretability validator, and a multimodal alignment supervisor. This framework paves the way for clinician-centered AI systems that prioritize verifiable reasoning, seamless workflow integration, and intelligible performance, thereby addressing key implementation barriers and outlining a path for future clinical deployment. eye tracking deep learning medical image analysis data efficiency interpretability multimodal integration National Key Research and Development Program 2023YFF0715400 2023YFF0715401 Quick Response Plan of Air Force Military Medical University 2023KXKT096 National Natural Science Foundation of China 82372035 Sword Action Talent Program 2024RCYGXXP Joint Founding Project of Innovation Research Institute of Xijing Hospital LHJJ24YG06 This research was funded by The National Key Research and Development Program (2023YFF0715400, 2023YFF0715401); The Quick Response Plan of Air Force Military Medical University(2023KXKT096); The National Natural Science Foundation of China (No.82372035); Sword Action Talent Program (2024RCYGXXP); Joint Founding Project of Innovation Research Institute of Xijing Hospital (LHJJ24YG06). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Deep Learning (DL) has emerged as the cornerstone of modern artificial intelligence (AI) in medical image analysis (MIA), demonstrating exceptional performance in critical clinical tasks including precise anatomical segmentation, early lesion identification, and diagnostic classification—fundamentally enhancing diagnostic accuracy and workflow efficiency [ 1 2 3 A fundamental limitation of current DL approaches lies in their purely data-driven nature, which often diverges from human clinical reasoning. While DL excels at learning hierarchical patterns [ 4 5 6 7 8 9 10 11 12 9 13 14 15 16 17 18 19 20 While existing reviews on ET in MIA provide comprehensive coverage of hardware and model performance [ 17 21 • A systematic review with a generalized framework. We present the first comprehensive review of ET-enhanced MIA, introducing a generalized framework where ET data plays three critical roles, including data efficiency enhancers (reducing annotation dependency), interpretability validators (aligning AI attention with clinician reasoning), and multimodal alignment supervisors (bridging visual and textual domain information in image analysis). Figure 1 • Structured Analysis of ET Integration in MIA. Through rigorous evaluation of over 100 studies, we provide the first unified analysis of how ET data: (i) guide clinically relevant feature extraction; (ii) validate model interpretability via gaze-aligned saliency maps; (iii) optimize model performance with human attention priors; and (iv) enable multimodal fusion in MLMs/VLMs. • Translational Roadmap. Based on emerging trends and critical gaps in the field revealed by structured synthesis, we further identify key challenges in clinical deployment, propose feasible solutions for real-world clinical application, and suggest future directions for ET-enabled medical AI. Overall, this research introduces a novel framework integrating ET data to enhance data efficiency, interpretability, and multimodal alignment in MIA; provides the first unified analysis of ET data’s role in guiding feature extraction, validating interpretability, optimizing performance, and enabling multimodal fusion; and offers a comprehensive roadmap addressing clinical deployment challenges and proposing solutions for real-world application, highlighting future directions for ET-enabled medical AI. 2. Methods 2.1. Eligibility Criteria This systematic review strictly adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) [ 22 23 2.2. Search Strategy and Literature Selection This study evaluates how integrating ET data with DL enhances MIA. Using PRISMA guidelines, we established inclusion (IC) and exclusion criteria (EC) for paper selection, ensuring a transparent screening process that identifies relevant studies while filtering out unsuitable ones. Table 1 2.3. Data Extraction and Quantitative Synthesis To support our qualitative analysis, we summarized the specific performance metrics reported in the included studies, such as AUC, F1-score, precision, recall, and false-positive rates. A formal meta-analysis with pooled effect sizes was not possible because the studies varied too widely in their imaging tasks, model types, datasets, and evaluation methods. Instead, we selected one key metric from each study. We prioritized the main test set result or the most clinically meaningful outcome. We then grouped these results by their task, whether it was segmentation, detection, or classification. For studies that reported relative changes, like improvements over a baseline, we used those figures directly. This narrative synthesis approach is considered a best practice when high variability among studies prevents statistical pooling. 2.4. ET Data Quality and Consistency ET acquisition methods vary considerably across studies, which has significant implications for gaze precision, robustness to head motion, and downstream model performance. The studies reviewed generally employ three main types of eye-tracking devices: (i) remote eye trackers [ 24 25 26 3. Results We conducted a systematic literature search across PubMed, Web of Science, and arXiv (January 2020–April 2025) using the Boolean query ((“eye tracking” OR “gaze*”) AND (“medical*” OR “medical image*” OR “radiology” OR “CAD” OR “computer-aided diagnosis”) AND (“deep learning” OR “DL” OR “CNN” OR “DNN”)) with an English-language restriction. The search yielded 331 initial records (Web of Science: 223; PubMed: 92; arXiv: 16). Of 331 initially identified records, 41 studies met all inclusion criteria following deduplication and a two-stage screening process. This involved (i) abstract-level exclusion of non-ET, non-medical, or non-DL studies and (ii) full-text review for relevance (see PRISMA diagram, Figure 2 Through systematic analysis of the final paper corpus, we identified the following four key research dimensions, each substantiated by case studies and empirical evidence: (a) Feature Extraction Guidance- ET data address critical challenges in MIA (e.g., feature redundancy and salient pattern identification with limited data) by directing feature extraction to clinically relevant regions; (b) Interpretability Validation- comparative analysis of model attention maps with expert ET patterns establishes decision alignment, bridging AI outputs with human cognitive processes for transparent interpretability; (c) Performance Optimization- ET-enhanced DL models demonstrate improved accuracy and robustness, particularly in data-scarce scenarios; and (d) Multimodal Fusion—emerging applications integrate ET with MLMs and VLMs to refine medical image–text grounding and cross-modal attention mechanisms. Figure 3 3.1. ET Data and Patterns Used In medical image processing, ET devices (e.g., eye trackers) are employed to record physicians’ gaze positions, scan paths, and dwell times during image interpretation in real-time. These visual attention metrics are subsequently transformed into gaze heatmaps or fixation sequences, serving as “human attention labels” for DL models. Commonly utilized ET patterns include fixations, saccades, scan paths, and heatmaps. As summarized in Table 2 Table 2 3.2. Commonly Used ET for Feature Extraction 3.2.1. Effective Feature Extraction Is Fundamental to DL Models While effective feature extraction is crucial for enabling DL models to capture discriminative patterns [ 31 32 33 34 35 36 However, these methods share two critical shortcomings: (i) failure to incorporate clinical prior knowledge (e.g., anatomical constraints or radiologist expertise), and (ii) limited capacity for precise lesion localization due to insufficient medical-domain feature extraction. These limitations fundamentally constrain diagnostic accuracy in complex clinical scenarios. 3.2.2. ET Used to Guide the Feature Extraction of DL ET data quantify visual behavior through fixation points, durations, saccades, and scan paths, revealing expert attention patterns. Table 3 37 38 Prior/domain knowledge integration. ET data are typically converted into attention heatmaps and integrated with image data either as training inputs or supervisory signals. This integration enables DL models to more precisely localize task-relevant features and develop more discriminative representations. Several studies demonstrate this approach’s effectiveness, e.g., Wang et al. [ 39 Figure 4 40 41 Recent advances demonstrate ET’s clinical versatility: Franceschiello et al. [ 42 43 27 44 Attention mechanism guidance. ET data further optimize feature extraction in attention-based architectures (e.g., transformers or self-attention), explicitly demonstrating feature importance during decision-making [ 45 46 47 48 49 50 bioengineering-12-00954-t003_Table 3 Table 3 The typical application of ET used for feature extraction. Ref. ET Feature Extraction Method Imaging DL Observations and Highlights [ 40 Fixation distribution Weighted fixation maps served as an auxiliary imaging modality (concatenated with fundus images) and as supervised masks to guide feature extraction. Fundus images ResNet-18 Using weighted fixation maps as auxiliary masks yielded the best performance, with an accuracy of 73.50% and an F1-score of 77.63%, confirming that gaze-guided feature extraction benefits diabetic retinopathy recognition. [ 44 Heatmaps A saliency prediction model mimics radiologist-level visual attention, and the predicted gaze heatmap conditions positive pair generation via GCA, preserving critical information like abnormal areas in contrastive views. Knee X-ray ResNet-50 Predicted gaze heatmaps used in gaze-conditioned augmentation raised knee-OA classification accuracy from 55.31% to 58.81%, indicating that expert–attention-conditioned views outperform handcrafted augmentations. [ 49 Heatmaps and scan paths Expert fixation heatmaps and scan path vectors are encoded as sparse attention weights, concatenated with the original image features along the channel dimension, and then fed into the model for joint feature extraction. Colposcopy Gaze-DETR Encoding expert heatmaps and scan paths as sparse attention improved detection: average precision increased across thresholds, recall reached 0.988, and false positives declined. These approaches show that radiologists’ gaze patterns provide valuable guidance for DL models, significantly enhancing feature extraction effectiveness when integrated into diagnostic models. 3.3. ET Used to Validate the Interpretability of DL 3.3.1. Interpretability of DL in MIA Recent advances in DL have significantly enhanced medical imaging analysis, improving diagnostic accuracy and optimizing clinical workflows. Although DL models achieve exceptional performance through complex, multi-layered architectures with numerous parameters—often characterized as “black box” systems [ 51 52 53 Explainable artificial intelligence (XAI) has emerged as a critical attribute of AI technology, directly influencing the acceptance and adoption of AI tools in clinical practice [ 54 55 56 57 58 In radiology AI, interpretability serves three essential functions: (i) strengthening clinician confidence in high-stakes diagnostic decisions, (ii) improving diagnostic efficiency and accuracy, and (iii) identifying model limitations for iterative refinement [ 59 59 3.3.2. Interpretability Methods in DL Saliency maps (SMs) are a key tool for interpreting DL model decisions in medical imaging. They address the following two critical questions: (1) Which input features most influence model outputs? and (2) Where does the model focus when making decisions? By quantifying output sensitivity to input features, SMs visualize the most influential image regions for predictions, thereby revealing critical input–output relationships. Common methods of SM analysis include saliency [ 60 61 62 63 64 SMs visually highlight the regions and features most influential in model predictions. This enables users to identify the input features driving model decisions, evaluate whether the model utilizes clinically relevant features, and enhance transparency in the decision-making process. However, current interpretability methods still face some key limitations. For instance, techniques like Grad-CAM and SmoothGrad are primarily designed for CNN and often fail to precisely localize subtle diagnostic features [ 59 3.3.3. Application of ET in Validating Interpretability ET technology has emerged as a well-established methodology for investigating radiologists’ visual attention patterns, offering valuable insights into diagnostic reasoning processes and decision-making strategies [ 21 65 13 ET data provide valuable insights into radiologists’ diagnostic focus areas, enabling direct comparison between DL model decisions and expert reasoning. Strong alignment between model saliency maps (SMs) and ET heatmaps validates prediction interpretability, while discrepancies reveal optimization opportunities. Recent advances include Gaze Estimation Models (GEMs) [ 66 67 68 69 39 30 70 71 28 These studies collectively establish a framework for clinically aligned AI development, where ET data serve dual critical functions as supervisory guidance during model training and a validation benchmark for decision verification. Through precise alignment of computational attention patterns with radiologists’ visual search strategies, ET technology effectively bridges the conceptual divide between artificial decision-making and clinical reasoning processes. This integration significantly enhances both the perceived trustworthiness and measurable interpretability of CAD systems in medical practice. 3.4. ET Used to Improve the Performance of DL ET technology captures clinicians’ visual cognition, integrating this expertise into DL models to enhance performance and streamline clinical workflows through human–AI synergy. 3.4.1. ET Used as Prior Knowledge to Improve the Performance of DL DL models learn statistical patterns, while radiologists’ gaze tracks clinically meaningful features. ET quantifies these expert attention patterns through SMs, providing supervision to align deep neural networks (DNNs) with clinical reasoning to enhance performance. Recent studies confirm ET-DL integration significantly improves MIA. Table 4 Segmentation improvement. Stember et al. [ 72 73 74 27 75 Detection enhancement. The GCA [ 44 49 76 77 78 9 Classification optimization. Huang et al. [ 79 80 Text report replacement. Zhao et al. [ 81 82 70 bioengineering-12-00954-t004_Table 4 Table 4 Summary of ET-DL research works to improve performance. Task/Domain Reference Gaze Processing Year Datasets Type of Disease DL Model Highlights Segmentation Stember et al. [ 72 ET mask 2019 Images from PubMed and Google images meningioma U-net [ 13 ET-derived masks improved meningioma segmentation performance. Stember et al. [ 73 Gaze position/Fixation 2021 BraTS [ 83 brain tumor CNN models [ 84 Gaze supervision enabled accurate brain tumor labeling and boosted downstream accuracy. Xie et al. [ 27 Fixation heatmaps 2024 Inbreast [ 27 breast cancer U-net [ 13 Gaze-supervised segmentation increased Dice and mIoU under limited data. Detection Wang et al. [ 44 Fixation heatmaps 2023 Knee X-ray images [ 85 osteoarthritis U-net [ 13 Gaze-conditioned contrastive views increased knee X-ray detection accuracy. Kong et al. [ 49 Fixation heatmaps 2024 Private * vulvovaginal candidiasis Transformer [ 27 Gaze-guided DETR raised AP/AR and reduced false positives in VVC screening. Wang et al. [ 75 Fixation Points 2023 GrabCut dataset [ 86 87 abdomen disease SAM [ 88 Eye gaze with SAM improved interactive segmentation mIoU. Colonnese et al. [ 76 Fixation Points 2024 “Saliency4ASD” dataset [ 89 ASD RM3ASD [ 90 91 92 76 Gaze features improved ASD classification across accuracy, recall, and F1-score. Tian et al. [ 78 Fixation Points 2024 Private * glaucoma U-net [ 13 Expert gaze guided OCT localization with higher precision, recall, and F1-score. Classification Huang et al. [ 79 Fixation heatmaps 2021 BraTS [ 83 77 brain tumor and U-net [ 13 93 94 Gaze-aware attention strengthened segmentation/classification metrics with scarce data. Ma et al. [ 9 Fixation heatmaps 2022 Inbreast [ 70 71 chest disease ResNet [ 95 96 96 Gaze-predicted saliency curtailed shortcut learning and improved ACC, F1-score, and AUC. Zhu et al. [ 80 Fixation heatmaps 2022 The multi-modal CXR dataset [ 77 heart disease and ResNet [ 95 97 Gaze-guided attention raised precision and recall in chest X-ray classification. Text report replacement Zhao et al. [ 81 Fixation heatmaps 2024 Inbreast [ 70 70 breast and ResNet [ 95 Gaze-driven contrastive pretraining improved accuracy and AUC without text reports. * Authors collected their ET data. 3.4.2. Other Applications of ET in Enhancing MIA Medical image Annotation DNNs demand extensive labeled training data, whose manual annotation is costly and time-intensive [ 40 98 72 73 99 Data Augmentation ET data enhance DL by guiding clinically meaningful data augmentation. While traditional augmentation (e.g., rotation, scaling, cropping) improves model generalization, ET integration preserves diagnostically critical regions by aligning transformations with radiologists’ ET patterns [ 44 30 77 99 To orient readers across heterogeneous settings, Table 5 Table 5 3.5. ET in MLMs and VLMs Multimodal data fusion seeks to exploit the complementary, cooperative, and redundant features of different modalities to aid in the diagnostic process [ 100 101 102 3.5.1. Cross-Modal Alignment Recent studies using ET data during radiology report reading reveal how gaze patterns create explicit links between text passages and corresponding image regions. This helps MLMs learn better grounded representations and improves tasks like report generation or visual question answering [ 103 104 Early multimodal pre-training relied on millions of aligned image and text pairs, but in radiology and other domains such scale is unattainable; recent work shows that human ET data bridge this gap by providing pixel-level links between what experts read and where they look. The EGMA framework [ 28 105 20 Figure 5 Existing resources like the REFLACX [ 106 107 3.5.2. ET-Supervised Training and Representation Learning ET patterns act as a form of self-supervised yet highly informative guidance for representation learning [ 108 109 110 3.5.3. Interpretability and Validation of Multimodal Reasoning MLMs often act as more complex ‘black boxes’ versus single-modality systems. Researchers evaluate their interpretability by comparing model attention maps with human ET data [ 111 112 Human gaze comparison offers a robust validation method for MLM reasoning [ 113 19 Table 6 4. Discussion This review documents the transformative potential of ET-enhanced DL paradigms in MIA, demonstrating its dual capacity to address the following two fundamental limitations: (i) data inefficiency in model training and (ii) opacity in decision-making. By encoding radiologists’ visual attention patterns, ET delivers cognitively grounded supervisory signals that simultaneously improve learning efficiency in data-scarce scenarios, enhance model performance through clinically relevant feature prioritization, and establish interpretable decision pathways aligned with medical reasoning. Empirical evidence across segmentation, detection, classification, and report-generation tasks consistently demonstrates that ET-guided models improve diagnostic accuracy, mitigate overfitting, and align SMs with clinically pertinent image regions. Although many studies report that ET enhances learning signals and decision alignment, the evidence is not uniformly positive. First, performance gains are often task- and domain-specific. For instance, despite designing attention-based CAD systems using expert gaze, Wang et al. [ 39 77 ET reinforces trust in AI-assisted decisions while extending these benefits to MLMs and VLMs through improved cross-modal alignment and refined reasoning processes, thereby signaling a paradigm shift toward clinician-centered, explainable medical AI. However, despite the proven value of radiologists’ ET data in validating and optimizing decision-making, key challenges persist in fully integrating this technology into DL systems. Challenges and Future Trends: The integration of ET into DL pipelines shows considerable promise in MIA, yet it confronts three interrelated challenges. A primary obstacle is the difficulty of acquiring synchronized, high-precision ET data alongside medical images, a process that is both labor-intensive and costly. This has resulted in a scarcity of publicly available datasets, especially compared to those in natural image domains. Current resources are limited to repositories such as REFLACX [ 106 82 114 68 115 26 116 39 77 117 21 117 118 119 120 74 Moreover, reliable generalization and faithful reproducibility in studies that integrate eye tracking with deep learning depend on transparent reporting, deterministic data handling, and evaluation that extends beyond a single site or cohort. To enable meaningful comparison across studies, investigators should disclose the device used for gaze acquisition, the sampling rate, the calibration targets and repetitions, the method used for head stabilization, the mapping accuracy expressed in degrees of visual angle, the approach to drift correction, the proportion of missing samples, and the exact procedure by which gaze is registered to images. Reproducible practice further requires the release of code for preprocessing, quality control, and model training, together with fixed data partitions and random seeds, and with grouping at the patient level so that leakage is avoided. Claims about the contribution of eye tracking ought to be supported by paired experiments that hold the architecture and data constant while comparing training with and without gaze, and by stress tests that examine transfer across sites or across modalities. In addition to headline accuracy, researchers should make available interpretable artifacts such as representative gaze maps, model attention maps, and quantitative alignment summaries so that readers can evaluate the decision process and not only the final score. Whenever possible, work should be anchored on public resources such as REFLACX and MIMIC Eye so that results can be replicated exactly and compared fairly. We also encourage future studies to adopt explicit reporting and sharing practices to strengthen external validity. In contrast to earlier reviews, our synthesis integrates a concise quantitative summary with a taxonomy of data quality and reporting recommendations, thereby enabling like-for-like comparisons across heterogeneous studies. This approach underscores the value of ET not only as a means of enhancing performance but also as a facilitator of reproducibility and a bridge between algorithmic attention and clinical reasoning. 5. Conclusions ET characteristic analysis provides an objective and quantifiable tool for the field of medical imaging, capable of capturing the dynamic behaviors of radiologists in real-time during visual search, information extraction, and decision-making processes. By analyzing key parameters such as fixations, heatmaps, saccades, and scan paths, this method reveals the spatial distribution of visual attention and underlying cognitive mechanisms, thereby contributing to improved diagnostic accuracy and a deeper understanding of professional knowledge and models. With the iterative upgrading of lightweight, high-precision, non-invasive eye trackers and the maturation of data analysis algorithms, the role of eye-tracking technology in MIA is set to become increasingly prominent. ET technology provides a vital cognitive bridge between radiologists’ expertise and artificial intelligence, offering an inherently interpretable signal to guide next-generation multimodal systems. This integration enables AI that simultaneously achieves diagnostic-grade accuracy, transparent decision-making, data-efficient learning, and alignment with clinical reasoning patterns. To realize this potential, the following three critical pathways require focused development: (i) affordable tracking solutions (webcam/headset-based systems) for scalable deployment; (ii) large-scale, publicly available ET-image benchmark datasets are urgently needed to enable rigorous, reproducible evaluation of ET-guided models and privacy-preserving computational architectures for sensitive gaze data; and (iii) seamless workflow integration enabling real-time ET-guided clinical diagnostic assistance. By consolidating evidence across tasks and modalities and by specifying transparent reporting practices, this review provides a practical foundation for reproducible ET-enhanced medical AI. We anticipate that this framework will accelerate rigorous clinical translation where performance gains, interpretability, and external validity are advanced in tandem. Disclaimer/Publisher’s Note: Author Contributions Conceptualization, J.D. and M.Z.; methodology, J.D.; software, J.D. and M.Z.; validation, J.D. and M.S.; formal analysis, J.D. and M.Z.; investigation, J.D. and M.Z.; resources, J.D. and M.S.; data curation, J.D. and M.Z.; writing—original draft preparation, J.D. and M.Z.; writing—review and editing, H.L. and X.X.; visualization, J.D., M.Z., and X.X.; supervision, H.L. and X.X.; project administration, J.D.; funding acquisition, J.D. and H.L. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement This study’s data are publicly available. Conflicts of Interest The authors declare no conflicts of interest. Abbreviations The following abbreviations are used in this manuscript: AI Artificial intelligence DL Deep learning ET Eye tracking MIA Medical image analysis CNNs Convolutional neural networks DNNs Deep neural networks PRISMA Preferred Reporting Items for Systematic Reviews and Meta-Analyses MLMs Multimodal large models VLMs Vision–language models CAD Computer-aided diagnosis C–CAD Collaborative CAD SM Saliency map McGIP Medical Contrastive Gaze Image Pre-training EGMA Eye movement guided multimodal alignment SOTA State-of-the-art ViT Vision transformer EG-ViT Eye gaze guided vision transformer SAM Segment anything model GCA Gaze-conditioned augmentation AUROC Area Under the Receiver Operating Characteristic Curve References 1. Suganyadevi S. Seethalakshmi V. Balasamy K. A review on deep learning in medical image analysis Int. J. Multimed. Inf. Retr. 2022 11 19 38 10.1007/s13735-021-00218-1 34513553 PMC8417661 2. Khera R. Simon M.A. Ross J.S. Automation bias and assistive AI: Risk of harm from AI-driven clinical decision support JAMA 2023 330 2255 2257 10.1001/jama.2023.22557 38112824 3. Mir A.N. Rizvi D.R. Advancements in deep learning and explainable artificial intelligence for enhanced medical image analysis: A comprehensive survey and future directions Eng. Appl. Artif. Intell. 2025 158 111413 10.1016/j.engappai.2025.111413 4. Li M. Jiang Y. Zhang Y. Zhu H. Medical image analysis using deep learning algorithms Front. Public Health 2023 11 1273253 10.3389/fpubh.2023.1273253 38026291 PMC10662291 5. Mazurowski M.A. Dong H. Gu H. Yang J. Konz N. Zhang Y. Segment anything model for medical image analysis: An experimental study Med. Image Anal. 2023 89 102918 10.1016/j.media.2023.102918 37595404 PMC10528428 6. Sistaninejhad B. Rasi H. Nayeri P. A review paper about deep learning for medical image analysis Comput. Math. Method Med. 2023 2023 7091301 10.1155/2023/7091301 PMC10241570 37284172 7. Rana M. Bhushan M. Machine learning and deep learning approach for medical image analysis: Diagnosis to detection Multimed. Tools Appl. 2023 82 26731 26769 10.1007/s11042-022-14305-w PMC9788870 36588765 8. Chen X. Wang X. Zhang K. Fung K. Thai T.C. Moore K. Mannel R.S. Liu H. Zheng B. Qiu Y. Recent advances and clinical applications of deep learning in medical image analysis Med. Image Anal. 2022 79 102444 10.1016/j.media.2022.102444 35472844 PMC9156578 9. Ma C. Zhao L. Chen Y. Guo L. Zhang T. Hu X. Shen D. Jiang X. Liu T. Rectify ViT shortcut learning by visual saliency IEEE Trans. Neural Netw. Learn. Syst. 2024 35 18013 18025 10.1109/TNNLS.2023.3310531 37703160 10. Yu K. Chen J. Ding X. Zhang D. Exploring cognitive load through neuropsychological features: An analysis using fNIRS-eye tracking Med. Biol. Eng. Comput. 2025 63 45 57 10.1007/s11517-024-03178-w 39107650 11. Meng F. Li F. Wu S. Yang T. Xiao Z. Zhang Y. Liu Z. Lu J. Luo X. Machine learning-based early diagnosis of autism according to eye movements of real and artificial faces scanning Front. Neurosci. 2023 17 1170951 10.3389/fnins.2023.1170951 37795184 PMC10545898 12. Castner N. Arsiwala-Scheppach L. Mertens S. Krois J. Thaqi E. Kasneci E. Wahl S. Schwendicke F. Expert gaze as a usability indicator of medical AI decision support systems: A preliminary study NPJ Digit. Med. 2024 7 199 10.1038/s41746-024-01192-8 39068241 PMC11283514 13. Geirhos R. Jacobsen J. Michaelis C. Zemel R. Brendel W. Bethge M. Wichmann F.A. Shortcut learning in deep neural networks Nat. Mach. Intell. 2020 2 665 673 10.1038/s42256-020-00257-z 14. Mall S. Krupinski E. Mello-Thoms C. Missed cancer and visual search of mammograms: What feature based machine-learning can tell us that deep-convolution learning cannot Proceedings of the Medical Imaging 2019: Image Perception, Observer Performance, and Technology Assessment San Diego, CA, USA 20–21 February 2019 15. Mall S. Brennan P.C. Mello-Thoms C. Modeling visual search behavior of breast radiologists using a deep convolution neural network J. Med. Imaging 2018 5 1 10.1117/1.JMI.5.3.035502 30128329 PMC6086967 16. Shneiderman B. Human-Centered AI 1st ed. Oxford University Press Oxford, UK 2022 62 135 17. Neves J. Hsieh C. Nobre I.B. Sousa S.C. Ouyang C. Maciel A. Duchowski A. Jorge J. Moreira C. Shedding light on ai in radiology: A systematic review and taxonomy of eye gaze-driven interpretability in deep learning Eur. J. Radiol. 2024 172 111341 10.1016/j.ejrad.2024.111341 38340426 18. Moradizeyveh S. Tabassum M. Liu S. Newport R.A. Beheshti A. Di Ieva A. When eye-tracking meets machine learning: A systematic review on applications in medical image analysis arXiv 2024 10.48550/arXiv.2403.07834 2403.07834 19. Awasthi A. Ahmad S. Le B. Van Nguyen H. Decoding radiologists’ intentions: A novel system for accurate region identification in chest X-ray image analysis arXiv 2025 2404.18981 10.1109/isbi56570.2024.10635322 PMC12176413 40534634 20. Kim Y. Wu J. Abdulle Y. Gao Y. Wu H. Enhancing human-computer interaction in chest X-ray analysis using vision and language model with eye gaze patterns arXiv 2024 2404.02370 21. Ibragimov B. Mello-Thoms C. The use of machine learning in eye tracking studies in medical imaging: A review IEEE J. Biomed. Health Inform. 2024 28 3597 3612 10.1109/JBHI.2024.3371893 38421842 PMC11262011 22. Page M.J. Mckenzie J.E. Bossuyt P.M. Boutron I. Hoffmann T.C. Mulrow C.D. Shamseer L. Tetzlaff J.M. Akl E.A. Brennan S.E. The PRISMA 2020 statement: An updated guideline for reporting systematic reviews BMJ 2021 372 n71 10.1136/bmj.n71 33782057 PMC8005924 23. Booth A. Clarke M. Dooley G. Ghersi D. Moher D. Petticrew M. Stewart L. The nuts and bolts of PROSPERO: An international prospective register of systematic reviews Syst. Rev. 2012 1 2 10.1186/2046-4053-1-2 22587842 PMC3348673 24. Carter B.T. Luke S.G. Best practices in eye tracking research Int. J. Psychophysiol. 2020 155 49 62 10.1016/j.ijpsycho.2020.05.010 32504653 25. Dowiasch S. Wolf P. Bremmer F. Quantitative comparison of a mobile and a stationary video-based eye-tracker Behav. Res. Methods 2020 52 667 680 10.3758/s13428-019-01267-5 31240632 PMC7148267 26. Kaduk T. Goeke C. Finger H. König P. Webcam eye tracking close to laboratory standards: Comparing a new webcam-based system and the EyeLink 1000 Behav. Res. Methods 2024 56 5002 5022 10.3758/s13428-023-02237-8 37821751 PMC11289017 27. Xie J. Zhang Q. Cui Z. Ma C. Zhou Y. Wang W. Shen D. Integrating eye tracking with grouped fusion networks for semantic segmentation on mammogram images IEEE Trans. Med. Imaging 2025 44 868 879 10.1109/TMI.2024.3468404 39331544 28. Ma C. Jiang H. Chen W. Li Y. Wu Z. Yu X. Liu Z. Guo L. Zhu D. Zhang T. Eye-gaze guided multi-modal alignment for medical representation learning Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Vancouver, BC, Canada 9–15 December 2024 29. Pham T.T. Nguyen T. Ikebe Y. Awasthi A. Deng Z. Wu C.C. Nguyen H. Le N. GazeSearch: Radiology findings search benchmark Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2025) Tucson, AZ, USA 28 February–4 March 2025 30. Ma C. Zhao L. Chen Y. Wang S. Guo L. Zhang T. Shen D. Jiang X. Liu T. Eye-gaze-guided vision transformer for rectifying shortcut learning IEEE Trans. Med. Imaging 2023 42 3384 3394 10.1109/TMI.2023.3287572 37335796 31. Attallah O. Skin-CAD: Explainable deep learning classification of skin cancer from dermoscopic images by feature selection of dual high-level CNNs features and transfer learning Comput. Biol. Med. 2024 178 108798 10.1016/j.compbiomed.2024.108798 38925085 32. Theng D. Bhoyar K.K. Feature selection techniques for machine learning: A survey of more than two decades of research Knowl. Inf. Syst. 2024 66 1575 1637 10.1007/s10115-023-02010-5 33. Yan P. Sun W. Li X. Li M. Jiang Y. Luo H. PKDN: Prior knowledge distillation network for bronchoscopy diagnosis Comput. Biol. Med. 2023 166 107486 10.1016/j.compbiomed.2023.107486 37757599 34. Sarkar S. Wu T. Harwood M. Silva A.C. A transfer learning-based framework for classifying lymph node metastasis in prostate cancer patients Biomedicines 2024 12 2345 10.3390/biomedicines12102345 39457657 PMC11504638 35. Muksimova S. Umirzakova S. Iskhakova N. Khaitov A. Cho Y.I. Advanced convolutional neural network with attention mechanism for alzheimer’s disease classification using MRI Comput. Biol. Med. 2025 190 110095 10.1016/j.compbiomed.2025.110095 40158456 36. Allogmani A.S. Mohamed R.M. Al-Shibly N.M. Ragab M. Enhanced cervical precancerous lesions detection and classification using archimedes optimization algorithm with transfer learning Sci. Rep. 2024 14 12076 10.1038/s41598-024-62773-x 38802525 PMC11130149 37. Kok E.M. de Bruin A.B.H. Robben S.G.F. van Merriënboer J.J.G. Looking in the same manner but seeing it differently: Bottom-up and expertise effects in radiology Appl. Cogn. Psychol. 2012 26 854 862 10.1002/acp.2886 38. Kok E.M. Jarodzka H. Before your very eyes: The value and limitations of eye tracking in medical education Med. Educ. 2017 51 114 122 10.1111/medu.13066 27580633 39. Wang S. Ouyang X. Liu T. Wang Q. Shen D. Follow my eye: Using gaze to supervise computer-aided diagnosis IEEE Trans. Med. Imaging 2022 41 1688 1698 10.1109/TMI.2022.3146973 35085074 40. Jiang H. Hou Y. Miao H. Ye H. Gao M. Li X. Jin R. Liu J. Eye tracking based deep learning analysis for the early detection of diabetic retinopathy: A pilot study Biomed. Signal Process. Control 2023 84 104830 10.1016/j.bspc.2023.104830 41. Dmitriev K. Marino J. Baker K. Kaufman A.E. Visual analytics of a computer-aided diagnosis system for pancreatic lesions IEEE Trans. Vis. Comput. Graph. 2021 27 2174 2185 10.1109/TVCG.2019.2947037 31613771 42. Franceschiello B. Noto T.D. Bourgeois A. Murray M.M. Minier A. Pouget P. Richiardi J. Bartolomeo P. Anselmi F. Machine learning algorithms on eye tracking trajectories to classify patients with spatial neglect Comput. Methods. Programs. Biomed. 2022 221 106929 10.1016/j.cmpb.2022.106929 35675721 43. Moinak B. Shubham J. Prateek P. GazeRadar: A gaze and radiomics-guided disease localization framework Proceedings of the Medical Image Computing and Computer Assisted Intervention—MICCAI 2022 Singapore 16 September 2022 44. Wang S. Zhao Z. Zhang L. Shen D. Wang Q. Crafting good views of medical images for contrastive learning via expert-level visual attention Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) New Orleans, LA, USA 10–16 December 2023 45. Li X. Ding H. Yuan H. Zhang W. Pang J. Cheng G. Chen K. Liu Z. Loy C.C. Transformer-based visual segmentation: A survey IEEE Trans. Pattern Anal. Mach. Intell. 2024 46 10138 10163 10.1109/TPAMI.2024.3434373 39074008 46. Dosovitskiy A. Beyer L. Kolesnikov A. Weissenborn D. Zhai X. Unterthiner T. Dehghani M. Minderer M. Heigold G. Gelly S. An image is worth 16×16 words: Transformers for image recognition at scale Proceedings of the 9th International Conference on Learning Representations (ICLR 2021) Vienna, Austria (Converted to fully virtual due to COVID-19) 3–7 May 2021 47. He K. Gan C. Li Z. Rekik I. Yin Z. Ji W. Gao Y. Wang Q. Zhang J. Shen D. Transformers in medical image analysis Intell. Med. 2023 3 59 78 10.1016/j.imed.2022.07.002 48. Azad R. Kazerouni A. Heidari M. Aghdam E.K. Molaei A. Jia Y. Jose A. Roy R. Merhof D. Advances in medical image analysis with vision transformers: A comprehensive review Med. Image Anal. 2024 91 103000 10.1016/j.media.2023.103000 37883822 49. Kong Y. Wang S. Cai J. Zhao Z. Shen Z. Li Y. Fei M. Wang Q. Gaze-DETR: Using expert gaze to reduce false positives in vulvovaginal candidiasis screening arXiv 2024 10.48550/arXiv.2405.09463 2405.09463 50. Bhattacharya M. Jain S. Prasanna P. RadioTransformer: A cascaded global-focal transformer for visual attention–guided disease classification Proceedings of the 17th European Conference on Computer Vision(ECCV) Tel Aviv, Israel 23–27 October 2022 51. Castelvecchi D. Can we open the black box of AI? Nature 2016 538 20 23 10.1038/538020a 27708329 52. Lipton Z.C. The mythos of model interpretability Commun. ACM 2018 61 36 43 10.1145/3233231 53. Hulsen T. Explainable artificial intelligence (XAI): Concepts and challenges in healthcare AI 2023 4 652 666 10.3390/ai4030034 54. Hildt E. What is the role of explainability in medical artificial intelligence? A case-based approach Bioengineering 2025 12 375 10.3390/bioengineering12040375 40281735 PMC12025101 55. Barredo Arrieta A. Díaz-Rodríguez N. Del Ser J. Bennetot A. Tabik S. Barbado A. Garcia S. Gil-Lopez S. Molina D. Benjamins R. Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI Inf. Fusion 2020 58 82 115 10.1016/j.inffus.2019.12.012 56. Awasthi A. Le N. Deng Z. Agrawal R. Wu C.C. Van Nguyen H. Bridging human and machine intelligence: Reverse-engineering radiologist intentions for clinical trust and adoption Comp. Struct. Biotechnol. J. 2024 24 711 723 10.1016/j.csbj.2024.11.012 PMC11629193 39660015 57. Bhati D. Neha F. Amiruzzaman M. A survey on explainable artificial intelligence (XAI) techniques for visualizing deep learning models in medical imaging J. Imaging 2024 10 239 10.3390/jimaging10100239 39452402 PMC11508748 58. Tjoa E. Guan C. A survey on explainable artificial intelligence (XAI): Toward medical XAI IEEE Trans. Neural Netw. Learn. Syst. 2021 32 4793 4813 10.1109/TNNLS.2020.3027314 33079674 59. Sadeghi Z. Alizadehsani R. Cifci M.A. Kausar S. Rehman R. Mahanta P. Bora P.K. Almasri A. Alkhawaldeh R.S. Hussain S. A review of explainable artificial intelligence in healthcare Comput. Electr. Eng. 2024 118 109370 10.1016/j.compeleceng.2024.109370 60. Simonyan K. Vedaldi A. Zisserman A. Deep inside convolutional networks: Visualising image classification models and saliency maps Proceedings of the 2nd International Conference on Learning Representations (ICLR 2014) Banff, AB, Canada 14–16 April 2014 61. Selvaraju R.R. Cogswell M. Das A. Vedantam R. Parikh D. Batra D. Grad-CAM: Visual explanations from deep networks via gradient-based localization Proceedings of the IEEE International Conference on Computer Vision (ICCV) Venice, Italy 22–29 October 2017 62. Smilkov D. Thorat N. Kim B. Egas F.V. Wattenberg M. SmoothGrad: Removing noise by adding noise Proceedings of the 34th International Conference on Machine Learning (ICML) (ICML 2017) Sydney, NSW, Australia 6–11 August 2017 63. Sundararajan M. Taly A. Yan Q. Axiomatic attribution for deep networks Proceedings of the 34th International Conference on Machine Learning (ICML 2017) Sydney, NSW, Australia 6–11 August 2017 64. Kapishnikov A. Venugopalan S. Avci B. Wedin B. Terry M. Bolukbasi T. Guided integrated gradients: An adaptive path method for removing noise Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Nashville, TN, USA 20–25 June 2021 65. Brunyé T.T. Mercan E. Weaver D.L. Elmore J.G. Accuracy is in the eyes of the pathologist: The visual interpretive process and diagnostic accuracy with digital whole slide images J. Biomed. Inform. 2017 66 171 179 10.1016/j.jbi.2017.01.004 28087402 PMC5316368 66. Liu S. Chen W. Liu J. Luo X. Shen L. GEM: Context-aware gaze EstiMation with visual search behavior matching for chest radiograph arXiv 2024 2408.05502 67. Kim J. Zhou H. Lipton Z. Do you see what i see? A comparison of radiologist eye gaze to computer vision saliency maps for chest X-ray classification Proceedings of the International Conference on Machine Learning (ICML) Vienna, Austria 23–24 July 2021 68. Khosravan N. Celik H. Turkbey B. Jones E.C. Wood B. Bagci U. A collaborative computer aided diagnosis (c-CAD) system with eye-tracking, sparse attentional model, and deep learning Med. Image Anal. 2019 51 101 115 10.1016/j.media.2018.10.010 30399507 PMC6407631 69. Aresta G. Ferreira C. Pedrosa J. Araujo T. Rebelo J. Negrao E. Morgado M. Alves F. Cunha A. Ramos I. Automatic lung nodule detection combined with gaze information improves radiologists’ screening performance IEEE J. Biomed. Health Inform. 2020 24 2894 2901 10.1109/JBHI.2020.2976150 32092022 70. Moreira I.C. Amaral I. Domingues I. Cardoso A. Cardoso M.J. Cardoso J.S. INbreast: Toward a full-field digital mammographic database Acad. Radiol. 2012 19 236 248 10.1016/j.acra.2011.09.014 22078258 71. SIIM-ACR Pneumothorax Segmentation Available online: https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation (accessed on 15 May 2025) 72. Stember J.N. Celik H. Krupinski E. Chang P.D. Mutasa S. Wood B.J. Lignelli A. Moonis G. Schwartz L.H. Jambawalikar S. Eye tracking for deep learning segmentation using convolutional neural networks J. Digit. Imaging 2019 32 597 604 10.1007/s10278-019-00220-4 31044392 PMC6646645 73. Stember J.N. Celik H. Gutman D. Swinburne N. Young R. Eskreis-Winkler S. Holodny A. Jambawalikar S. Wood B.J. Chang P.D. Integrating eye tracking and speech recognition accurately annotates MR brain images for deep learning: Proof of principle Radiol. Artif. Intell. 2021 3 e200047 10.1148/ryai.2020200047 33842890 PMC7845782 74. Khosravan N. Celik H. Turkbey B. Cheng R. Mccreedy E. Mcauliffe M. Bednarova S. Jones E. Chen X. Choyke P. Gaze2segment: A pilot study for integrating eye-tracking technology into medical image segmentation Proceedings of the 19th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2016) Athens, Greece 17–21 October 2016 75. Wang B. Aboah A. Zhang Z. Pan H. Bagci U. GazeSAM: Interactive image segmentation with eye gaze and segment anything model Proceedings of the Gaze Meets Machine Learning Workshop New Orleans, LA, USA 30 November 2024 76. Colonnese F. Di Luzio F. Rosato A. Panella M. Enhancing autism detection through gaze analysis using eye tracking sensors and data attribution with distillation in deep neural networks Sensors 2024 24 7792 10.3390/s24237792 39686328 PMC11645092 77. Karargyris A. Kashyap S. Lourentzou I. Wu J.T. Sharma A. Tong M. Abedin S. Beymer D. Mukherjee V. Krupinski E.A. Creation and validation of a chest X-ray dataset with eye-tracking and report dictation for AI development Sci. Data 2021 8 92 10.1038/s41597-021-00863-5 33767191 PMC7994908 78. Tian Y. Sharma A. Mehta S. Kaushal S. Liebmann J.M. Cioffi G.A. Thakoor K.A. Automated identification of clinically relevant regions in glaucoma OCT reports using expert eye tracking data and deep learning Transl. Vis. Sci. Technol. 2024 13 24 10.1167/tvst.13.10.24 PMC11482640 39405074 79. Huang Y. Li X. Yang L. Gu L. Zhu Y. Hirofumi S. Meng Q. Harada T. Sato Y. Leveraging human selective attention for medical image analysis with limited training data Proceedings of the 32nd British Machine Vision Conference (BMVC 2021) Virtually/Online 22–25 November 2021 80. Zhu H. Salcudean S. Rohling R. Gaze-guided class activation mapping: Leveraging human attention for network attention in chest X-rays classification Proceedings of the 15th International Symposium on Visual Information Communication and Interaction (VINCI 2022) Chur, Switzerland (hybrid in-person + virtual) 16–18 August 2022 81. Zhao Z. Wang S. Wang Q. Shen D. Mining gaze for contrastive learning toward computer-assisted diagnosis Proceedings of the 38th AAAI Conference on Artificial Intelligence, AAAI 2024 Vancouver, BC, Canada 20–27 February 2024 82. Panetta K. Rajendran R. Ramesh A. Rao S. Agaian S. Tufts dental database: A multimodal panoramic X-ray dataset for benchmarking diagnostic systems IEEE J. Biomed. Health Inform. 2022 26 1650 1659 10.1109/JBHI.2021.3117575 34606466 83. Menze B.H. Jakab A. Bauer S. Kalpathy-Cramer J. Farahani K. Kirby J. Burren Y. Porz N. Slotboom J. Wiest R. The multimodal brain tumor image segmentation benchmark (BRATS) IEEE Trans. Med. Imaging 2015 34 1993 2024 10.1109/TMI.2014.2377694 25494501 PMC4833122 84. Gu J. Wang Z. Kuen J. Ma L. Shahroudy A. Shuai B. Liu T. Wang X. Wang G. Cai J. Recent advances in convolutional neural networks Pattern Recognit. 2018 77 354 377 10.1016/j.patcog.2017.10.013 85. Chen P. Gao L. Shi X. Allen K. Yang L. Fully automatic knee osteoarthritis severity grading using deep neural networks with a novel ordinal loss Comput. Med. Imaging. Graph. 2019 75 84 92 10.1016/j.compmedimag.2019.06.002 31238184 PMC9531250 86. Rother C. Kolmogorov V. Blake A. GrabCut: Interactive foreground extraction using iterated graph cuts ACM Trans. Graph. 2004 23 309 314 10.1145/1015706.1015720 87. Martin D. Fowlkes C. Tal D. Malik J. A Database of Human Segmented Natural Images and Its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics University of California at Berkeley Berkeley, CA, USA 2001 88. Kirillov A. Mintun E. Ravi N. Mao H. Rolland C. Gustafson L. Xiao T. Whitehead S. Berg A.C. Lo W. Segment anything Proceedings of the 20th IEEE/CVF International Conference on Computer Vision (ICCV 2023) Paris, France 2–6 October 2023 89. Gutiérrez J. Che Z. Zhai G. Le Callet P. Saliency4ASD: Challenge, dataset and tools for visual attention modeling for autism spectrum disorder Signal Process. Image Commun. 2021 92 116092 10.1016/j.image.2020.116092 90. Wei Q. Dong W. Yu D. Wang K. Yang T. Xiao Y. Long D. Xiong H. Chen J. Xu X. Early identification of autism spectrum disorder based on machine learning with eye-tracking data J. Affect. Disord. 2024 358 326 334 10.1016/j.jad.2024.04.049 38615846 91. Liaqat S. Wu C. Duggirala P.R. Cheung S.S. Chuah C. Ozonoff S. Young G. Predicting ASD diagnosis in children with synthetic and image-based eye gaze data Signal Process. Image Commun. 2021 94 116198 10.1016/j.image.2021.116198 33859457 PMC8043618 92. Chen S. Zhao Q. Attention-based autism spectrum disorder screening with privileged modality Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Seoul, Republic of Korea 27 October–2 November 2019 93. Isensee F. Jaeger P.F. Kohl S.A.A. Petersen J. Maier-Hein K.H. Nnu-net: A self-configuring method for deep learning-based biomedical image segmentation Nat. Methods 2021 18 203 211 10.1038/s41592-020-01008-z 33288961 94. Chen C. Liu X. Ding M. Zheng J. Li J. 3d dilated multi-fiber network for real-time brain tumor segmentation in MRI arXiv 2019 1904.03355 95. He K. Zhang X. Ren S. Sun J. Deep residual learning for image recognition Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2016) Las Vegas, NV, USA 26–30 June 2016 96. Liu Z. Lin Y. Cao Y. Hu H. Wei Y. Zhang Z. Lin S. Guo B. Swin transformer: Hierarchical vision transformer using shifted windows Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV 2021) Virtual 11–17 October 2021 97. Tan M. Le Q.V. EfficientNet: Rethinking model scaling for convolutional neural networks Proceedings of the 36th International Conference on Machine Learning (ICML 2019) Long Beach, CA, USA 9–15 June 2019 98. Leveque L. Bosmans H. Cockmartin L. Liu H. State of the art: Eye-tracking studies in medical imaging IEEE Access 2018 6 37023 37034 10.1109/ACCESS.2018.2851451 99. Zuo F. Jing P. Sun J. Duan J. Ji Y. Liu Y. Deep learning-based eye-tracking analysis for diagnosis of alzheimer’s disease using 3d comprehensive visual stimuli IEEE J. Biomed. Health Inform. 2024 28 2781 2793 10.1109/JBHI.2024.3365172 38349825 100. Kumar S. Rani S. Sharma S. Min H. Multimodality fusion aspects of medical diagnosis: A comprehensive review Bioengineering 2024 11 1233 10.3390/bioengineering11121233 39768051 PMC11672922 101. Baltrusaitis T. Ahuja C. Morency L. Multimodal machine learning: A survey and taxonomy IEEE Trans. Pattern Anal. Mach. Intell. 2019 41 423 443 10.1109/TPAMI.2018.2798607 29994351 102. Warner E. Lee J. Hsu W. Syeda-Mahmood T. Kahn C.E. Gevaert O. Rao A. Multimodal machine learning in image-based and clinical biomedicine: Survey and prospects Int. J. Comput. Vis. 2024 132 3753 3769 10.1007/s11263-024-02032-8 39211895 PMC11349845 103. Kabir R. Haque N. Islam M.S. Marium-E-Jannat A comprehensive survey on visual question answering datasets and algorithms arXiv 2024 10.48550/arXiv.2411.11150 2411.11150 104. Masse B. Ba S. Horaud R. Tracking gaze and visual focus of attention of people involved in social interaction IEEE Trans. Pattern Anal. Mach. Intell. 2018 40 2711 2724 10.1109/TPAMI.2017.2782819 29990014 105. Peng P. Fan W. Shen Y. Liu W. Yang X. Zhang Q. Wei X. Zhou D. Eye gaze guided cross-modal alignment network for radiology report generation IEEE J. Biomed. Health Inform. 2024 28 7406 7419 10.1109/JBHI.2024.3422168 38995704 106. Lanfredi R.B. Zhang M. Auffermann W.F. Chan J. Duong P.T. Srikumar V. Drew T. Schroeder J.D. Tasdizen T. REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest X-rays Sci. Data 2022 9 350 10.1038/s41597-022-01441-z 35717401 PMC9206650 107. MIMIC-Eye: Integrating MIMIC Datasets with REFLACX and Eye Gaze for Multimodal Deep Learning Applications Available online: https://physionet.org/content/mimic-eye-multimodal-datasets/1.0.0/ (accessed on 23 April 2025) 108. Tutek M. Šnajder J. Toward practical usage of the attention mechanism as a tool for interpretability IEEE Access 2022 10 47011 47030 10.1109/access.2022.3169772 109. Wang S. Zhao Z. Shen Z. Wang B. Wang Q. Shen D. Improving self-supervised medical image pre-training by early alignment with human eye gaze information IEEE Trans. Med. Imaging 2025 in press 10.1109/TMI.2025.3528965 40031039 110. Wang S. Zhao Z. Zhuang Z. Ouyang X. Zhang L. Li Z. Ma C. Liu T. Shen D. Wang Q. Learning better contrastive view from radiologist’s gaze Pattern Recognit. 2025 162 111350 10.1016/j.patcog.2025.111350 111. Zeger E. Pilanci M. Black boxes and looking glasses: Multilevel symmetries, reflection planes, and convex optimization in deep networks arXiv 2024 10.48550/arXiv.2410.04279 2410.04279 112. Zhang M. Cui Q. Lü Y. Yu W. Li W. A multimodal learning machine framework for alzheimer’s disease diagnosis based on neuropsychological and neuroimaging data Comput. Ind. Eng. 2024 197 110625 10.1016/j.cie.2024.110625 113. Kumar P. Khandelwal E. Tapaswi M. Sreekumar V. Seeing eye to AI: Comparing human gaze and model attention in video memorability arXiv 2025 2311.16484 114. Khosravi S. Khan A.R. Zoha A. Ghannam R. Self-directed learning using eye-tracking: A comparison between wearable head-worn and webcam-based technologies Proceedings of the IEEE Global Engineering Education Conference (EDUCON) Tunis, Tunisia 28–31 March 2022 115. Kora P. Ooi C.P. Faust O. Raghavendra U. Gudigar A. Chan W.Y. Meenakshi K. Swaraja K. Plawiak P. Rajendra Acharya U. Transfer learning techniques for medical image analysis: A review Biocybern. Biomed. Eng. 2022 42 79 107 10.1016/j.bbe.2021.11.004 116. Saxena S. Fink L.K. Lange E.B. Deep learning models for webcam eye tracking in online experiments Behav. Res. Methods 2024 56 3487 3503 10.3758/s13428-023-02190-6 37608235 PMC11133145 117. Valtakari N.V. Hooge I.T.C. Viktorsson C. Nyström P. Falck-Ytter T. Hessels R.S. Eye tracking in human interaction: Possibilities and limitations Behav. Res. Methods 2021 53 1592 1608 10.3758/s13428-020-01517-x 33409984 PMC7787418 118. Sharafi Z. Sharif B. Guéhéneuc Y. Begel A. Bednarik R. Crosby M. A practical guide on conducting eye tracking studies in software engineering Empir. Softw. Eng. 2020 25 3128 3174 10.1007/s10664-020-09829-4 119. Guideline for Reporting Standards of Eye-Tracking Research in Decision Sciences Available online: https://osf.io/preprints/psyarxiv/f6qcy_v1 (accessed on 25 March 2025) 120. Seyedi S. Jiang Z. Levey A. Clifford G.D. An investigation of privacy preservation in deep learning-based eye-tracking Biomed. Eng. Online 2022 21 67 10.1186/s12938-022-01035-1 36100851 PMC9469631 Figure 1 The overall framework of this review. Figure 2 Flow diagram of the review process using modified PRISMA. Figure 3 Overview of the different aspects covered in this review. Figure 4 Unlike the conventional method (left panel), the right panel employs ET data to supervise the network’s attention mechanism, enhancing classification accuracy and abnormality localization performance. Adapted from Wang et al. (2022) [ 39 Figure 5 Overview of enhancing human–computer interaction in chest X-ray analysis using VLM with eye gaze patterns. Adapted from Kim et al. (2024) [ 20 bioengineering-12-00954-t001_Table 1 Table 1 The outline of inclusion and exclusion criteria defined for this review. List of Inclusion and Exclusion Criteria Inclusion Criteria (IC) Exclusion Criteria (EC) IC1 Should contain at least one of the keywords. EC1 Manuscripts containing duplicated passages lack originality or fail to contribute meaningful insights. IC2 Must be sourced from reputable academic databases, such as PubMed and Web of Science. EC2 The full-text publication could not be retrieved or accessed through available channels. IC3 Published after 2019 (inclusive). EC3 Study was rejected or contains a warning. IC4 Publications included peer-reviewed journal papers, conference or workshop papers, non-peer-reviewed papers, and preprints. EC4 Non-English documents or translations that exhibit structural disorganization, ambiguous phrasing, or critical information gaps. IC5 Selected studies must demonstrate a clear alignment with DL and ET’s focus on medical image analysis, with relevant titles, abstracts, and content. EC5 Papers unrelated to the application or development of DL or ET or medical image analysis. bioengineering-12-00954-t002_Table 2 Table 2 The typical application of the ET patterns. Application Type ET Patterns Mechanism Typical Case As weakly supervised labels Fixations “Image + ET” dual-channel input uses ET data as a weakly supervised label, replaces manual box selection with the observation of fixations, and trains the model to perform lesion segmentation. A novel radiologist gaze-guided weakly supervised segmentation framework [ 27 Multimodal alignment Heatmaps Align the heatmaps with the words and sentences in the radiological reports to enhance the consistency of cross-modal retrieval and diagnosis of images and text. The ET-guided multimodal alignment (EGMA) framework [ 28 Visual search modeling Saccades and Train the model using the scan paths to predict the complete search trajectory of doctors when locating lesions. The ChestSearch model [ 29 Interpretability validation Heatmaps Compare the model’s activation map with the expert’s gaze heatmaps to verify whether the model is focusing on clinically critical areas. By incorporating the radiologists’ ET heatmaps, we can determine whether the attention mechanism of the model is reasonable [ 30 bioengineering-12-00954-t005_Table 5 Table 5 Quantitative presentation of typical tasks integrated into ET-DL. Task Study Modality and Disease Dataset Reported Metric(s) with ET Highlights Detection Gaze-DETR [ 49 Colposcopy (vulvovaginal candidiasis) Colposcopy images Average precision increased at different thresholds, and average recall increased to 0.988. ET is encoded as sparse attention weights concatenated with image features. Classification RadioTransformer [ 50 Chest X-ray (pneumonia) MIMIC-CXR F1-score ↑ and AUC ↑ By integrating visual attention into the network, the model focuses on diagnostically relevant regions of interest, leading to higher confidence in decision-making. Annotation (auxiliary to segmentation) ET + speech for annotation [ 72 Brain MRI (brain tumor lesion marking) BraTS Accuracy = 92% (training), 85% (independent test) Supports scalable, high-quality supervision for DL. ↑ Indicates an improvement in performance metrics. bioengineering-12-00954-t006_Table 6 Table 6 Application of ET in a multimodal model of medical images: strategy and performance evaluation. Task/Domain Multimodal Model Methods ET Strategy Year Performance Highlights Radiology image EGMA [ 28 Utilize radiologists’ fixation points to precisely align visual and textual elements within a dual-encoder framework. 2024 SOTA on multiple medical datasets (improved classification AUC and retrieval recall). Gaze-guided alignment improved AUROC and retrieval through stronger image–text grounding. Chest X-ray EGGCA-Net [ 105 Integrate radiologists’ eye gaze regions (prior knowledge) to guide image–text feature alignment for report generation. 2024 Outperformed previous models on MIMIC-CXR. ET-guided alignment produced more accurate, comprehensible radiology reports. Chest X-ray analysis VLMs incorporating ET data [ 20 Leverage ET heatmaps overlaid on chest X-rays to highlight radiologists’ key focus areas during evaluation. 2024 Different evaluation metrics for different tasks; all the baseline models performed better with ET. Adding gaze improved chest X-ray diagnostic accuracy across tasks. Self-supervised medical image pre-training GzPT [ 109 Integrate ET with existing contrastive learning methods to focus on images with similar gaze patterns. 2025 SOTA on three medical datasets. Gaze-similarity positives delivered SOTA pretraining and more interpretable features. Knee X-ray classification FocusContrast [ 110 Use gaze to supervise the training for visual attention prediction. 2025 Consistently improved SOTA contrastive learning methods in classification accuracy. Gaze-predicted attention consistently lifted knee X-ray classification. Video memorability CNN + Transformer (CLIP-based 113 Predict memorability scores using an attention-based model aligned with human gaze fixations (collected via ET). 2025 Matched SOTA memorability prediction. Model attention aligned with human gaze on memorable content, matching SOTA performance. Chest radiograph abnormality diagnosis TGID [ 19 Predict radiology report intentions with temporal grounding, using fixation heatmap videos and embedded time steps as inputs. 2025 Superior to SOTA methods. Temporal grounding from gaze improved intention detection beyond prior methods. ",
  "metadata": {
    "Title of this paper": "An investigation of privacy preservation in deep learning-based eye-tracking",
    "Journal it was published in:": "Bioengineering",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12467291/"
  }
}
{
  "title": "Paper_810",
  "abstract": "pmc Front Artif Intell Front Artif Intell 3981 frontai Front. Artif. Intell. Frontiers in Artificial Intelligence 2624-8212 Frontiers Media SA PMC12463984 PMC12463984.1 12463984 12463984 41018739 10.3389/frai.2025.1627876 1 Artificial Intelligence Original Research An artificial intelligence model for early-stage breast cancer classification from histopathological biopsy images Chaudhary Neil  1  * Dhunny A. Z.  2 1 Pangea Society New Delhi India 2 Cyber Analytics Port Louis Mauritius Edited by: Farah Kidwai-Khan Reviewed by: Marco Cavaco Abdelmoniem Helmy *Correspondence: Neil Chaudhary, neil.c1107@gmail.com 12 9 2025 2025 8 481073 1627876 13 5 2025 13 8 2025 12 09 2025 27 09 2025 29 09 2025 Copyright © 2025 Chaudhary and Dhunny. 2025 Chaudhary and Dhunny https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Accurate identification of breast cancer subtypes is essential for guiding treatment decisions and improving patient outcomes. In current clinical practice, determining histological subtypes often requires additional invasive procedures, delaying treatment initiation. This study proposes a deep learning-based model built on a DenseNet121 backbone with a multi-scale feature fusion strategy, designed to classify breast cancer from histopathological biopsy images. Trained and evaluated on the publicly available BreaKHis dataset using 5-fold cross-validation, the model achieved a binary classification accuracy of 97.1%, and subtype classification accuracies of 93.8% for benign tumors and 92.0% for malignant tumors. These results demonstrate the model’s ability to capture morphological cues at multiple levels of abstraction and highlight its potential as a diagnostic support tool in digital pathology workflows. breast cancer classification histopathological images deep learning DenseNet121 multi-scale feature fusion convolutional neural networks (CNNs) subtype detection medical image analysis The author(s) declare that no financial support was received for the research and/or publication of this article. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes section-at-acceptance Medicine and Public Health 1 Introduction Breast cancer remains one of the most prevalent cancers globally, representing a significant health challenge for women across all age groups. According to the World Health Organization (WHO), over 2.3 million women were diagnosed with breast cancer in 2020, making it the most diagnosed cancer worldwide and the leading cause of cancer-related deaths among women ( WHO, 2021 World Health Organization (WHO), 2022 GLOBOCAN, 2020 Early detection has been identified as a critical factor in improving survival rates, with studies showing that early-stage breast cancer has a 90% 5-year survival rate compared to late-stage diagnoses, which can drop to 27% ( Siegel et al., 2022 Kolb et al., 2002 Kuhl et al., 2005 Esteva et al., 2021 McKinney et al., 2020 Despite significant progress in imaging modalities and AI-driven diagnostic tools, most existing models focus primarily on binary classification—differentiating benign from malignant tumors—without addressing the more nuanced and clinically challenging task of histological subtype classification. This limitation hinders their real-world applicability, particularly in cases where treatment decisions depend on precise subtype identification. These gaps underscore the need for a more robust, subtype-aware AI framework trained on whole-slide biopsy images to improve diagnostic granularity and clinical utility. 1.1 Brief review of artificial intelligence in breast cancer This section provides a brief literature review of what technology has been used for the detection of breast cancer over the past few years for breast cancer detection. And this is going to justify our innovative model and method in the further sections. AI has revolutionized the healthcare landscape, offering transformative capabilities in automating and enhancing diagnostic processes. In the context of breast cancer detection, AI models have demonstrated superior performance in analyzing complex MRI datasets, identifying patterns that may elude human experts ( Lehman et al., 2019 Litjens et al., 2017 McKinney et al., 2020 Rodrigues et al., 2021 Kaymak et al. (2017) Fu et al. (2022) Punitha et al. (2021) Sharma et al. (2024) Liu et al. (2024a) Li et al. (2024b) Munshi et al. (2024) Atrey et al. (2024) Robinson and Preethi (2024) Zhang et al. (2024) In their 2021 study, Meha Desai and Manan Shah compared the effectiveness of MLP and CNN for breast cancer detection. The study used datasets such as BreakHis, WBCD, and WDBC, with images and biomarkers preprocessed through normalization and feature extraction (e.g., Discrete Cosine Transform). While both MLP and CNN were trained to classify breast abnormalities, CNN consistently outperformed MLP in terms of accuracy. CNN achieved ~99.86% accuracy on the BreakHis dataset, while MLP’s performance was generally lower. CNN was found to be more reliable for image-based diagnosis, whereas MLP showed limitations for larger datasets. Further to this, Ezzat et al. (2023) Joseph et al. (2022) Paul et al. (2023) Taheri and Omranpour (2023) Muduli et al. (2021) Dr. Suvidha Tripathi’s research contributes significantly to the field of breast cancer histopathology image classification, particularly through innovative approaches that integrate spatial context and hybrid feature representations. In one study, Tripathi et al. proposed a BiLSTM-based patch modeling framework, which treats histopathology image patches as sequential data to capture spatial continuity—an aspect often overlooked in conventional CNNs. This model demonstrated strong performance on the Breast Cancer Histology (Grand Challenge 2018 dataset), highlighting the value of contextual learning in improving classification accuracy. In subsequent work, she introduced a hybrid architecture combining CNNs with Bag-of-Visual-Words (BoVW), effectively integrating handcrafted and deep features to enhance discrimination in limited data settings. This approach outperformed standard deep networks like ResNet and DenseNet on the same dataset, underscoring the benefits of feature selection and hybrid modeling in medical imaging tasks. Together, these studies emphasize alternative paths for improving classification performance—either by modeling inter-patch relationships or by enriching feature spaces—both of which are highly relevant for advancing automated diagnosis in breast cancer histopathology. This research is toward an innovative approach for breast cancer detection, including the multiple subclasses of breast cancer, which still confuses medical doctors, and it hence makes patients undergo more advanced invasive tests for a better understanding of the disease. The technique is based on an AI model trained on a sizable batch of data. Section 2 discusses the model in detail, Section 3 shows the Results and Discussion, and the Conclusion is given in Section 4. 2 Methodology This methodology section combines an explanation of the different types of breast cancer and their subclasses, along with an advanced AI technique which has been developed to analyse breast cancer results of biopsy. The aim is, instead of making women go again and again through trial and error with treatments or misdiagnosing the cancer, using AI can make all the difference. As mentioned in the literature review section above, AI tech has changed the way the world views medicine now. Therefore, in this study, we have designed a powerful tool for diagnosing the multiple subclasses. This goes toward a humanitarian cause as women are the target here. Benign and malignant breast cancers differ in their behavior, prognosis, and treatment approach. Benign tumors, such as fibroadenomas or cysts, are non-cancerous growths that do not invade surrounding tissues or spread to other parts of the body. They tend to have well-defined borders, grow slowly, and usually pose little to no health risk. In contrast, malignant tumors are cancerous and have the potential to invade nearby tissues and metastasize to distant organs through the lymphatic system or bloodstream ( Robbins et al., 2010 National Cancer Institute, 2021 Robbins et al., 2010 Rakhlin et al., 2018 The visual features in the biopsy images provide key characteristics for distinguishing malignant and benign breast tissues. Figure 1a Robbins et al., 2010 Figure 1b National Cancer Institute, 2021 Figure 1 Biopsy image of malignant and benign sample (BreakHis dataset). (a) (b) A plain white background with no visible objects or text. 2.1 Dataset In this study, we utilize the BreaKHis dataset compiled by Spanhol et al. (2016) Spanhol et al., 2016 It comprises samples from 82 patients collected at a single institution in Brazil, with no publicly available metadata on age, ethnicity, or receptor status—24 with benign tumors and 58 with malignant tumors. Of the 7,909 images used in the final analysis, 5,429 are malignant and 2,480 are benign, distributed across the aforementioned magnification levels. Following preprocessing and class balancing, the revised class distribution is outlined in Table 1 Table 1 Dataset image count for each cancer type. Cancer type Final image count Subtype Adenosis 444 Benign Fibroadenoma 730 Benign Phyllodes tumor 453 Benign Tubular adenoma 569 Benign Ductal carcinoma 797 Malignant Lobular carcinoma 626 Malignant Mucinous carcinoma 792 Malignant Papillary carcinoma 560 Malignant Benign tumors, by histological standards, lack features associated with malignancy, such as cellular atypia, mitotic activity, invasive growth, or metastatic potential. They typically exhibit localized, slow growth and are considered non-lethal ( Robbins and Cotran, 2010 Robbins and Cotran, 2010 American Cancer Society, 2019 National Cancer Institute, 2021 The BreakHis dataset was used for both binary (benign vs. malignant) and multiclass subclass classification. All histopathological images were resized to 128 × 128 pixels and normalized to a standard RGB range. A stratified 80:20 train-test split was employed to preserve class balance across sets. For cross-validation, a 5-fold split was used with data shuffling to ensure unbiased performance evaluation, and no image was shared across folds. Prior to splitting, real-time augmentation was applied separately to the benign and malignant image sets, increasing the number of benign samples to 7,440 and malignant samples to 9,385. The augmented dataset, totaling 16,825 images, was then split into training, validation, and test sets using a stratified class-wise approach. For both benign and malignant classes, 20% of the data was reserved for testing, and the remaining 80% was further divided in a 75:25 ratio to form the training and validation sets. This resulted in 10,095 images in the training set and 3,365 images each in the validation and test sets. To improve generalization and address the risk of overfitting due to limited data, we implemented an extensive on-the-fly augmentation pipeline using TensorFlow’s data generators. This included spatial and color transformations to simulate natural histological variation while preserving key tissue features. Table 2 Table 2 Augmentation parameters. Augmentation type Range/value Rotation 90°, 180°, 270° Horizontal flip Random Vertical flip Random Zoom Up to ±10% Shear ±10° Brightness adjustment ±20% range Contrast adjustment ±20% range Saturation/hue jitter Applied with random factor These augmentations were applied to each batch during training to expose the model to diverse variations without expanding the dataset size. In addition, we used early stopping (patience = 5) and ReduceLROnPlateau scheduling to prevent overfitting, along with dropout (0.45), L2 kernel regularization, and batch normalization in each dense layer. Learning curves were generated during each fold to monitor training behavior. The curves showed a minimal gap between training and validation accuracy, indicating stable convergence and effective generalization across classes. To address class imbalance—particularly for underrepresented subtypes, such as PT and LC—we applied targeted data augmentation during preprocessing. For most classes, each image was augmented using 90° and 180° rotations along with horizontal flipping, effectively tripling their sample size. However, for class label 4 (DC), which was already sufficiently represented, only minimal augmentation was applied to prevent overrepresentation. This strategy helped balance the dataset and reduce bias during training, without altering the loss function or applying weighted sampling. 2.2 Artificial intelligence model A CNN is a deep learning architecture tailored for grid-like data such as images, leveraging spatial hierarchies to extract local and global features ( LeCun et al., 1998 Krizhevsky et al., 2012 Glorot et al., 2011 Scherer et al., 2010 Simonyan and Zisserman, 2014 He et al., 2016 Figure 2 Figure 2 CNN architecture ( Alzubaidi et al., 2021 Diagram of a neural network processing an image of a dog. The image progresses through layers: Convolution, ReLU, Pooling, and Fully Connected. The output classifies the image as \"Dog\" or \"Not Dog.\" 2.2.1 Classifying images as benign or malignant In the first approach, a conventional CNN architecture was developed to perform a binary classification of histopathological breast cancer images into benign and malignant categories. Rather than selecting parameters arbitrarily, several architectural configurations were tested iteratively to arrive at a structure that balanced computational efficiency and classification performance. The final model comprised three convolutional layers with 16, 32, and 16 filters, respectively, employing 4 × 4 kernels to effectively capture both low- and mid-level spatial features. This configuration aligns with literature that emphasizes the benefit of using multiple convolutional layers of varying depth for feature extraction in histological images ( Spanhol et al., 2016 Bayramoglu et al., 2017 Rakhlin et al., 2018 The output of the convolutional stack was flattened and passed into a fully connected layer with 256 neurons, enabling high-capacity representation before final classification. A sigmoid-activated output layer was used for binary probability prediction, and binary cross-entropy served as the loss function, both standard choices in binary medical image classification tasks ( Gandomkar et al., 2018 Liu et al., 2019 Bandi et al., 2018 Figure 3 Table 3 Figure 3 Initial CNN. Diagram of a convolutional neural network architecture. It starts with an input layer, followed by layers with 16 and 32 filters, pooling, flattening, a dense layer with 256 neurons, and ends with a sigmoid output. Legend indicates input, convolutional layer, pooling, flatten, dense layer, and output. Table 3 Hyperparameters for initial binary classification CNN. Hyperparameter Value Explanation Convolutional filters 16, 32, 16 The number of filters in each convolutional layer. More filters allow for learning more complex features but increase computational cost. Kernel size 4×4 Defines the size of the filter that scans the image. A smaller kernel captures fine details, while a larger one captures broader patterns. Activation function ReLU Introduces non-linearity to the network, allowing it to learn complex patterns. ReLU prevents the vanishing gradient problem. Pooling type MaxPooling Reduces spatial dimensions, retaining the most important features while lowering computational cost and reducing overfitting. Fully connected layer 256 neurons Processes extracted features and maps them to the final classification decision. A higher number of neurons allows for better learning capacity. Output activation Sigmoid Outputs a probability score between 0 and 1, making it suitable for binary classification tasks. Loss function Binary cross-entropy Measures how well the predicted probabilities match the true labels, ensuring optimal training for binary classification. Optimizer Adam Adjusts learning rates dynamically for each parameter, leading to faster convergence and better training stability. Epochs 20 The number of times the model passes through the entire dataset during training. More epochs improve learning but may cause overfitting. To overcome the limitations of the initial CNN model, a refined approach was adopted, incorporating particle swarm optimization (PSO) for hyperparameter tuning. PSO has demonstrated effectiveness in medical imaging by dynamically optimizing parameters for improved convergence ( Houssein et al., 2021 Figure 4 Srinivasu et al., 2021 Figure 4 Updated CNN. Diagram showing the architecture of a neural network using MobileNetV2 as a feature extractor. It includes components for input, MobileNetV2 model, PSO tuning, dense layer, learning and dropout rate, global average pooling, 256 neurons, and sigmoid output. Color-coded key explains each component: light purple for input, teal for MobileNetV2, red for PSO tuning, green for dense layer, blue for learning and dropout rate, orange for global average pooling, and yellow for output. The BreaKHis dataset was stratified by magnification level (40×, 100×, 200×, and 400×), and separate models were trained for each resolution, aligning with evidence that such granularity improves classification performance in multi-resolution tasks ( Bayramoglu et al., 2017 Bandi et al., 2018 PSO was used to fine-tune key hyperparameters—specifically, learning rate (searched in the range 1e-5 to 1e-2) and dropout rate (0.3–0.7)—to enhance convergence and prevent overfitting. Optimization was first performed on the 40 × magnification model; the resulting best parameters were then transferred to models for the other magnifications. To further stabilize training, a ReduceLROnPlateau scheduler (factor = 0.5, patience = 3, min_lr = 1e-6) dynamically adjusted the learning rate based on validation loss trends, while EarlyStopping (patience = 5, restore_best_weights = true) halted training when no improvement was observed, thereby avoiding overfitting and saving compute resources ( Srinivasu et al., 2021 Figure 4 Table 4 Table 4 Hyperparameters for the first attempt of per-magnification training. Hyperparameter Value(s) used Explanation Optimizer Particle swarm optimization (PSO) Instead of using Adam, PSO dynamically selects the best learning rate and dropout rate by minimizing validation loss. This helps in fine-tuning model performance more efficiently. Learning rate Optimized by PSO (range: 1e-5 to 1e-2) Controls the step size of weight updates. A smaller learning rate ensures stable learning, while a larger one speeds up convergence but risks overshooting the optimal point. Dropout rate Optimized by PSO (range: 0.3–0.7) Dropout prevents overfitting by randomly deactivating neurons during training. The PSO algorithm selects the best dropout rate for optimal generalization. Batch size 16 The number of images processed at once during training. A smaller batch size helps conserve memory but may result in higher training variance. Input image size (128, 128, 3) The input images are resized to 128 × 128 pixels with three color channels to ensure uniformity and reduce computational complexity. Base model MobileNetV2 (pre-trained on ImageNet) A lightweight deep learning model optimized for mobile and embedded vision applications. The feature extraction layers are frozen to retain learned representations. Magnification-based training 40×, 100×, 200×, and 400 × (trained separately) Instead of training all images together, the dataset is split based on magnification levels to improve feature learning at each resolution. Data augmentation Rescaling (1./255), width and height shifts (0.2), shear (0.2), zoom (0.2), and horizontal flips Artificially increases dataset size by applying transformations, helping the model generalize better to unseen samples. Validation split 20% A portion of the dataset is reserved for validation to evaluate model performance and prevent overfitting. Loss function Binary cross-entropy Since the classification is binary (benign vs. malignant), binary cross-entropy is used to compute the error between predicted and actual labels. Activation function ReLU (hidden layers), sigmoid (output layer) ReLU helps prevent vanishing gradients in hidden layers, while Sigmoid is used in the output layer for binary classification (probabilities between 0 and 1). ReduceLROnPlateau Factor = 0.5, patience = 3, min_lr = 1e-6 Reduces the learning rate if validation loss stops improving for three consecutive epochs, preventing unnecessary weight updates. EarlyStopping Monitor = val_loss, patience = 5, restore_best_weights = true Stops training if validation loss does not improve for five consecutive epochs, preventing overfitting and saving computation time. Epochs 30 The number of times the model sees the entire dataset during training. Early stopping ensures the model does not train longer than necessary. However, this approach too yielded an accuracy far below acceptable levels. F1-scores, recall and precision also hovered around the 50% mark, forcing another change in approach. The classification of breast cancer histopathology images involved separating the dataset based on different magnification levels (40×, 100×, 200×, and 400×). While this method aimed to leverage magnification-specific features, it resulted in severe dataset fragmentation. Each individual model was trained on a significantly smaller subset of images, which limited the ability to generalize across different test samples. Even with extensive data augmentation, the number of available images remained insufficient, leading to high variance and suboptimal performance during evaluation. Additionally, the previous models relied on relatively shallow CNNs for feature extraction. These networks struggled to effectively capture both low-level and high-level representations within the histopathology images, leading to limited discriminatory power when distinguishing between benign and malignant samples. Given these challenges, a more robust and generalized model was required to improve classification performance across all magnification levels. To overcome the limitations of dataset fragmentation and poor feature extraction, a new model was developed using DenseNet121 as the backbone for feature extraction. DenseNet121, a deep CNN pre-trained on the ImageNet dataset, was chosen due to its efficient parameter utilization and ability to capture intricate hierarchical features. Unlike the previous approach, this model does not separate images based on magnification, allowing for a larger and more diverse training dataset, thereby improving generalization. The proposed model extracts features from three different depths of DenseNet121: conv3_block12_concat, conv4_block24_concat, and conv5_block16_concat. These layers correspond to different levels of abstraction within the network, ensuring that both fine-grained and high-level morphological characteristics are captured. The selection of these layers allows for multi-scale feature extraction, improving the model’s ability to differentiate between benign and malignant samples. The extracted features are processed through a structured pipeline to refine and enhance their representation before classification. Feature extraction in this model occurred at three levels. The lowest-level feature map, obtained from conv3_block12_concat, captures fundamental visual characteristics such as edges, textures, and color distributions. These features are crucial for identifying initial structural differences in histopathology images, such as variations in cellular arrangements. The mid-level feature map, extracted from conv4_block24_concat, represents more complex patterns, including structural organization within the tissue and variations in gland formation. These features help differentiate between normal and abnormal cellular arrangements. The highest-level feature map, obtained from conv5_block16_concat, provides a more abstract representation, focusing on morphological changes indicative of malignancy, such as nuclear pleomorphism, mitotic activity, and stromal alterations. By incorporating multiple feature extraction levels, the model ensures a comprehensive understanding of tissue characteristics rather than relying on a single-layer representation. To ensure a balance between computational efficiency and adequate feature resolution, an input image size of 128 × 128 was chosen over 256 × 256. While higher resolutions like 256 × 256 could preserve more fine-grained tissue structures, they also significantly increase memory usage and computational load. Given that histopathology images already exhibit high variability and detailed cellular patterns, 128 × 128 provides a sufficient level of detail for feature extraction while allowing for larger batch sizes, faster training, and reduced GPU memory constraints. This choice is particularly important when training multiple models, as excessive computational demands could slow down hyperparameter tuning and optimization. Each extracted feature map undergoes global average pooling (GAP) to reduce dimensionality while retaining essential spatial information. Unlike max pooling, which selects only the most prominent activations, GAP ensures that all features contribute proportionally to the final representation, enhancing robustness and stability. After pooling, L2 normalization is applied to ensure numerical stability and prevent dominance of certain feature values due to large magnitudes. The normalized feature vectors are then transformed through fully connected layers, where a 64-neuron dense layer introduces non-linearity, allowing for more complex feature interactions. Batch normalization is used to stabilize learning by standardizing feature distributions and improving gradient flow, leading to faster convergence and better generalization. This entire process for one feature is shown by the four blocks in each row in Figure 5 Figure 5 Figure 5 Final binary class CNN. Diagram of a neural network using the DenseNet architecture. It begins with an input layer, followed by pre-trained DenseNet121 features, L2 normalization, batch normalization, and dropout layers. The network includes dense layers with sixty-four and sixteen neurons leading to a softmax output. Features are fused before the final output. The final classification layer consists of a softmax activation function with two neurons, representing the benign and malignant classes. This output layer assigns a probability score to each class, allowing for precise decision-making in binary classification tasks. By leveraging deep feature extraction, multi-scale fusion, and regularization techniques, the proposed model significantly improves the accuracy and robustness of breast cancer classification in histopathology images compared to the previous two models, as well as objectively. Figure 5 Table 5 Table 5 Hyperparameters for the final binary classification model. Hyperparameter Value Explanation Base model DenseNet121 (pre-trained on ImageNet) Acts as the backbone for feature extraction, leveraging pre-trained hierarchical features. Input image size (128, 128, 3) Defines the input shape of images with three color channels (RGB), ensuring consistency across the dataset. Feature extraction layers conv3_block12_concat, conv4_block24_concat, conv5_block16_concat Extracts multi-scale features from different levels of abstraction within the DenseNet121 model. Pooling method Global average pooling (GAP) Reduces dimensionality while retaining essential spatial information from feature maps. Feature normalization L2 normalization Ensures all extracted features contribute proportionally, preventing large-magnitude features from dominating learning. Hidden dense layer (feature processing) 64 neurons, ReLU activation, L2 regularization (0.001) Introduces non-linearity to enhance feature interactions and prevents overfitting via L2 regularization. Batch normalization Applied after dense layers Standardizes feature distributions, improving gradient flow and training stability. Feature fusion Concatenation of three feature levels Integrates low-level, mid-level, and high-level features into a unified representation for better classification. Final dense layer 16 neurons, ReLU activation, L2 regularization (0.001) Further refines the fused feature representation before classification. Dropout rate 0.45 Randomly deactivated neurons to prevent overfitting, ensuring better generalization. Output layer 2 neurons, softmax activation Generates class probabilities for binary classification (Benign vs. Malignant). Loss function Categorical cross-entropy Suitable for multi-class classification tasks, even though the output is binary (ensures numerical stability). Optimizer Adam (learning rate: 0.0001) Adaptive optimization algorithm that adjusts learning rates dynamically for efficient training. Learning rate scheduler ReduceLROnPlateau (factor = 0.5, patience = 3, min_lr = 1e-6) Reduces the learning rate when validation loss stagnates, helping fine-tune the model. EarlyStopping Patience = 5, restore_best_weights = true Stops training when validation loss stops improving, preventing unnecessary computation and overfitting. Epochs 50 Maximum number of iterations through the dataset, ensuring the model learns enough before early stopping. 2.2.2 Classifying images as subtypes of benign and malignant There are four histologically distinct types of benign breast tumors: adenosis (A), fibroadenoma (F), PT, and TA; and four malignant tumors (breast cancer): carcinoma (DC), LC, mucinous carcinoma (MC) and PC. Adenosis (A) is characterized by the proliferation of glandular structures, which may appear as distorted acini or ducts in biopsy images. These lesions typically show increased glandular tissue without significant atypia, making them relatively easy to differentiate from malignancies under a microscope ( Jiang et al., 2017 Schulz-Wendtland et al., 2016 Barker et al., 2018 Pike et al., 2020 In contrast, malignant breast cancers are often more challenging to classify due to the diversity of their histopathological features. DC is the most common type of breast cancer and typically appears as irregularly shaped masses with infiltrating ductal structures. The biopsy images show marked cellular pleomorphism, nuclear atypia, and often necrotic areas, which are indicative of aggressive behavior ( Tung et al., 2016 Vargas et al., 2017 Manning et al., 2018 Thompson et al., 2019 A CNN was initially trained to distinguish breast cancer subtypes by learning hierarchical spatial features from biopsy images. It captured architectural differences such as the regular tubules in TAs and the disorganized ductal structures in carcinomas, as well as key histological features like pleomorphism, stromal composition, and mucin presence ( Rajendran et al., 2018 Srinivasu et al., 2020 Table 6 Table 6 Hyperparameters for the initial subclass classification model. Hyperparameter Value Explanation Input image size (256, 256) The spatial dimensions of each input image are fed to the model. Batch size 32 Number of images processed at once during training. Learning rate (Adam) 0.0001 Speed at which the model updates weights during training using Adam optimizer. Learning rate (SGD) 0.01 Speed of weight updates for the SGD optimizer. SGD momentum 0.9 Momentum helps accelerate SGD by dampening oscillations during updates. Learning rate (RMSprop) 0.0001 Step size for weight updates using RMSprop optimizer. Conv2D filters (1st layer) 32 Number of filters in the first convolutional layer. Conv2D filters (2nd layer) 64 Number of filters in the second convolutional layer. Conv2D filters (3rd layer) 128 Number of filters in the third convolutional layer. Kernel Size (3, 3) Size of the sliding window applied in convolution operations. Activation function ReLU Non-linear function applied to neuron outputs to introduce non-linearity. Dropout rate (conv layers) 0.25 The fraction of neurons dropped to prevent overfitting during training. Dropout rate (dense layer) 0.5 Fraction of dense layer neurons dropped for regularization. pooling size (2, 2) Size of the window for max pooling operation to reduce feature map dimensions. Dense layer size 512 Number of neurons in the fully connected layer before the output. Output layer activation Softmax Ensures output values represent probabilities for multi-class classification. Loss function Categorical cross-entropy Measures model error for multi-class classification tasks. Number of epochs 30 Total number of complete passes through the training dataset. To enhance diagnostic granularity, we developed two separate CNN classifiers—one for benign subtypes and another for malignant subtypes—rather than training a single multi-class model across all categories to extend the binary class classifier to multiple classes. This decision is rooted in both clinical and computational rationale. Clinically, benign and malignant lesions differ not just in severity but in their underlying histomorphological characteristics, tissue organization, and staining patterns ( Barker et al., 2018 Pike et al., 2020 Gandomkar et al., 2018 Patel et al., 2024 Umer et al. (2022) Arevalo et al. (2016) McKinney et al., 2020 Lee et al., 2024 The multi-class CNN architectures for subtype classification were directly adapted from the binary classification model described previously. DenseNet121, used as the backbone in both approaches, provides efficient deep feature extraction through its dense connectivity and hierarchical learning capabilities. As in the binary model, features were extracted from three depths—conv3_block12_concat, conv4_block24_concat, and conv5_block16_concat—capturing progressively abstract morphological features necessary for histopathological differentiation. These include low-level structures such as cellular edges and textures, mid-level patterns like glandular and stromal organization, and high-level attributes such as nuclear pleomorphism and mitotic activity. Each extracted feature map underwent GAP, L2 normalization, and transformation via Instead of a binary output, the final softmax layer consisted of four neurons to predict between the benign or malignant subtypes, depending on the network. Two separate CNNs were trained—one for benign subtypes and another for malignant—to reduce feature entanglement and focus each model on intra-class variation. This design choice improved the model’s capacity to learn subtle distinctions specific to each class group, such as distinguishing fibroadenoma from PT, or DC from PC. Optimization and training procedures mirrored those of the binary classification model, utilizing the Adam optimizer (learning rate 0.0001), EarlyStopping, and ReduceLROnPlateau for improved convergence and generalization (see Figures 6 7 Table 7 Figure 6 CNN used for multi-class classification. Diagram of a neural network architecture incorporating DenseNet features. Input flows through DenseNet121 layers, including L2 normalization and global average pooling. It transitions through 64 neurons and 16 neurons before a softmax layer with 4 neurons. Components include batch normalization, dropout layers, and L2 regularization. The color-coded legend indicates each layer type. Figure 7 Diagram for feature workflows. Flowchart of a feature workflow using a neural network. Begins with DenseNet121 feature extraction, then global average pooling in two dimensions, followed by L2 normalization. This is passed through a dense layer with 64 neurons, then L2 regularization and batch normalization. Table 7 Hyperparameters for the final subclass classification model. Hyperparameter Value Explanation Base model DenseNet121 (pre-trained on ImageNet) Acts as a deep feature extractor, leveraging hierarchical feature learning for efficient classification. Input image size (128, 128, 3) Ensures computational efficiency while preserving essential histopathological details. A higher resolution, like 256 × 256, would increase memory usage without significantly improving feature representation. Feature extraction layers conv3_block12_concat, conv4_block24_concat, conv5_block16_concat Extracts features at multiple abstraction levels, capturing both fine-grained and high-level morphological details. Pooling method Global average pooling (GAP) Reduces dimensionality while preserving spatial characteristics of the extracted feature maps. Feature normalization L2 normalization Prevents feature values with high magnitudes from dominating the learning process, ensuring numerical stability. Hidden dense layer (feature processing) 64 neurons, ReLU activation, L2 regularization (0.001) Introduces non-linearity for complex feature interactions and applies L2 regularization to prevent overfitting. Batch normalization Applied after dense layer s Standardizes feature distributions, accelerating convergence and improving training stability. Feature fusion Concatenation of three feature levels Integrates information across different levels of abstraction, enhancing classification performance. Final dense layer 16 neurons, ReLU activation, L2 regularization (0.001) Refines the multi-scale feature representation before classification. Dropout rate 0.45 Prevents overfitting by randomly deactivating neurons, ensuring better generalization. Output layer 4 neurons, softmax activation Outputs probability scores for the four benign subtypes. Loss function Categorical cross-entropy Suitable for multi-class classification tasks, optimizing probability-based learning. Optimizer Adam (learning rate: 0.0001) Adaptive optimization that adjusts learning rates dynamically for efficient and stable training. Learning rate scheduler ReduceLROnPlateau (factor = 0.5, patience = 3, min_lr = 1e-6) Reduces learning rate when validation loss stagnates, improving fine-tuning of weights. EarlyStopping Patience = 5, restore_best_weights = true Stops training when validation loss stops improving, preventing unnecessary computation and overfitting. Epochs 50 Maximum number of training iterations through the dataset, ensuring sufficient learning. 2.3 Novel characteristics of the model The proposed classification algorithm is distinguished by its multi-scale feature fusion architecture built on DenseNet121. Instead of relying only on the final layer features (as done in most standard CNN classifiers), it extracts and fuses features from three different depths of the DenseNet (the end of conv3, conv4, and conv5 blocks). Each feature map is subjected to GAP and L2 normalization, then passed through a small fully connected layer with batch normalization (and dropout) before concatenation. This design enables the model to capture discriminative patterns at multiple spatial scales—from fine-grained cellular details to higher-level tissue organization—within a single network. In essence, the model behaves like an integrated multi-scale ensemble, where lower-level texture features and higher-level semantic features jointly inform the final prediction. This multi-scale fusion is particularly well-suited to histopathology images, where diagnostically relevant cues appear at different magnifications. Prior work has recognized the value of multi-scale information: Araújo et al. (2017) Araújo et al., 2017 Gupta and Bhavsar (2018) Gupta and Bhavsar, 2018 By fusing intermediate feature vectors, our approach yields a richer representation that combines context from multiple receptive field sizes. This strategy is analogous to the “hypercolumn” or feature pyramid concept—effectively consolidating features at various levels of abstraction. In histopathology, such multi-level fusion has clear advantages: for example, benign vs. malignant differentiation may depend on both cellular morphology and the broader tissue architecture. Traditional transfer-learning classifiers often discard these multi-level cues by only using the final global feature map of a pre-trained CNN. In contrast, our DenseNet121-based model preserves multi-scale feature information by tapping conv3, conv4, and conv5 outputs. Zhu et al. (2019) Zhu et al., 2019 Huang et al., 2017 Liew et al. (2021) Liew et al., 2021 Another novel aspect of our approach is the use of L2 normalization and balanced regularization in the feature fusion process. Each branch’s pooled feature vector is L2-normalized before fusion, ensuring that no single scale’s features dominate the others due to scale differences. This is an uncommon yet effective practice in classification networks—more frequently seen in metric learning or multi-instance aggregation contexts—and it promotes stability when training the fused classifier. For instance, Xie et al. (2020) Xie et al., 2020 Kode and Barkana (2023) Kode and Barkana, 2023 Compared to existing deep learning methods in breast cancer histopathology, the proposed model offers a new balance of simplicity and multi-scale sophistication. Many prior studies have focused either on transfer learning with single CNNs or on ensembles of multiple models. On the one hand, simple transfer-learning approaches, fine-tuning a single network (e.g., ResNet or DenseNet), have shown strong baseline performance ( Spanhol et al., 2016 Wahab et al., 2017 Aldakhil et al. (2024) Aldakhil et al., 2024 Mehta et al. (2022) Mehta et al., 2022 Umer et al. (2022) 6B-Net Umer et al., 2022 Zhu et al. (2019) Zhu et al., 2019 Wakili et al. (2022) Wakili et al., 2022 Araújo et al., 2017 Liew et al., 2021 In summary, the novelty of our algorithm lies in combining multi-scale feature fusion, transfer learning with DenseNet121, and strategic normalization/regularization into one coherent model for breast histopathology classification. This design captures the multi-scale nature of tissue patterns more explicitly than standard CNN classifiers. It leverages DenseNet’s strength in feature reuse while addressing multiple scales akin to multi-branch networks, but with fewer parameters and a single-pass inference. By comparing with the literature, we see that our approach is unique in how Gupta and Bhavsar (2018) Wakili et al., 2022 Liew et al., 2021 2.4 Metrics evaluations In AI, especially in domains like classification and regression, evaluation metrics play a vital role in measuring model performance. Within medical diagnostics, metrics such as precision and recall are particularly significant. Precision reflects the proportion of correct positive predictions, meaning that when a model identifies cancer, it is likely to be correct—thereby reducing unnecessary alarm ( Sokolova and Lapalme, 2009 Sokolova and Lapalme, 2009 Tharwat, 2020 To address such limitations, the F1-score is often employed. This metric, calculated as the harmonic mean of precision and recall, offers a more balanced view by considering both false positives and false negatives—an essential factor in medical imaging, where failing to detect malignant tumors can have serious consequences ( Chicco and Jurman, 2020 Yang and Ying, 2022 Table 8 Table 8 Metrics for evaluation of the algorithm. Metric Formula Explanation References Precision TP/(TP + FP) Measures the proportion of correctly predicted positive cases out of all predicted positives.  Sokolova and Lapalme (2009) Recall (sensitivity) TP/(TP + FN) Measures the proportion of actual positive cases correctly identified by the model.  Sokolova and Lapalme (2009) F1-score 2(Precision*Recall)/(Precision+Recall) Harmonic mean of precision and recall, providing a balanced measure for imbalanced datasets.  Chicco and Jurman (2020) Accuracy (TP + TN)/(TP + TN + FP + FN) Measures overall correctness of the model across all classes.  Tharwat (2020) Area under the curve (AUC) Computed from ROC curve Represents the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative one.  Yang and Ying (2022) TP (True Positives): Correctly predicted positive cases; TN (True Negatives): Correctly predicted negative cases; FP (False Positives): Incorrectly predicted positive cases; FN (False Negatives): Incorrectly predicted negative cases. 3 Results and discussion The following sections present a comprehensive evaluation of the developed AI models across various experimental phases, including binary classification and multi-class subtype detection for breast cancer biopsy images. Detailed performance metrics such as accuracy, precision, recall, F1-score, and ROC-AUC are reported to assess the effectiveness and generalizability of each model. The study began by analyzing the outcomes of the initial CNN architecture, followed by performance improvements observed with the final DenseNet121-based model. It then extends this evaluation to multi-class classification tasks, detailing the results for both benign and malignant subtypes. These results collectively demonstrate the diagnostic power and clinical viability of the proposed framework. 3.1 The binary categorical classification as benign or malignant The initial CNN architecture, developed for binary classification of histopathology images into benign and malignant categories, was evaluated using different filter and kernel size configurations. Table 9 Table 9 Initial results of binary classification. Filters Mesh size Precision Recall Binary accuracy AUC 32, 32, 32 3×3 0.8603 0.9283 0.8424 0.8768 16, 32, 16 4×4 0.8893 0.8994 0.8542 0.8872 16, 32, 16 5×5 0.8253 0.8465 0.7799 0.8290 Upon using the proposed DenseNet121-based binary classification, the model demonstrated outstanding performance in distinguishing between benign and malignant breast cancer histopathology images. The model was trained on out-of-sample biopsy images. As shown in Table 10 Figure 8 Figure 9 Table 10 Final results of binary classification algorithm. Metric Value Test accuracy 0.9863 Precision 0.9888 Recall 0.9867 F1-score 0.9877 ROC–AUC 0.9863 Figure 8 Confusion matrix for final binary classification. Confusion matrix for a binary model showing true labels versus predicted labels. The matrix has quadrants with the following values: top-left (true negatives) 1467, top-right (false positives) 21, bottom-left (false negatives) 25, bottom-right (true positives) 1852. A blue gradient color bar on the right indicates higher values with darker shades. Figure 9 Accuracy/loss vs. epoch graph. Line chart showing training and validation accuracy and loss over 50 epochs for binary classification. Training accuracy (blue) and validation accuracy (red) both reach near 1.0. Training loss (green) and validation loss (yellow) decrease, with validation loss showing more fluctuation. 3.2 Multi-category classification of benign and malignant cancers The performance of the multi-class classification models for benign and malignant breast cancer subtypes is summarized in Table 11 Figure 10 Figure 11 Figure 12 Figure 13 Table 11 Final results for multi-category classification. Classification task Accuracy Precision Recall F1-score Benign 0.9483 0.9541 0.9324 0.9415 Malignant 0.9254 0.9318 0.9193 0.9251 Figure 10 Confusion matrix for benign sub-category classification. Confusion matrix for a benign model showing true labels versus predicted labels. Values range from zero to six hundred, with notable entries: 247, 648, 186, and 330 along the diagonal, indicating correct predictions. A color gradient from light to dark blue represents frequency. Figure 11 Confusion matrix for malignant sub-category classification. Confusion matrix for a malignant model with true labels on the y-axis and predicted labels on the x-axis. Values: 0:655, 28, 8, 6; 1:37, 319, 7, 0; 2:16, 4, 465, 3; 3:24, 3, 4, 298. Color intensity corresponds to the count, with a scale from 0 to 655. Figure 12 Accuracy/loss vs. epoch graph for benign subclass model. Graph showing training and validation accuracy, and loss over 50 epochs for benign subtype classification. Blue and red lines represent training and validation accuracy stabilizing around 0.9 and 0.8. Green and yellow lines show decreasing loss starting above 1.6, stabilizing around 0.4 and 0.6, respectively. Figure 13 Accuracy/loss vs. epoch graph for malignant subclass model. Plot showing training and validation accuracy for malignant subtype classification over 50 epochs. The blue line represents training accuracy and stabilizes around 1.0. The red line indicates validation accuracy, fluctuating near 0.8. The green line shows loss decreasing, while the yellow line for validation loss fluctuates significantly. The results indicate that the model for benign subtypes outperformed the malignant classification model in terms of overall accuracy. However, the malignant classification model maintained strong recall and F1-score values, demonstrating its ability to correctly identify malignant subtypes while balancing precision. These findings support the hypothesis that training separate models for benign and malignant subtypes allows for more specialized learning, leading to improved classification performance. The integration of DenseNet121 as a feature extractor has further contributed to the model’s ability to capture multi-scale morphological patterns in histopathology images. 3.2.1 Per-class metrics for benign subtype classification To provide deeper insight into the model’s subclass-level performance, we evaluated per-class precision, recall, and F1-score for the benign tumor categories ( Table 12 Table 12 Per-class metrics for benign classes. Classification task Precision Recall F1-score Adenosis 0.99 0.96 0.98 Fibroadenoma 0.89 0.99 0.94 Phyllodes tumor 0.94 0.74 0.83 Tubular adenoma 0.98 0.94 0.96 This result aligns with known clinical challenges, as PTs and cellular fibroadenomas often exhibit overlapping histological features, especially under low magnification. Even experienced pathologists can find it difficult to differentiate between these entities, given their shared stromal overgrowth and similar architectural patterns ( Barker et al., 2018 To improve classification in this region of diagnostic uncertainty, future iterations of the model could incorporate additional histological cues beyond morphology alone—such as mitotic count, stromal cellularity, or margin assessment, which are often critical in distinguishing PTs from fibroadenomas. Enhanced annotation protocols that focus on these differentiating features, particularly at multiple magnifications, could help reduce misclassification. Overall, while the model demonstrates strong performance in benign subtype differentiation, the results also highlight the necessity of targeted refinement in clinically ambiguous classes like PTs. Figures 14 17 Figure 14 Adenosis confusion matrix. Confusion matrix for adenosis classification. True positives: 247, true negatives: 1229, false positives: 3, false negatives: 9. Predicted and actual labels: adenosis and other. Figure 15 Fibroadenoma confusion matrix. Confusion matrix titled \"One-vs-All Confusion Matrix: Fibroadenoma\" with actual categories as \"Other\" and \"Fibroadenoma,\" and predicted categories as \"Other\" and \"Fibroadenoma.\" It shows values: 741 true negatives, 80 false positives, 10 false negatives, and 657 true positives. Figure 16 Phyllodes tumour confusion matrix. Confusion matrix for Phyllodes Tumor classification. True positives: 171, False positives: 11. True negatives: 1246, False negatives: 60. Shows predicted versus actual classifications for Phyllodes Tumor and Other. Figure 17 Tubular adenoma confusion matrix. Confusion matrix titled \"One-vs-All Confusion Matrix: Tubular Adenoma\" with actual values on the vertical axis and predicted values on the horizontal axis. The matrix shows true negatives: 1149, false positives: 5, false negatives: 20, and true positives: 314. 3.2.2 Per-class metrics for malignant subtype classification For malignant tumor classification, per-class evaluation similarly revealed strong and balanced performance (see Table 13 Figures 18 21 Table 13 Per-class metrics for malignant classes. Classification task Precision Recall F1-score Ductal carcinoma 0.93 0.89 0.91 Lobular carcinoma 0.83 0.90 0.86 Mucinous carcinoma 0.93 0.97 0.95 Papillary carcinoma 0.99 0.92 0.95 Figure 18 Ductal carcinoma confusion matrix. Confusion matrix for ductal carcinoma classification. True positives: 620, false negatives: 77, true negatives: 1136, false positives: 44. Columns represent predictions; rows represent actuals. Figure 19 Lobular carcinoma confusion matrix. Confusion matrix titled \"One-vs-All Confusion Matrix: Lobular Carcinoma.\" It displays actual versus predicted values. True negatives: 1446, false positives: 68, false negatives: 36, true positives: 327. Figure 20 Mucinous carcinoma confusion matrix. Confusion matrix for Mucinous Carcinoma classification. True positives: 475, false positives: 35, true negatives: 1,354, false negatives: 13. Used for assessing model performance. Figure 21 Papillary carcinoma confusion matrix. Confusion matrix titled \"One-vs-All Confusion Matrix: Papillary Carcinoma\" with rows labeled \"Other\" and \"Papillary Carcinoma\" against columns labeled \"Other\" and \"Papillary Carcinoma.\" Values are 1544 (true negatives), 4 (false positives), 25 (false negatives), and 304 (true positives). Although this study primarily focuses on classification performance, model interpretability is essential for clinical adoption. Techniques such as gradient-weighted class activation mapping (grad-CAM) or SHapley additive exPlanations (SHAP) can be used to generate heatmaps that visualize which regions of a biopsy image most influenced the model’s predictions. These visual explanations could help verify whether the model is focusing on diagnostically relevant features—Such as nuclear pleomorphism, stromal arrangement, or mitotic activity—Thereby increasing clinician trust and facilitating integration into diagnostic workflows. 3.3 K-fold cross-validation and performance stability To evaluate the generalizability of the proposed classifiers, we performed 5-fold cross-validation on each of the three tasks: binary classification, benign subtype classification, and malignant subtype classification. The mean values, standard deviations, and 95% confidence intervals are presented in Table 14 Table 14 Cross-validation scores for each model. Task Metric Score (±SD) 95% CI Binary class Accuracy 0.9712 ± 0.0069 0.9616–0.9808 Precision 0.9874 ± 0.0064 0.9785–0.9964 Recall 0.9608 ± 0.0160 0.9386–0.9830 F1-score 0.9738 ± 0.0062 0.9652–0.9825 Benign subtype Accuracy 0.9375 ± 0.0208 0.9087–0.9663 Precision 0.9468 ± 0.0132 0.9284–0.9651 Recall 0.9300 ± 0.0274 0.8919–0.9680 F1-score 0.9369 ± 0.0203 0.9087–0.9650 Malignant subtype Accuracy 0.9202 ± 0.0080 0.9091–0.9314 Precision 0.9048 ± 0.0122 0.8878–0.9217 Recall 0.8808 ± 0.0265 0.8440–0.9176 F1-score 0.8907 ± 0.0157 0.8689–0.9125 As shown in Table 14 The observed decline in recall and F1-scores compared to the single train-test split is expected, and it reflects the increased rigor of cross-validation, which exposes the model to harder-to-classify samples in more diverse folds. From a diagnostic perspective, the relatively stable precision across tasks implies that the model is conservative in positive predictions, minimizing false positives—an important trait in histopathological screening scenarios. However, the slightly larger variation in recall for rare or borderline subtypes warrants attention, as it may affect sensitivity to clinically ambiguous cases. These findings reinforce the utility of k-fold cross-validation as a stress-testing tool, revealing not just average performance but also subtype-specific reliability under different sampling conditions. Future iterations of the model could incorporate stratified sampling by subtype or uncertainty-aware training strategies to further reduce variability in underrepresented classes. 3.4 Final notes The final model, built on DenseNet121 with multi-scale feature extraction, demonstrated the strongest performance among all evaluated architectures, achieving high accuracy, recall, and F1-scores in the binary classification of breast cancer biopsy images. By extracting and fusing features from three intermediate convolutional blocks, the model was able to learn both fine-grained cellular structures and broader tissue-level patterns relevant to histopathological classification. The incorporation of L2 normalization, dropout regularization, and batch normalization within each branch further contributed to its stability and generalization. This design effectively addressed limitations observed in earlier models, such as feature loss due to single-layer reliance or overfitting from insufficient regularization. Compared to existing models evaluated on the same BreaKHis dataset, the proposed approach offers a substantial improvement. Araújo et al. (2017) Vo et al. (2019) Bayramoglu et al. (2016) Munshi et al. (2024) The model’s strong and consistent performance across both binary and subtype classification tasks positions it as a scalable and technically rigorous approach for digital histopathology. With additional validation on multi-institutional and heterogeneous datasets, this framework has the potential to contribute meaningfully to diagnostic support systems for breast cancer, especially in settings with limited expert pathologist availability. 4 Conclusion This study presents a deep learning framework for breast cancer diagnosis that performs subtype-level classification directly from H&E-stained biopsy images using a DenseNet121-based multi-scale feature fusion architecture. In conventional diagnostic workflows, the determination of histological subtypes often requires additional procedures—such as immunohistochemistry (IHC), molecular assays, or serial imaging—that increase diagnostic latency, invasiveness, and healthcare costs ( Robbins et al., 2010 National Cancer Institute, 2021 Manning et al., 2018 Spanhol et al., 2016 Rakhlin et al., 2018 Our novel methodological contributions allow the model to capture both fine-grained cellular features and broader tissue-level patterns in a unified, end-to-end framework. Notably, this design improves over standard transfer learning approaches that rely solely on final-layer representations or require external fusion mechanisms ( Gupta and Bhavsar, 2018 Zhu et al., 2019 Wakili et al., 2022 While the model achieved strong binary classification accuracy (98.63%) and high F1-scores in multi-class subtype tasks (benign: 0.9415, malignant: 0.9251), its clinical utility is best positioned in supporting difficult diagnostic cases—such as distinguishing PTs from fibroadenomas or mucinous from PCs—where visual overlap challenges even experienced pathologists ( Barker et al., 2018 Thompson et al., 2019 Kohavi, 1995 Berrar, 2019 We emphasize that this tool is not a replacement for pathologists but a potential assistive technology to aid in triaging, second-opinion support, and prioritizing ambiguous cases. In resource-limited settings where access to expert pathology or advanced molecular testing is constrained, such a tool could help reduce diagnostic bottlenecks and improve equity in care delivery ( McKinney et al., 2020 WHO, 2021 However, further validation on larger and more heterogeneous datasets is necessary before clinical deployment ( Liew et al., 2021 Xie et al., 2020 Lee et al., 2024 5 Limitations and future work While the proposed model demonstrates strong performance in both binary and subtype-level breast cancer classification, several limitations must be acknowledged. First, the model was trained and validated solely on the BreaKHis dataset, which—despite its popularity in computational pathology research—is limited in scale and diversity. Its 7,909 images come from only 82 patients, restricting the model’s generalizability across varied populations, staining protocols, and imaging equipment. Future studies should aim to validate the model on larger, multi-institutional datasets that capture real-world clinical variability to increase the generalizability of the study. Second, the model’s predictions are based exclusively on morphological features extracted from H&E-stained slides and do not incorporate molecular information, such as Human Epidermal Growth Factor Receptor 2 (HER2), Estrogen Receptor / Progesterone Receptor (ER/PR), or triple-negative status. These receptor-level biomarkers are critical for treatment planning, and their exclusion limits the clinical applicability of the system. Expanding the model to include IHC images or genomic profiles would enable a more comprehensive diagnostic tool aligned with current oncology workflows. Another limitation lies in the model’s use of single-magnification images during training, despite the fact that pathologists typically examine biopsies at multiple magnification levels to evaluate both cellular and tissue-level structures. Although our multi-scale feature extraction within DenseNet121 captures some hierarchical information, it does not replicate the diagnostic reasoning derived from viewing across magnifications. Future work could explore multi-resolution input strategies or hierarchical CNNs to better reflect clinical interpretation. Moreover, the current model lacks interpretability features, which are increasingly essential for clinical integration. Tools such as Grad-CAM or SHAP could be used to highlight regions of interest in biopsy images, helping pathologists understand model decisions and assess reliability. Including these visual explanations would significantly enhance trust and transparency in real-world applications. Additionally, while this study emphasizes a novel architecture, it does not provide comparative results against widely used CNN baselines such as VGG16, ResNet50, or EfficientNet, nor does it present ablation experiments isolating the impact of architectural components like L2 normalization or feature fusion. Including these comparisons would strengthen claims of architectural innovation and clarify which design elements drive performance gains. Finally, deployment considerations remain speculative. The model has not yet been tested in live clinical settings or integrated into diagnostic workflows, where computational constraints, system latency, and compatibility with laboratory information systems pose practical challenges. Future work should also consider automating the augmentation pipeline—currently hand-tuned—using approaches such as AutoAugment or Generative Adversarial Network (GAN)-based synthesis to improve performance in rare subtypes and low-data scenarios. By addressing these limitations through external validation, multimodal expansion, improved interpretability, and clinical simulation, this work can move closer to real-world deployment as a reliable assistive tool in digital breast pathology. Data availability statement The original contributions presented in the study are included in the article/supplementary material; further inquiries can be directed to the corresponding author. Author contributions NC: Conceptualization, Investigation, Methodology, Software, Writing – original draft, Writing – review & editing. ZD: Conceptualization, Investigation, Methodology, Software, Writing – original draft, Writing – review & editing. Conflict of interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Generative AI statement The authors declare that no Gen AI was used in the creation of this manuscript. Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us. Publisher’s note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. References Aldakhil L. A. Alhasson H. F. Alharbi S. S 2024 EfficientNetV2 + CBAM for breast histology classification Diagnostics 14 56 10.3390/diagnostics14010056 PMC11241245 39001292 Alzubaidi L. Zhang J. Humaidi A. J. Al-Dujaili A. Al-Shamma O. Santamaría J. 2021 Review of deep learning: concepts, CNN architectures, challenges, applications, future directions J. Big Data 8 53 10.1186/s40537-021-00444-8 33816053 PMC8010506 American Cancer Society 2019 Types of breast biopsies https://www.cancer.org/cancer/breast-cancer/screening-tests-and-early-detection/breast-biopsy/types-of-biopsies.html Araújo T. Aresta G. Castro E. Rouco J. Aguiar P. Eloy C. 2017 Classification of breast cancer histology images using Convolutional Neural Networks PLoS One 12 e0177544 10.1371/journal.pone.0177544 28570557 PMC5453426 Arevalo J. González F. A. Ramos-Pollán R. Oliveira J. L. Guevara Lopez M. A. 2016 Representation learning for mammography mass lesion classification with convolutional neural networks Comput. Methods Prog. Biomed. 127 248 257 10.1016/j.cmpb.2015.12.014 26826901 Atrey K. Singh B. K. Bodhey N. K. 2024 Multimodal breast cancer classification with mammogram and ultrasound fusion using ResNet18 and SVM J. Med. Imaging. 11 021405 10.1117/1.JMI.11.2.021405 Bandi P. Geessink O. Manson Q. Van Dijk M. C. Balkenhol M. Hermsen M. 2018 From detection of individual metastases to classification of lymph node status at the patient level: the CAMELYON17 challenge IEEE Trans. Med. Imaging 37 2613 2624 10.1109/TMI.2018.2867350 30716025 Barker E. Gosein M. Kulkarni S. Jones S. 2018 Phyllodes tumors of the breast: a comprehensive review of clinical and histopathological features J. Pathol. Transl. Med. 52 221 229 Bayramoglu N. Kannala J. Heikkilä J. 2016 Deep learning for magnification-independent breast cancer histopathology image classification Int. Conf. Pattern Recognit. 2440 2445 10.1109/ICPR.2016.7900002 Bayramoglu N. Kannala J. Heikkilä J. 2017 Deep learning for magnification-independent breast cancer histopathology image classification Proceedings of the IEEE International Conference on Biomedical Imaging 914 918 Berrar D. 2019 Cross-validation Encyclopedia of bioinformatics and computational biology Rouse M. Oxford, United Kingdom Elsevier 542 545 Chicco D. Jurman G. 2020 The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation BMC Genomics 21 6 10.1186/s12864-019-6413-7 31898477 PMC6941312 Esteva A. Chou K. Yeung S. Naik N. Madani A. Mottaghi A. 2021 A guide to deep learning in healthcare Comput. Biol. Med. 27 24 29 10.1038/s41591-020-1122-4 Ezzat D. Hassanien A. E. 2023 Optimized Bayesian CNN (OBCNN) with ResNet101V2 and Monte Carlo dropout for breast cancer diagnosis Comput. Biol. Med. 152 106352 10.1016/j.cmpb.2023.106352 Fu A. Yao B. Dong T. Chen Y. Yao J. Liu Y. 2022 Tumor-resident intracellular microbiota promotes metastatic colonization in breast cancer Cell. 185 1356 1372.e26 10.1016/j.cell.2022.02.027 35395179 Gandomkar Z. Brennan P. C. Mello-Thoms C. 2018 Computer-based image analysis in breast pathology: a review of current methods and future directions Breast 42 56 67 10.4103/2153-3539.192814 PMC5100199 28066683 GLOBOCAN 2020 Global cancer statistics: Breast cancer incidence and mortality worldwide International Agency for Research on Cancer (IARC) Available https://gco.iarc.fr/today/home Glorot X. Bordes A. Bengio Y. 2011 Deep sparse rectifier neural networks Proceedings of the 14th International Conference on Artificial Intelligence and Statistics 315 323 Gupta A. Bhavsar A. 2018 Breast cancer histopathological image classification: is magnification important? 2018 IEEE 15th international symposium on biomedical imaging (ISBI), IEEE 1110 1113 He K. Zhang X. Ren S. Sun J. 2016 Deep residual learning for image recognition Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 770 778 Houssein E. H. Hosney M. E. Zaher A. S. 2021 Particle swarm optimization for deep learning hyperparameter tuning in medical image classification Neural Comput. & Applic. 33 13359 13378 Huang G. Liu Z. Van Der Maaten L. Weinberger K. Q. 2017 Densely connected convolutional networks Proceedings of CVPR. 2017 4700 4708 10.1109/CVPR.2017.243 Jiang Y. Song M. Zhang X. Lin Y. 2017 The diagnostic challenges of adenosis in breast pathology Pathol. Res. Pract. 213 1 9 Joseph S. Kumar S. Rajesh R. Thomas J. Varghese R. Nair S. 2022 Multi-classification of breast cancer using handcrafted and deep features from BreakHis Pattern Recogn. Lett. 161 76 83 10.1016/j.patrec.2022.01.004 Kaymak S. Kandemir M. Zhang C. Caputo B. Hamprecht F. A. Lampert C. H. 2017 Neural network classification of breast cancer histopathology images using discrete Haar wavelets Biomed. Res. Int. 2017 1 9 10.1155/2017/5179020 Kode R. Barkana B. D. 2023 Analysis of CNN performance in classifying BreakHis breast histopathology images under varied dropout configurations Biomed. Signal Process. Control 82 104534 10.1016/j.bspc.2023.104534 Kohavi R. 1995 A study of cross-validation and bootstrap for accuracy estimation and model selection Proceedings of the 14th International Joint Conference on Artificial Intelligence 1137 1143 Kolb T. M. Lichy J. Newhouse J. H. 2002 Comparison of the performance of screening mammography, physical examination, and breast US and evaluation of factors that influence them: an analysis of 27,825 patient evaluations Radiology 225 165 175 10.1148/radiol.2251011667 12355001 Krizhevsky A. Sutskever I. Hinton G. E. 2012 ImageNet classification with deep convolutional neural networks Adv. Neural Inf. Proces. Syst. 25 1097 1105 10.48550/arXiv.1207.0580 Kuhl C. K. Schrading S. Leutner C. C. Morakkabati-Spitz N. Wardelmann E. Fimmers R. 2005 MRI for diagnosis of pure ductal carcinoma in situ: a prospective observational study Lancet Oncol. 6 433 441 10.1016/S1470-2045(05)70148-0 17693177 LeCun Y. Bottou L. Bengio Y. Haffner P. 1998 Gradient-based learning applied to document recognition Proc. IEEE 86 2278 2324 10.1109/5.726791 Lee H. Banerjee S. Choi Y. 2024 AI-powered interpretability in breast cancer subtype detection Insights Imaging 15 227 10.1186/s13244-024-01810-9 39320560 PMC11424596 Lehman C. D. Wellman R. D. Buist D. S. Kerlikowske K. Tosteson A. N. Miglioretti D. L. 2019 Diagnostic accuracy of digital screening mammography with and without computer-aided detection J. Am. Coll. Radiol. 16 1303 1311 10.1001/jamainternmed.2015.5231 PMC4836172 26414882 Li J. Lu W. Fei H. Luo M. Dai M. Xia M. 2024b Multimodal fusion of MRI and RNA-seq data using ResNet34 and transformers for breast cancer treatment prediction Sci. Rep. 14 11976 10.1038/s41598-024-71976-9 Liew C. Y. Basri R. Aizat M. A. Rajendran P. 2021 DenseNet201-XGBoost ensemble for breast cancer classification using histopathological images Biocybern. Biomed. Eng. 41 977 987 Litjens G. Kooi T. Bejnordi B. E. Setio A. A. A. Ciompi F. Ghafoorian M. 2017 A survey on deep learning in medical image analysis Med. Image Anal. 42 60 88 10.1016/j.media.2017.07.005 28778026 Liu R. Zhang T. Tian F. Yang J. Yue H. Liu Y. 2024a Transfer learning and focal loss applied to mammogram classification using VGG16 Medical Image Analysis. 90 102953 10.1117/12.3032878 Liu Y. Jain A. Eng C. Way D. H. Lee K. Bui P. 2019 A deep learning system for differential diagnosis of skin diseases Nat. Med. 25 900 908 10.1038/s41591-020-0842-3 32424212 Manning S. E. Kauffmann R. Keating J. J. Goldberg M. 2018 Mucinous carcinoma of the breast: a comprehensive review of histological features and prognosis Am. J. Surg. Pathol. 42 1505 1513 McKinney S. M. Sieniek M. Godbole V. Godwin J. Antropova N. Ashrafian H. 2020 International evaluation of an AI system for breast cancer screening Nature 577 89 94 10.1038/s41586-019-1799-6 31894144 Mehta P. Arora A. Singh R. 2022 HATNet: hybrid attention transformer for breast cancer subtype classification Comput. Med. Imaging Graph. 97 102079 10.1016/j.compmedimag.2022.102079 Muduli D. Dash R. Majhi B. 2021 Deep CNN for mammogram and ultrasound breast cancer classification Expert Syst. Appl. 181 115190 10.1016/j.eswa.2021.115190 Munshi R. M. Cascone L. Alturki N. Saidani O. Alshardan A. Umer M. 2024 An explainable ensemble AI approach for breast cancer diagnosis with CNNs and traditional ML IEEE Access. 12 42177 42189 10.1109/ACCESS.2024.1234567 National Cancer Institute 2021 Breast cancer treatment (adult) (PDQ®) – patient version https://www.cancer.gov/types/breast/patient/breast-treatment-pdq Patel R. Kumar A. Sharma L. Desai M. 2024 Multi-scale fusion with DenseNet for histopathology image subtype classification Diagnostics (Basel) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11899611/ Paul G. R. Preethi J. 2023 Breast cancer detection using SDM-WHO-RNN with LS-CED segmentation Healthc. Anal. 3 100154 10.1016/j.health.2023.100154 Pike C. M. Walters S. Gleeson J. Ahmed S. 2020 Tubular adenoma of the breast: a rare benign lesion with histological characteristics Breast J. 26 1024 1028 Punitha S. Stephan T. Gandomi A. H. 2021 Artificial immune system and bee colony optimization for feature selection in automated breast cancer diagnosis Comput. Biol. Med. 134 104460 10.1016/j.cmpb.2021.104460 Rajendran N. P. Santhosh K. Varghese J. Sivakumar A. 2018 Deep learning in medical image analysis: a survey J. Pathol. Inform. 9 10 Rakhlin A. Shvets A. Iglovikov V. Kalinin A. A. 2018 Deep convolutional neural networks for breast cancer histology image analysis arXiv preprint arXiv 1802.00752 10.48550/arXiv.1802.00752 Robbins S. L. Cotran R. S. 2010 Robbins and Cotran Pathologic Basis of Disease 8th ed. Philadelphia, PA, USA Elsevier/Saunders Robbins S. L. Cotran R. S. Kumar V. 2010 Robbins and Cotran pathologic basis of disease 8th Elsevier/Philadelphia, PA, USA Saunders Robinson P. Preethi S. 2024 SDM-WHO-RNN model for histopathology image classification Appl. Intell. 10.1007/s10489-024-XXXX-X Rodrigues J. Oliveira L. Pereira P. M. Cardoso J. S. Lima C. S. 2021 AI-enhanced breast cancer detection Front. Oncol. 11 654210 10.3389/fonc.2021.654210 Scherer D. Müller A. Behnke S. 2010 International Conference on Artificial Neural Networks Schulz-Wendtland R. Fasching P. Bani M. R. Lux M. P. Jud S. Rauh C. 2016 Fibroadenomas and their clinical characteristics Breast J. 21 324 330 10.1111/tbj.12432 Sharma R. Jain G. Jain S. Phulre A. K. Sharma R. Joshi S. 2024 Stacked ensemble framework for breast cancer classification using WBCD dataset Diagnostics. 14 220 10.3390/diagnostics14030220 Siegel R. L. Miller K. D. Jemal A. 2022 Cancer statistics, 2022 CA Cancer J. Clin. 72 7 33 10.3322/caac.21708 35020204 Simonyan K. Zisserman A. 2014 Very deep convolutional networks for large-scale image recognition arXiv preprint arXiv 1409.1556 10.48550/arXiv.1409.1556 Sokolova M. Lapalme G. 2009 A systematic analysis of performance measures for classification tasks Inf. Process. Manag. 45 427 437 10.1016/j.ipm.2009.03.002 Spanhol F. A. Oliveira L. S. Petitjean C. Heutte L. 2016 A dataset for breast cancer histopathological image classification IEEE Trans. Biomed. Eng. 63 1455 1462 10.1109/TBME.2015.2496264 26540668 Srinivasu P. N. SivaSai J. G. Ijaz M. F. Bhoi A. K. Kim W. Kang S. J. 2020 Multi-class breast cancer subtype classification using CNNs J. Ambient Intell. Humaniz. Comput. 11 6029 6039 10.1007/s12652-020-01841-2 Srinivasu P. N. SivaSai J. G. Ijaz M. F. Bhoi A. K. Kim W. Kang S. J. 2021 Breast cancer detection using deep learning and transfer learning techniques J. Ambient. Intell. Humaniz. Comput. 12 6019 6030 Taheri H. Omranpour H. 2023 EMFSG-Net: Ensemble meta-feature generator for ultrasound breast cancer classification with VGG16 SVR Diagnostics. 13 1342 10.3390/diagnostics13071342 37046559 PMC10093281 Tharwat A. 2020 Classification assessment methods Appl. Comput. Inform. 17 168 192 10.1016/j.aci.2018.08.003 Thompson E. S. Gill R. K. Rao S. Mandava N. 2019 Papillary carcinoma of the breast: an analysis of clinicopathological features and prognostic markers Breast Cancer Res. Treat. 177 391 398 Tung H. H. Hsu L. H. Hsieh Y. C. Cheng C. C. 2016 Ductal carcinoma of the breast: histopathological features and prognostic markers J. Clin. Pathol. 69 437 445 Umer M. Khan M. A. Sharif M. Raza M. Kadry S. 2022 6B-net: a novel CNN-based multi-branch model for breast cancer classification in histopathological images Biomed. Signal Process. Control 71 103170 10.1016/j.bspc.2021.103170 34567236 PMC8450520 Vargas A.-C. Lakhani S. R. Simpson P. T. 2017 Histological features of invasive lobular carcinoma Pathol. Int. 67 255 264 10.1111/pin.12521 Vo D. M. Nguyen N.-Q. Lee S.-W. 2019 Breast cancer histopathological image classification using hybrid feature representations BMC Med. Inform. Decis. Mak. 19 71 10.1186/s12911-019-0802-7 Wahab N. Khan A. Lee Y. 2017 Transfer learning with CNNs for breast cancer histopathology image classification Comput. Biol. Med. 85 86 97 10.1016/j.compbiomed.2017.04.012 28477446 Wakili S. Rajaraman S. Antani S. 2022 DenTNet: DenseNet-based transfer learning for breast cancer image classification Diagnostics 12 1452 10.3390/diagnostics12061452 35741262 PMC9221735 WHO 2021 Breast cancer: key facts https://www.who.int/news-room/fact-sheets/detail/breast-cancer World Health Organization (WHO) 2022 Breast cancer: Key facts World Health Organization Fact Sheet Available at https://www.who.int/news-room/fact-sheets/detail/breast-cancer Xie S. Ma Y. Li C. 2020 A dual-branch CNN with patch-level supervision for histopathology image classification IEEE Access 8 68246 68256 Yang F. Ying H. 2022 ROC-AUC as a robust metric for medical classification IEEE Access. 10 45671 45683 10.1109/ACCESS.2022.3156789 Zhang T. Mehta A. Kapoor S. Rao P. 2024 Performance analysis of breast cancer histopathology image classification using transfer learning models Sci. Rep. https://www.nature.com/articles/s41598-024-75876-2 Zhu Y. Zhang S. Liu W. Zhang X. 2019 Multi-scale CNN based breast cancer histopathological image classification IEEE Access 7 28987 28994 ",
  "metadata": {
    "Title of this paper": "Multi-scale CNN based breast cancer histopathological image classification",
    "Journal it was published in:": "Frontiers in Artificial Intelligence",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12463984/"
  }
}